{
  "title": "[Paper Review] StyleGAN (Style Generative Adversarial Nets)",
  "publishedAt": "2024-07-13T00:00:00.000Z",
  "updatedAt": "2024-07-13T00:00:00.000Z",
  "description": "StyleGAN introduces a novel architecture for generative adversarial networks that separates high-level attributes from stochastic variation, enabling fine-grained control over the generated images through an intermediate latent space and adaptive instance normalization.",
  "image": {
    "filePath": "../public/blogs/stylegan/screenshot.png",
    "relativeFilePath": "../../public/blogs/stylegan/screenshot.png",
    "format": "png",
    "height": 1454,
    "width": 1846,
    "aspectRatio": 1.2696011004126548,
    "blurhashDataUrl": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAPFBMVEX////R09JxY1/DzL/y8/Gzr7D6+vrr7On19vXh5t+FaF9MKBplSkaVhYDArqDEuLCalpZkZ2msoJsyDgAUhgyrAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAO0lEQVR4nCWLSQ7AIAzEDJ0khC50+f9fq1KfLEuGj5aBHIglWQ/kUYLNaLNUQ64Ue8EZl82J/tRf7n6+KGsBOixupnoAAAAASUVORK5CYII="
  },
  "isPublished": true,
  "author": "junbrro",
  "tags": [
    "Deep Learning"
  ],
  "body": {
    "raw": "\nThis post is reviewing the StyleGAN paper.\n\n# PGGAN: Progressive Growing of GANs\n\n![PGGAN Diagram](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/c0838342-f24f-4c0d-bec3-43061057e24b)\n\n**Progressive training** starts with low-resolution images and gradually increases the resolution by adding layers. This technique enhances the stability and quality of the images generated by the GAN. By progressively growing the GAN, it generates high-resolution, high-quality images and reduces instability in training high-resolution images. The **adaptive learning rate** adjusts for each layer, with new layers having higher rates, while **pixelwise feature vector normalization** in the generator ensures stable training. This method effectively captures fine details, making the generated images more realistic and has proven to be flexible across various domains, not just faces. The influence of PGGAN on subsequent GAN research is significant, promoting progressive training and high-resolution generation.\n\n# Style-based Generator: StyleGAN\n\n![StyleGAN Diagram](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/808eb66e-c516-4201-aacd-99b0e362702d)\n\nTraditional GANs receive input through the input layer, but **StyleGAN omits this layer** and instead uses a nonlinear mapping from a learned constant to an **intermediate latent space W**. In this process, the latent code z from the input latent space Z is transformed into the intermediate latent space W through a nonlinear mapping network f, implemented using an 8-layer MLP. The converted vector w is then specialized into a **spatially invariant style y** through learned affine transformations. This style y controls the **Adaptive Instance Normalization (AdaIN)** at each convolution layer of the generator.\n\n![AdaIN](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/e57716fe-908c-45b7-b0a1-2b7138bf0d57)\n![AdaIN Operation](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/bf6ee61d-e861-4476-a3be-cf9a630d9afd)\n\nThe **AdaIN operation** normalizes each feature map separately and then scales and biases it using the corresponding scalar components of style y. For each feature map, AdaIN adjusts its scale and shifts based on the style information encoded in y. This process allows the generator to apply complex style patterns to the generated images, influencing everything from color schemes to textures in a coherent and controlled manner. The use of a spatially invariant style ensures the generated image maintains a consistent style throughout.\n\nTo enhance the generator's ability to create stochastic details, **explicit noise inputs** are implemented. These inputs are single-channel images filled with uncorrelated Gaussian noise. Each layer of the synthesis network receives its own unique noise image, which is scaled for each feature map based on learned scaling factors and then added to the output of its respective convolution layer.\n\n![Noise Input](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/f23ed6e1-a98e-447c-b603-2e707f8b1ed7)\n\n### Quality of Generated Images\n\nThe improvement in image quality with StyleGAN is demonstrated by the reduction in **FID (Frechet Inception Distances)** across different generator architectures for the CELEBA-HQ and the new FFHQ datasets. This advancement and the enhancement of image resolution represent significant strides in GAN research.\n\n### Prior Art\n\nPrevious research focused heavily on **improving the discriminator**, using techniques such as multiple discriminators, multi-resolution discrimination, and self-attention mechanisms. For the generator, research concentrated on fine-tuning the distribution in the input latent space or shaping the latent space, with conditional generators exploring new methods of feeding class identifiers through several layers of the generator.\n\n# Properties of the Style-based Generator\n\n### Style Mixing\n\n![Style Mixing](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/bb33d2d1-e774-4e62-9f6d-f07bd9e689a5)\n\n**Mixing Regularization:** During the learning process, StyleGAN uses ‘mixing’ as a form of regularization. By mixing styles during training, the model learns to handle a variety of style combinations more effectively. This regularization improves the model's robustness and generalization capabilities.\n\n**Style Mixing:** During testing, the same mixing process allows for the combination of styles from two different images. This feature enables the generation of images that blend characteristics from multiple source images, resulting in unique and diverse outputs.\n\n### Stochastic Variation\n\n![Stochastic Variation 1](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/20f20554-eee4-495f-a5c9-0ee3018e759f)\n![Stochastic Variation 2](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/2a99646c-5964-4536-aa8c-f6d6ece78598)\n\n**Noise Inputs and Stochastic Details:** By adding explicit noise inputs to each layer of the synthesis network, StyleGAN can generate stochastic details in the images. These noise inputs are single-channel images filled with uncorrelated Gaussian noise, scaled by learned factors and added to the output of their respective convolution layers. This process allows the generator to introduce fine-grained details, such as hair strands or skin texture, enhancing the realism of the generated images.\n\n### Separation of Global Effects from Stochasticity\n\nIn StyleGAN, the model can distinguish between global effects and stochastic details. Global effects, such as the overall shape and structure of the generated objects, are controlled by the style vectors. In contrast, stochastic variations, like the fine details and textures, are influenced by the noise inputs. This separation allows for better control and manipulation of the generated images, enabling users to tweak global features without affecting fine details and vice versa.\n\n# Disentanglement Studies\n\n### Perceptual Path Length\n\n![Perceptual Path Length 1](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/ad13296e-8e76-466b-8ee5-0c3b8a797224)\n![Perceptual Path Length 2](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/298a1098-4644-43e0-b161-b5191c2e8fef)\n![Perceptual Path Length 3](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/a8571e11-5ea9-4f18-816d-f8cc21fa54e5)\n\n**Perceptual Path Length:** This metric measures the perceptual difference between images as they move through the latent space. A lower perceptual path length indicates smoother transitions and better disentanglement of the latent space, meaning that changes in the latent vectors correspond to meaningful changes in the generated images.\n\n### Linear Separability\n\nStyleGAN’s architecture improves the **linear separability** of the latent space, which makes it easier to isolate and manipulate individual features of the generated images. This characteristic is particularly valuable for applications requiring specific attribute modifications, such as changing the hair color or facial expression in generated portraits.\n\n# Conclusion\n\nStyleGAN represents a significant advancement in GAN technology. By introducing a style-based generator and progressive growing, it achieves high-resolution, high-quality image generation with improved stability. The model's ability to mix styles, generate stochastic details, and separate global effects from fine details allows for unprecedented control and flexibility in image generation. The improvements in perceptual path length and linear separability further enhance the quality and usability of the generated images, making StyleGAN a powerful tool in the field of generative models.\n",
    "code": "var Component=(()=>{var dn=Object.create;var P=Object.defineProperty;var cn=Object.getOwnPropertyDescriptor;var ln=Object.getOwnPropertyNames;var fn=Object.getPrototypeOf,mn=Object.prototype.hasOwnProperty;var q=(c,n)=>()=>(n||c((n={exports:{}}).exports,n),n.exports),bn=(c,n)=>{for(var g in n)P(c,g,{get:n[g],enumerable:!0})},ve=(c,n,g,N)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let y of ln(n))!mn.call(c,y)&&y!==g&&P(c,y,{get:()=>n[y],enumerable:!(N=cn(n,y))||N.enumerable});return c};var hn=(c,n,g)=>(g=c!=null?dn(fn(c)):{},ve(n||!c||!c.__esModule?P(g,\"default\",{value:c,enumerable:!0}):g,c)),gn=c=>ve(P({},\"__esModule\",{value:!0}),c);var je=q((xn,xe)=>{xe.exports=React});var ke=q(z=>{\"use strict\";(function(){\"use strict\";var c=je(),n=Symbol.for(\"react.element\"),g=Symbol.for(\"react.portal\"),N=Symbol.for(\"react.fragment\"),y=Symbol.for(\"react.strict_mode\"),B=Symbol.for(\"react.profiler\"),X=Symbol.for(\"react.provider\"),K=Symbol.for(\"react.context\"),E=Symbol.for(\"react.forward_ref\"),A=Symbol.for(\"react.suspense\"),C=Symbol.for(\"react.suspense_list\"),w=Symbol.for(\"react.memo\"),O=Symbol.for(\"react.lazy\"),we=Symbol.for(\"react.offscreen\"),Q=Symbol.iterator,Ue=\"@@iterator\";function Te(e){if(e===null||typeof e!=\"object\")return null;var r=Q&&e[Q]||e[Ue];return typeof r==\"function\"?r:null}var x=c.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED;function m(e){{for(var r=arguments.length,a=new Array(r>1?r-1:0),i=1;i<r;i++)a[i-1]=arguments[i];He(\"error\",e,a)}}function He(e,r,a){{var i=x.ReactDebugCurrentFrame,s=i.getStackAddendum();s!==\"\"&&(r+=\"%s\",a=a.concat([s]));var d=a.map(function(u){return String(u)});d.unshift(\"Warning: \"+r),Function.prototype.apply.call(console[e],console,d)}}var Re=!1,Se=!1,Pe=!1,Ae=!1,Ce=!1,Z;Z=Symbol.for(\"react.module.reference\");function Oe(e){return!!(typeof e==\"string\"||typeof e==\"function\"||e===N||e===B||Ce||e===y||e===A||e===C||Ae||e===we||Re||Se||Pe||typeof e==\"object\"&&e!==null&&(e.$$typeof===O||e.$$typeof===w||e.$$typeof===X||e.$$typeof===K||e.$$typeof===E||e.$$typeof===Z||e.getModuleId!==void 0))}function Ie(e,r,a){var i=e.displayName;if(i)return i;var s=r.displayName||r.name||\"\";return s!==\"\"?a+\"(\"+s+\")\":a}function J(e){return e.displayName||\"Context\"}function p(e){if(e==null)return null;if(typeof e.tag==\"number\"&&m(\"Received an unexpected object in getComponentNameFromType(). This is likely a bug in React. Please file an issue.\"),typeof e==\"function\")return e.displayName||e.name||null;if(typeof e==\"string\")return e;switch(e){case N:return\"Fragment\";case g:return\"Portal\";case B:return\"Profiler\";case y:return\"StrictMode\";case A:return\"Suspense\";case C:return\"SuspenseList\"}if(typeof e==\"object\")switch(e.$$typeof){case K:var r=e;return J(r)+\".Consumer\";case X:var a=e;return J(a._context)+\".Provider\";case E:return Ie(e,e.render,\"ForwardRef\");case w:var i=e.displayName||null;return i!==null?i:p(e.type)||\"Memo\";case O:{var s=e,d=s._payload,u=s._init;try{return p(u(d))}catch{return null}}}return null}var v=Object.assign,D=0,ee,ne,re,te,ae,ie,oe;function ue(){}ue.__reactDisabledLog=!0;function Fe(){{if(D===0){ee=console.log,ne=console.info,re=console.warn,te=console.error,ae=console.group,ie=console.groupCollapsed,oe=console.groupEnd;var e={configurable:!0,enumerable:!0,value:ue,writable:!0};Object.defineProperties(console,{info:e,log:e,warn:e,error:e,group:e,groupCollapsed:e,groupEnd:e})}D++}}function Le(){{if(D--,D===0){var e={configurable:!0,enumerable:!0,writable:!0};Object.defineProperties(console,{log:v({},e,{value:ee}),info:v({},e,{value:ne}),warn:v({},e,{value:re}),error:v({},e,{value:te}),group:v({},e,{value:ae}),groupCollapsed:v({},e,{value:ie}),groupEnd:v({},e,{value:oe})})}D<0&&m(\"disabledDepth fell below zero. This is a bug in React. Please file an issue.\")}}var I=x.ReactCurrentDispatcher,F;function U(e,r,a){{if(F===void 0)try{throw Error()}catch(s){var i=s.stack.trim().match(/\\n( *(at )?)/);F=i&&i[1]||\"\"}return`\n`+F+e}}var L=!1,T;{var Me=typeof WeakMap==\"function\"?WeakMap:Map;T=new Me}function se(e,r){if(!e||L)return\"\";{var a=T.get(e);if(a!==void 0)return a}var i;L=!0;var s=Error.prepareStackTrace;Error.prepareStackTrace=void 0;var d;d=I.current,I.current=null,Fe();try{if(r){var u=function(){throw Error()};if(Object.defineProperty(u.prototype,\"props\",{set:function(){throw Error()}}),typeof Reflect==\"object\"&&Reflect.construct){try{Reflect.construct(u,[])}catch(_){i=_}Reflect.construct(e,[],u)}else{try{u.call()}catch(_){i=_}e.call(u.prototype)}}else{try{throw Error()}catch(_){i=_}e()}}catch(_){if(_&&i&&typeof _.stack==\"string\"){for(var o=_.stack.split(`\n`),b=i.stack.split(`\n`),l=o.length-1,f=b.length-1;l>=1&&f>=0&&o[l]!==b[f];)f--;for(;l>=1&&f>=0;l--,f--)if(o[l]!==b[f]){if(l!==1||f!==1)do if(l--,f--,f<0||o[l]!==b[f]){var h=`\n`+o[l].replace(\" at new \",\" at \");return e.displayName&&h.includes(\"<anonymous>\")&&(h=h.replace(\"<anonymous>\",e.displayName)),typeof e==\"function\"&&T.set(e,h),h}while(l>=1&&f>=0);break}}}finally{L=!1,I.current=d,Le(),Error.prepareStackTrace=s}var k=e?e.displayName||e.name:\"\",Ne=k?U(k):\"\";return typeof e==\"function\"&&T.set(e,Ne),Ne}function We(e,r,a){return se(e,!1)}function Ve(e){var r=e.prototype;return!!(r&&r.isReactComponent)}function H(e,r,a){if(e==null)return\"\";if(typeof e==\"function\")return se(e,Ve(e));if(typeof e==\"string\")return U(e);switch(e){case A:return U(\"Suspense\");case C:return U(\"SuspenseList\")}if(typeof e==\"object\")switch(e.$$typeof){case E:return We(e.render);case w:return H(e.type,r,a);case O:{var i=e,s=i._payload,d=i._init;try{return H(d(s),r,a)}catch{}}}return\"\"}var R=Object.prototype.hasOwnProperty,de={},ce=x.ReactDebugCurrentFrame;function S(e){if(e){var r=e._owner,a=H(e.type,e._source,r?r.type:null);ce.setExtraStackFrame(a)}else ce.setExtraStackFrame(null)}function Ye(e,r,a,i,s){{var d=Function.call.bind(R);for(var u in e)if(d(e,u)){var o=void 0;try{if(typeof e[u]!=\"function\"){var b=Error((i||\"React class\")+\": \"+a+\" type `\"+u+\"` is invalid; it must be a function, usually from the `prop-types` package, but received `\"+typeof e[u]+\"`.This often happens because of typos such as `PropTypes.function` instead of `PropTypes.func`.\");throw b.name=\"Invariant Violation\",b}o=e[u](r,u,i,a,null,\"SECRET_DO_NOT_PASS_THIS_OR_YOU_WILL_BE_FIRED\")}catch(l){o=l}o&&!(o instanceof Error)&&(S(s),m(\"%s: type specification of %s `%s` is invalid; the type checker function must return `null` or an `Error` but returned a %s. You may have forgotten to pass an argument to the type checker creator (arrayOf, instanceOf, objectOf, oneOf, oneOfType, and shape all require an argument).\",i||\"React class\",a,u,typeof o),S(null)),o instanceof Error&&!(o.message in de)&&(de[o.message]=!0,S(s),m(\"Failed %s type: %s\",a,o.message),S(null))}}}var $e=Array.isArray;function M(e){return $e(e)}function qe(e){{var r=typeof Symbol==\"function\"&&Symbol.toStringTag,a=r&&e[Symbol.toStringTag]||e.constructor.name||\"Object\";return a}}function ze(e){try{return le(e),!1}catch{return!0}}function le(e){return\"\"+e}function fe(e){if(ze(e))return m(\"The provided key is an unsupported type %s. This value must be coerced to a string before before using it here.\",qe(e)),le(e)}var G=x.ReactCurrentOwner,Be={key:!0,ref:!0,__self:!0,__source:!0},me,be,W;W={};function Xe(e){if(R.call(e,\"ref\")){var r=Object.getOwnPropertyDescriptor(e,\"ref\").get;if(r&&r.isReactWarning)return!1}return e.ref!==void 0}function Ke(e){if(R.call(e,\"key\")){var r=Object.getOwnPropertyDescriptor(e,\"key\").get;if(r&&r.isReactWarning)return!1}return e.key!==void 0}function Qe(e,r){if(typeof e.ref==\"string\"&&G.current&&r&&G.current.stateNode!==r){var a=p(G.current.type);W[a]||(m('Component \"%s\" contains the string ref \"%s\". Support for string refs will be removed in a future major release. This case cannot be automatically converted to an arrow function. We ask you to manually fix this case by using useRef() or createRef() instead. Learn more about using refs safely here: https://reactjs.org/link/strict-mode-string-ref',p(G.current.type),e.ref),W[a]=!0)}}function Ze(e,r){{var a=function(){me||(me=!0,m(\"%s: `key` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\",r))};a.isReactWarning=!0,Object.defineProperty(e,\"key\",{get:a,configurable:!0})}}function Je(e,r){{var a=function(){be||(be=!0,m(\"%s: `ref` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\",r))};a.isReactWarning=!0,Object.defineProperty(e,\"ref\",{get:a,configurable:!0})}}var en=function(e,r,a,i,s,d,u){var o={$$typeof:n,type:e,key:r,ref:a,props:u,_owner:d};return o._store={},Object.defineProperty(o._store,\"validated\",{configurable:!1,enumerable:!1,writable:!0,value:!1}),Object.defineProperty(o,\"_self\",{configurable:!1,enumerable:!1,writable:!1,value:i}),Object.defineProperty(o,\"_source\",{configurable:!1,enumerable:!1,writable:!1,value:s}),Object.freeze&&(Object.freeze(o.props),Object.freeze(o)),o};function nn(e,r,a,i,s){{var d,u={},o=null,b=null;a!==void 0&&(fe(a),o=\"\"+a),Ke(r)&&(fe(r.key),o=\"\"+r.key),Xe(r)&&(b=r.ref,Qe(r,s));for(d in r)R.call(r,d)&&!Be.hasOwnProperty(d)&&(u[d]=r[d]);if(e&&e.defaultProps){var l=e.defaultProps;for(d in l)u[d]===void 0&&(u[d]=l[d])}if(o||b){var f=typeof e==\"function\"?e.displayName||e.name||\"Unknown\":e;o&&Ze(u,f),b&&Je(u,f)}return en(e,o,b,s,i,G.current,u)}}var V=x.ReactCurrentOwner,he=x.ReactDebugCurrentFrame;function j(e){if(e){var r=e._owner,a=H(e.type,e._source,r?r.type:null);he.setExtraStackFrame(a)}else he.setExtraStackFrame(null)}var Y;Y=!1;function $(e){return typeof e==\"object\"&&e!==null&&e.$$typeof===n}function ge(){{if(V.current){var e=p(V.current.type);if(e)return`\n\nCheck the render method of \\``+e+\"`.\"}return\"\"}}function rn(e){{if(e!==void 0){var r=e.fileName.replace(/^.*[\\\\\\/]/,\"\"),a=e.lineNumber;return`\n\nCheck your code at `+r+\":\"+a+\".\"}return\"\"}}var pe={};function tn(e){{var r=ge();if(!r){var a=typeof e==\"string\"?e:e.displayName||e.name;a&&(r=`\n\nCheck the top-level render call using <`+a+\">.\")}return r}}function _e(e,r){{if(!e._store||e._store.validated||e.key!=null)return;e._store.validated=!0;var a=tn(r);if(pe[a])return;pe[a]=!0;var i=\"\";e&&e._owner&&e._owner!==V.current&&(i=\" It was passed a child from \"+p(e._owner.type)+\".\"),j(e),m('Each child in a list should have a unique \"key\" prop.%s%s See https://reactjs.org/link/warning-keys for more information.',a,i),j(null)}}function ye(e,r){{if(typeof e!=\"object\")return;if(M(e))for(var a=0;a<e.length;a++){var i=e[a];$(i)&&_e(i,r)}else if($(e))e._store&&(e._store.validated=!0);else if(e){var s=Te(e);if(typeof s==\"function\"&&s!==e.entries)for(var d=s.call(e),u;!(u=d.next()).done;)$(u.value)&&_e(u.value,r)}}}function an(e){{var r=e.type;if(r==null||typeof r==\"string\")return;var a;if(typeof r==\"function\")a=r.propTypes;else if(typeof r==\"object\"&&(r.$$typeof===E||r.$$typeof===w))a=r.propTypes;else return;if(a){var i=p(r);Ye(a,e.props,\"prop\",i,e)}else if(r.PropTypes!==void 0&&!Y){Y=!0;var s=p(r);m(\"Component %s declared `PropTypes` instead of `propTypes`. Did you misspell the property assignment?\",s||\"Unknown\")}typeof r.getDefaultProps==\"function\"&&!r.getDefaultProps.isReactClassApproved&&m(\"getDefaultProps is only used on classic React.createClass definitions. Use a static property named `defaultProps` instead.\")}}function on(e){{for(var r=Object.keys(e.props),a=0;a<r.length;a++){var i=r[a];if(i!==\"children\"&&i!==\"key\"){j(e),m(\"Invalid prop `%s` supplied to `React.Fragment`. React.Fragment can only have `key` and `children` props.\",i),j(null);break}}e.ref!==null&&(j(e),m(\"Invalid attribute `ref` supplied to `React.Fragment`.\"),j(null))}}function un(e,r,a,i,s,d){{var u=Oe(e);if(!u){var o=\"\";(e===void 0||typeof e==\"object\"&&e!==null&&Object.keys(e).length===0)&&(o+=\" You likely forgot to export your component from the file it's defined in, or you might have mixed up default and named imports.\");var b=rn(s);b?o+=b:o+=ge();var l;e===null?l=\"null\":M(e)?l=\"array\":e!==void 0&&e.$$typeof===n?(l=\"<\"+(p(e.type)||\"Unknown\")+\" />\",o=\" Did you accidentally export a JSX literal instead of a component?\"):l=typeof e,m(\"React.jsx: type is invalid -- expected a string (for built-in components) or a class/function (for composite components) but got: %s.%s\",l,o)}var f=nn(e,r,a,s,d);if(f==null)return f;if(u){var h=r.children;if(h!==void 0)if(i)if(M(h)){for(var k=0;k<h.length;k++)ye(h[k],e);Object.freeze&&Object.freeze(h)}else m(\"React.jsx: Static children should always be an array. You are likely explicitly calling React.jsxs or React.jsxDEV. Use the Babel transform instead.\");else ye(h,e)}return e===N?on(f):an(f),f}}var sn=un;z.Fragment=N,z.jsxDEV=sn})()});var Ge=q((kn,De)=>{\"use strict\";De.exports=ke()});var Nn={};bn(Nn,{default:()=>yn,frontmatter:()=>pn});var t=hn(Ge()),pn={title:\"[Paper Review] StyleGAN (Style Generative Adversarial Nets)\",description:\"StyleGAN introduces a novel architecture for generative adversarial networks that separates high-level attributes from stochastic variation, enabling fine-grained control over the generated images through an intermediate latent space and adaptive instance normalization.\",image:\"../../public/blogs/stylegan/screenshot.png\",publishedAt:\"2024-07-13\",updatedAt:\"2024-07-13\",author:\"junbrro\",isPublished:!0,tags:[\"Deep Learning\"]};function Ee(c){let n=Object.assign({p:\"p\",h1:\"h1\",a:\"a\",span:\"span\",img:\"img\",strong:\"strong\",h3:\"h3\"},c.components);return(0,t.jsxDEV)(t.Fragment,{children:[(0,t.jsxDEV)(n.p,{children:\"This post is reviewing the StyleGAN paper.\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:13,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h1,{id:\"pggan-progressive-growing-of-gans\",children:[(0,t.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#pggan-progressive-growing-of-gans\",children:(0,t.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\"},this),\"PGGAN: Progressive Growing of GANs\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:15,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:(0,t.jsxDEV)(n.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/c0838342-f24f-4c0d-bec3-43061057e24b\",alt:\"PGGAN Diagram\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:17,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:17,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[(0,t.jsxDEV)(n.strong,{children:\"Progressive training\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:19,columnNumber:1},this),\" starts with low-resolution images and gradually increases the resolution by adding layers. This technique enhances the stability and quality of the images generated by the GAN. By progressively growing the GAN, it generates high-resolution, high-quality images and reduces instability in training high-resolution images. The \",(0,t.jsxDEV)(n.strong,{children:\"adaptive learning rate\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:19,columnNumber:351},this),\" adjusts for each layer, with new layers having higher rates, while \",(0,t.jsxDEV)(n.strong,{children:\"pixelwise feature vector normalization\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:19,columnNumber:445},this),\" in the generator ensures stable training. This method effectively captures fine details, making the generated images more realistic and has proven to be flexible across various domains, not just faces. The influence of PGGAN on subsequent GAN research is significant, promoting progressive training and high-resolution generation.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:19,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h1,{id:\"style-based-generator-stylegan\",children:[(0,t.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#style-based-generator-stylegan\",children:(0,t.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\"},this),\"Style-based Generator: StyleGAN\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:21,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:(0,t.jsxDEV)(n.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/808eb66e-c516-4201-aacd-99b0e362702d\",alt:\"StyleGAN Diagram\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:23,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:23,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[\"Traditional GANs receive input through the input layer, but \",(0,t.jsxDEV)(n.strong,{children:\"StyleGAN omits this layer\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:25,columnNumber:61},this),\" and instead uses a nonlinear mapping from a learned constant to an \",(0,t.jsxDEV)(n.strong,{children:\"intermediate latent space W\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:25,columnNumber:158},this),\". In this process, the latent code z from the input latent space Z is transformed into the intermediate latent space W through a nonlinear mapping network f, implemented using an 8-layer MLP. The converted vector w is then specialized into a \",(0,t.jsxDEV)(n.strong,{children:\"spatially invariant style y\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:25,columnNumber:431},this),\" through learned affine transformations. This style y controls the \",(0,t.jsxDEV)(n.strong,{children:\"Adaptive Instance Normalization (AdaIN)\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:25,columnNumber:529},this),\" at each convolution layer of the generator.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:25,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[(0,t.jsxDEV)(n.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/e57716fe-908c-45b7-b0a1-2b7138bf0d57\",alt:\"AdaIN\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:27,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/bf6ee61d-e861-4476-a3be-cf9a630d9afd\",alt:\"AdaIN Operation\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:28,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:27,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[\"The \",(0,t.jsxDEV)(n.strong,{children:\"AdaIN operation\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:30,columnNumber:5},this),\" normalizes each feature map separately and then scales and biases it using the corresponding scalar components of style y. For each feature map, AdaIN adjusts its scale and shifts based on the style information encoded in y. This process allows the generator to apply complex style patterns to the generated images, influencing everything from color schemes to textures in a coherent and controlled manner. The use of a spatially invariant style ensures the generated image maintains a consistent style throughout.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:30,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[\"To enhance the generator's ability to create stochastic details, \",(0,t.jsxDEV)(n.strong,{children:\"explicit noise inputs\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:32,columnNumber:66},this),\" are implemented. These inputs are single-channel images filled with uncorrelated Gaussian noise. Each layer of the synthesis network receives its own unique noise image, which is scaled for each feature map based on learned scaling factors and then added to the output of its respective convolution layer.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:32,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:(0,t.jsxDEV)(n.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/f23ed6e1-a98e-447c-b603-2e707f8b1ed7\",alt:\"Noise Input\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:34,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:34,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h3,{id:\"quality-of-generated-images\",children:[(0,t.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#quality-of-generated-images\",children:(0,t.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\"},this),\"Quality of Generated Images\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:36,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[\"The improvement in image quality with StyleGAN is demonstrated by the reduction in \",(0,t.jsxDEV)(n.strong,{children:\"FID (Frechet Inception Distances)\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:38,columnNumber:84},this),\" across different generator architectures for the CELEBA-HQ and the new FFHQ datasets. This advancement and the enhancement of image resolution represent significant strides in GAN research.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:38,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h3,{id:\"prior-art\",children:[(0,t.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#prior-art\",children:(0,t.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\"},this),\"Prior Art\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:40,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[\"Previous research focused heavily on \",(0,t.jsxDEV)(n.strong,{children:\"improving the discriminator\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:42,columnNumber:38},this),\", using techniques such as multiple discriminators, multi-resolution discrimination, and self-attention mechanisms. For the generator, research concentrated on fine-tuning the distribution in the input latent space or shaping the latent space, with conditional generators exploring new methods of feeding class identifiers through several layers of the generator.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:42,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h1,{id:\"properties-of-the-style-based-generator\",children:[(0,t.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#properties-of-the-style-based-generator\",children:(0,t.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\"},this),\"Properties of the Style-based Generator\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:44,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h3,{id:\"style-mixing\",children:[(0,t.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#style-mixing\",children:(0,t.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\"},this),\"Style Mixing\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:46,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:(0,t.jsxDEV)(n.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/bb33d2d1-e774-4e62-9f6d-f07bd9e689a5\",alt:\"Style Mixing\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:48,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:48,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[(0,t.jsxDEV)(n.strong,{children:\"Mixing Regularization:\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:50,columnNumber:1},this),\" During the learning process, StyleGAN uses \\u2018mixing\\u2019 as a form of regularization. By mixing styles during training, the model learns to handle a variety of style combinations more effectively. This regularization improves the model's robustness and generalization capabilities.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:50,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[(0,t.jsxDEV)(n.strong,{children:\"Style Mixing:\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:52,columnNumber:1},this),\" During testing, the same mixing process allows for the combination of styles from two different images. This feature enables the generation of images that blend characteristics from multiple source images, resulting in unique and diverse outputs.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:52,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h3,{id:\"stochastic-variation\",children:[(0,t.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#stochastic-variation\",children:(0,t.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\"},this),\"Stochastic Variation\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:54,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[(0,t.jsxDEV)(n.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/20f20554-eee4-495f-a5c9-0ee3018e759f\",alt:\"Stochastic Variation 1\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:56,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/2a99646c-5964-4536-aa8c-f6d6ece78598\",alt:\"Stochastic Variation 2\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:57,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:56,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[(0,t.jsxDEV)(n.strong,{children:\"Noise Inputs and Stochastic Details:\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:59,columnNumber:1},this),\" By adding explicit noise inputs to each layer of the synthesis network, StyleGAN can generate stochastic details in the images. These noise inputs are single-channel images filled with uncorrelated Gaussian noise, scaled by learned factors and added to the output of their respective convolution layers. This process allows the generator to introduce fine-grained details, such as hair strands or skin texture, enhancing the realism of the generated images.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:59,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h3,{id:\"separation-of-global-effects-from-stochasticity\",children:[(0,t.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#separation-of-global-effects-from-stochasticity\",children:(0,t.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\"},this),\"Separation of Global Effects from Stochasticity\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:61,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"In StyleGAN, the model can distinguish between global effects and stochastic details. Global effects, such as the overall shape and structure of the generated objects, are controlled by the style vectors. In contrast, stochastic variations, like the fine details and textures, are influenced by the noise inputs. This separation allows for better control and manipulation of the generated images, enabling users to tweak global features without affecting fine details and vice versa.\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:63,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h1,{id:\"disentanglement-studies\",children:[(0,t.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#disentanglement-studies\",children:(0,t.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\"},this),\"Disentanglement Studies\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:65,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h3,{id:\"perceptual-path-length\",children:[(0,t.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#perceptual-path-length\",children:(0,t.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\"},this),\"Perceptual Path Length\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:67,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[(0,t.jsxDEV)(n.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/ad13296e-8e76-466b-8ee5-0c3b8a797224\",alt:\"Perceptual Path Length 1\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:69,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/298a1098-4644-43e0-b161-b5191c2e8fef\",alt:\"Perceptual Path Length 2\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:70,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/a8571e11-5ea9-4f18-816d-f8cc21fa54e5\",alt:\"Perceptual Path Length 3\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:71,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:69,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[(0,t.jsxDEV)(n.strong,{children:\"Perceptual Path Length:\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:73,columnNumber:1},this),\" This metric measures the perceptual difference between images as they move through the latent space. A lower perceptual path length indicates smoother transitions and better disentanglement of the latent space, meaning that changes in the latent vectors correspond to meaningful changes in the generated images.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:73,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h3,{id:\"linear-separability\",children:[(0,t.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#linear-separability\",children:(0,t.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\"},this),\"Linear Separability\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:75,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[\"StyleGAN\\u2019s architecture improves the \",(0,t.jsxDEV)(n.strong,{children:\"linear separability\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:77,columnNumber:38},this),\" of the latent space, which makes it easier to isolate and manipulate individual features of the generated images. This characteristic is particularly valuable for applications requiring specific attribute modifications, such as changing the hair color or facial expression in generated portraits.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:77,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h1,{id:\"conclusion\",children:[(0,t.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#conclusion\",children:(0,t.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\"},this),\"Conclusion\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:79,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"StyleGAN represents a significant advancement in GAN technology. By introducing a style-based generator and progressive growing, it achieves high-resolution, high-quality image generation with improved stability. The model's ability to mix styles, generate stochastic details, and separate global effects from fine details allows for unprecedented control and flexibility in image generation. The improvements in perceptual path length and linear separability further enhance the quality and usability of the generated images, making StyleGAN a powerful tool in the field of generative models.\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:81,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\",lineNumber:1,columnNumber:1},this)}function _n(c={}){let{wrapper:n}=c.components||{};return n?(0,t.jsxDEV)(n,Object.assign({},c,{children:(0,t.jsxDEV)(Ee,c,void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\"},this)}),void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-c4710dd6-fe50-46d6-9aa8-1604c30ae8fc.mdx\"},this):Ee(c)}var yn=_n;return gn(Nn);})();\n/*! Bundled license information:\n\nreact/cjs/react-jsx-dev-runtime.development.js:\n  (**\n   * @license React\n   * react-jsx-dev-runtime.development.js\n   *\n   * Copyright (c) Facebook, Inc. and its affiliates.\n   *\n   * This source code is licensed under the MIT license found in the\n   * LICENSE file in the root directory of this source tree.\n   *)\n*/\n;return Component;"
  },
  "_id": "stylegan/index.mdx",
  "_raw": {
    "sourceFilePath": "stylegan/index.mdx",
    "sourceFileName": "index.mdx",
    "sourceFileDir": "stylegan",
    "contentType": "mdx",
    "flattenedPath": "stylegan"
  },
  "type": "Blog",
  "url": "/blogs/stylegan",
  "readingTime": {
    "text": "5 min read",
    "minutes": 4.63,
    "time": 277800,
    "words": 926
  },
  "toc": [
    {
      "level": "one",
      "text": "PGGAN: Progressive Growing of GANs",
      "slug": "pggan-progressive-growing-of-gans"
    },
    {
      "level": "one",
      "text": "Style-based Generator: StyleGAN",
      "slug": "style-based-generator-stylegan"
    },
    {
      "level": "three",
      "text": "Quality of Generated Images",
      "slug": "quality-of-generated-images"
    },
    {
      "level": "three",
      "text": "Prior Art",
      "slug": "prior-art"
    },
    {
      "level": "one",
      "text": "Properties of the Style-based Generator",
      "slug": "properties-of-the-style-based-generator"
    },
    {
      "level": "three",
      "text": "Style Mixing",
      "slug": "style-mixing"
    },
    {
      "level": "three",
      "text": "Stochastic Variation",
      "slug": "stochastic-variation"
    },
    {
      "level": "three",
      "text": "Separation of Global Effects from Stochasticity",
      "slug": "separation-of-global-effects-from-stochasticity"
    },
    {
      "level": "one",
      "text": "Disentanglement Studies",
      "slug": "disentanglement-studies"
    },
    {
      "level": "three",
      "text": "Perceptual Path Length",
      "slug": "perceptual-path-length"
    },
    {
      "level": "three",
      "text": "Linear Separability",
      "slug": "linear-separability"
    },
    {
      "level": "one",
      "text": "Conclusion",
      "slug": "conclusion"
    }
  ]
}