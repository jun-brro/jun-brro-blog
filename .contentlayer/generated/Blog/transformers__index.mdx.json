{
  "title": "Transformers - Attention is All You Need",
  "publishedAt": "2024-07-10T00:00:00.000Z",
  "updatedAt": "2024-07-10T00:00:00.000Z",
  "description": "Transformers are a deep learning architecture that enhances natural language processing by using self-attention mechanisms to capture long-range dependencies and contextual relationships in text.",
  "image": {
    "filePath": "../public/blogs/transformers/screenshot.png",
    "relativeFilePath": "../../public/blogs/transformers/screenshot.png",
    "format": "png",
    "height": 1226,
    "width": 1616,
    "aspectRatio": 1.3181076672104404,
    "blurhashDataUrl": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAJFBMVEX////39vbHzczm4+PU0Mfx8PD08/LBw73JwK7a1tWwvrzf29QmaxtBAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAMUlEQVR4nC2Lxw0AMAwCcY+T/feNXPgcCAAAI4xChn65QMxHO8ca06fZVZrM2F3q/gESnACVhwl6nAAAAABJRU5ErkJggg=="
  },
  "isPublished": true,
  "author": "junbrro",
  "tags": [
    "Deep Learning"
  ],
  "body": {
    "raw": "\n# Introduction\n\n**Transformer:** entirely based on attention mechanism (no use of recurrence or convolution → Therefore, less training time and calculations)\n\n**SOTA:** State of the Art (The highest result)\n\n### Limit of Recurrent Model\n\n- Parallelization is impossible (all sequences should be processed sequently)\n- Extremely low ability when long sequence length. (The longer, the worse)\n\n→ “The fundamental constraint of sequential computation” remains in case of recurrent model.\n\n→ Transformers model is proposed, based on attention mechanism.\n\n# Model Architecture\n\n![](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/81e6ddca-ceca-4905-ac7a-dfa0d595e461)\n\n### Encoder\n\n- **Structure**: Consists of 6 identical layers, each with two sub-layers: a multi-head self-attention mechanism and a positionwise fully connected feed-forward network.\n- **Residual Connections and Normalization**: Each sub-layer's output is added to its input (residual connection) and normalized, enhancing training stability.\n- **Dimensionality**: Outputs from sub-layers and embedding layers are dimensionally consistent at *d*model=512.\n\n### Decoder\n\n- **Structure**: Mirrors the encoder with 6 identical layers, but includes an additional third sub-layer for multi-head attention over the encoder's output.\n- **Masked Self-Attention**: The first sub-layer uses masked self-attention to ensure predictions for a position depend only on earlier positions.\n- **Consistency and Adaptations**: Applies residual connections and layer normalization like the encoder, maintaining output dimensionality consistency.\n\n### Attention (Scaled Dot-Product Attention)\n\n![](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/801463d1-7795-4fe9-8c01-8b8b8a105f3c)\nAn attention mechanism maps a query along with a collection of key-value pairs to an output, with the query, keys, values, and the output all being represented as vectors.\n\nAttention = Mapping Query & Key-Value pair to output (Output is calculated as weighted sum of values)\n\n![](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/80646f25-8912-431f-86c4-f83e9ddd4691)\n**Additive Attention:** Calculate the compatibility function using a feed-forward layer network with a single hidden layer\n\n**Dot-Product Attention:** The Attention method used in this study\n\n### Multi-Head Attention\n\n![](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/d4a77ddc-95c9-49a8-85fb-927c7e4afc33)\n→ Since the dimension is reduced at each head, total calculation cost is similar to that of single-head attention.\n\nIt was found out that linearly projecting to dk, dv, using differently trained (h-times trained) linear projections. Performing attention in parallel on these projections yields _dv_-dimensional outputs, addressing the scale issue in dot products between queries and keys.\n\n### Applications of Attentions (Multi-head attention)\n\n- In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.\n- The encoder contains self-attention layers.\n- Self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.\n\n### Position-wise Feedforward Networks\n\nAlthough the linear transformations are same for different positions, the parameters are different at each layers.\n\n![](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/46db30d9-fb54-4826-8ece-74dba40e75ec)\n\n### Embeddings and Softmax\n\n- Using trained embeddings, in order to convert input and ouput tokens to dmodel vector.\n- In the Transformer, to convert the decoder output into probabilities for the next token prediction, a linear transformation followed by a softmax is used. The model shares the same weight matrix across the two embedding layers and the pre-softmax linear transformation, with the embedding layers' weights being scaled by dmodel.\n\n![](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/815a5759-166a-4552-9f4c-494eae2e02fe)\n\n### [Additional] Embeddings vs Encodings\n\n- **Embeddings**:\n  - **Learnable**: Embeddings are vectors learned from data during the model training process. They can be optimized for specific tasks to learn semantic representations.\n  - **Semantic Relationship Learning**: Embedding vectors are trained such that words or entities with similar meanings are located close to each other in the vector space.\n  - **High-Dimensional Dense Vectors**: Embeddings typically represent data in dense vectors of several hundred to thousands of dimensions. Each dimension can represent specific semantic attributes.\n- **Encodings**:\n  - **Fixed Representations**: Encodings often transform data into vectors using predefined methods. They tend to remain unchanged during the training process (e.g., one-hot encoding).\n  - **Structural/Positional Information**: Encodings can provide structural or positional information to the model.\n  - **Sparse or Dense Vectors**: Encodings can take the form of sparse vectors (e.g., one-hot encoding) or dense vectors (e.g., positional encodings), depending on the encoding method used.\n\n[Understanding Differences Between Encoding and Embedding](https://www.linkedin.com/pulse/understanding-differences-between-encoding-embedding-mba-ms-phd/)\n\n### Positional Encoding\n\n- Since Transformers does not use any of recurrent process or convolution, it is necessary to input the ‘positional data’ to train the order of sequences.\n\n![](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/1f65ac3b-e113-49ec-88e7-4888914b87f4)\n\n**Question: Why using sin, cos functions for positional encoding?**\n\n- The position vector must not explode, to prevent data deterioration. Sin&cos function value is from -1 to 1, therefore satisfying the condition.\n- Sine and cosine functions are periodic functions. For the sigmoid function, with long sequences of sentences, the difference in position vector values can become negligible. However, for trigonometric functions (sine & cosine), since they oscillate periodically between -1 and 1, even with long sequences of sentences, the difference in position vector values can be maintained significantly.\n\n![](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/b58e91fa-ac46-4e5e-810d-1262d3e2fe38)\n\n- If there is only a single sine or cosine function, the position vector value might be equal, for each different token. To prevent this case, sine and cosine functions are used with various periods. Therefore, the word vector can have different position vector for each dimensions.\n\n![](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/45f062d0-17f7-4478-98f9-441b86f28cb8)\n\n[트랜스포머(Transformer) 파헤치기—1. Positional Encoding](https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding)\n\n# Why Self-Attention?\n\n1. Total Computational complexity per layer\n   - Self-Attention Layers lower total computational complexity per layer. This happens especially when sequence length is less than the dimensionality of the representations.\n2. Amount of computation that can be parallelized\n   - Self-attention allows for greater amount of computations that can be parallelized, minimizing the number of required sequential operations and reducing training & inference time.\n3. The path length between long-range dependencies in the network\n   - Self-attention mechanism shorten the path length, facilitating easier, more efficient learning of dependencies.\n\n# Training / Result\n\n### Training Setting\n\n- Optimizer: Adam\n\n![](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/f267f567-6357-48ad-98b3-1304ec558284)\n\n- Three types of Regularization\n  - Residual Dropout\n    - Applying Dropout to outputs of each sublayers\n    - Applying Dropout to the sum of embeddings and positional encoding\n  - Label Smoothing\n\n![](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/5aa0389a-e172-44e7-86ae-078fcb95fac4)\n\n### Result\n\n![](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/c47159de-d168-4e12-b208-4165d10ee2e7)\n![](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/914a0020-0bc7-4401-814e-cdaa2dfca022)\n",
    "code": "var Component=(()=>{var sn=Object.create;var C=Object.defineProperty;var an=Object.getOwnPropertyDescriptor;var bn=Object.getOwnPropertyNames;var mn=Object.getPrototypeOf,fn=Object.prototype.hasOwnProperty;var q=(a,n)=>()=>(n||a((n={exports:{}}).exports,n),n.exports),cn=(a,n)=>{for(var _ in n)C(a,_,{get:n[_],enumerable:!0})},xe=(a,n,_,N)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let y of bn(n))!fn.call(a,y)&&y!==_&&C(a,y,{get:()=>n[y],enumerable:!(N=an(n,y))||N.enumerable});return a};var hn=(a,n,_)=>(_=a!=null?sn(mn(a)):{},xe(n||!a||!a.__esModule?C(_,\"default\",{value:a,enumerable:!0}):_,a)),_n=a=>xe(C({},\"__esModule\",{value:!0}),a);var ke=q((jn,je)=>{je.exports=React});var ve=q(z=>{\"use strict\";(function(){\"use strict\";var a=ke(),n=Symbol.for(\"react.element\"),_=Symbol.for(\"react.portal\"),N=Symbol.for(\"react.fragment\"),y=Symbol.for(\"react.strict_mode\"),B=Symbol.for(\"react.profiler\"),K=Symbol.for(\"react.provider\"),X=Symbol.for(\"react.context\"),G=Symbol.for(\"react.forward_ref\"),P=Symbol.for(\"react.suspense\"),O=Symbol.for(\"react.suspense_list\"),H=Symbol.for(\"react.memo\"),A=Symbol.for(\"react.lazy\"),He=Symbol.for(\"react.offscreen\"),Q=Symbol.iterator,we=\"@@iterator\";function Ee(e){if(e===null||typeof e!=\"object\")return null;var t=Q&&e[Q]||e[we];return typeof t==\"function\"?t:null}var j=a.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED;function f(e){{for(var t=arguments.length,i=new Array(t>1?t-1:0),d=1;d<t;d++)i[d-1]=arguments[d];Te(\"error\",e,i)}}function Te(e,t,i){{var d=j.ReactDebugCurrentFrame,l=d.getStackAddendum();l!==\"\"&&(t+=\"%s\",i=i.concat([l]));var s=i.map(function(u){return String(u)});s.unshift(\"Warning: \"+t),Function.prototype.apply.call(console[e],console,s)}}var Re=!1,Se=!1,Ce=!1,Pe=!1,Oe=!1,J;J=Symbol.for(\"react.module.reference\");function Ae(e){return!!(typeof e==\"string\"||typeof e==\"function\"||e===N||e===B||Oe||e===y||e===P||e===O||Pe||e===He||Re||Se||Ce||typeof e==\"object\"&&e!==null&&(e.$$typeof===A||e.$$typeof===H||e.$$typeof===K||e.$$typeof===X||e.$$typeof===G||e.$$typeof===J||e.getModuleId!==void 0))}function Ie(e,t,i){var d=e.displayName;if(d)return d;var l=t.displayName||t.name||\"\";return l!==\"\"?i+\"(\"+l+\")\":i}function Z(e){return e.displayName||\"Context\"}function p(e){if(e==null)return null;if(typeof e.tag==\"number\"&&f(\"Received an unexpected object in getComponentNameFromType(). This is likely a bug in React. Please file an issue.\"),typeof e==\"function\")return e.displayName||e.name||null;if(typeof e==\"string\")return e;switch(e){case N:return\"Fragment\";case _:return\"Portal\";case B:return\"Profiler\";case y:return\"StrictMode\";case P:return\"Suspense\";case O:return\"SuspenseList\"}if(typeof e==\"object\")switch(e.$$typeof){case X:var t=e;return Z(t)+\".Consumer\";case K:var i=e;return Z(i._context)+\".Provider\";case G:return Ie(e,e.render,\"ForwardRef\");case H:var d=e.displayName||null;return d!==null?d:p(e.type)||\"Memo\";case A:{var l=e,s=l._payload,u=l._init;try{return p(u(s))}catch{return null}}}return null}var x=Object.assign,D=0,ee,ne,re,te,ie,de,oe;function ue(){}ue.__reactDisabledLog=!0;function Fe(){{if(D===0){ee=console.log,ne=console.info,re=console.warn,te=console.error,ie=console.group,de=console.groupCollapsed,oe=console.groupEnd;var e={configurable:!0,enumerable:!0,value:ue,writable:!0};Object.defineProperties(console,{info:e,log:e,warn:e,error:e,group:e,groupCollapsed:e,groupEnd:e})}D++}}function Me(){{if(D--,D===0){var e={configurable:!0,enumerable:!0,writable:!0};Object.defineProperties(console,{log:x({},e,{value:ee}),info:x({},e,{value:ne}),warn:x({},e,{value:re}),error:x({},e,{value:te}),group:x({},e,{value:ie}),groupCollapsed:x({},e,{value:de}),groupEnd:x({},e,{value:oe})})}D<0&&f(\"disabledDepth fell below zero. This is a bug in React. Please file an issue.\")}}var I=j.ReactCurrentDispatcher,F;function w(e,t,i){{if(F===void 0)try{throw Error()}catch(l){var d=l.stack.trim().match(/\\n( *(at )?)/);F=d&&d[1]||\"\"}return`\n`+F+e}}var M=!1,E;{var We=typeof WeakMap==\"function\"?WeakMap:Map;E=new We}function le(e,t){if(!e||M)return\"\";{var i=E.get(e);if(i!==void 0)return i}var d;M=!0;var l=Error.prepareStackTrace;Error.prepareStackTrace=void 0;var s;s=I.current,I.current=null,Fe();try{if(t){var u=function(){throw Error()};if(Object.defineProperty(u.prototype,\"props\",{set:function(){throw Error()}}),typeof Reflect==\"object\"&&Reflect.construct){try{Reflect.construct(u,[])}catch(g){d=g}Reflect.construct(e,[],u)}else{try{u.call()}catch(g){d=g}e.call(u.prototype)}}else{try{throw Error()}catch(g){d=g}e()}}catch(g){if(g&&d&&typeof g.stack==\"string\"){for(var o=g.stack.split(`\n`),c=d.stack.split(`\n`),b=o.length-1,m=c.length-1;b>=1&&m>=0&&o[b]!==c[m];)m--;for(;b>=1&&m>=0;b--,m--)if(o[b]!==c[m]){if(b!==1||m!==1)do if(b--,m--,m<0||o[b]!==c[m]){var h=`\n`+o[b].replace(\" at new \",\" at \");return e.displayName&&h.includes(\"<anonymous>\")&&(h=h.replace(\"<anonymous>\",e.displayName)),typeof e==\"function\"&&E.set(e,h),h}while(b>=1&&m>=0);break}}}finally{M=!1,I.current=s,Me(),Error.prepareStackTrace=l}var v=e?e.displayName||e.name:\"\",Ne=v?w(v):\"\";return typeof e==\"function\"&&E.set(e,Ne),Ne}function Ye(e,t,i){return le(e,!1)}function Le(e){var t=e.prototype;return!!(t&&t.isReactComponent)}function T(e,t,i){if(e==null)return\"\";if(typeof e==\"function\")return le(e,Le(e));if(typeof e==\"string\")return w(e);switch(e){case P:return w(\"Suspense\");case O:return w(\"SuspenseList\")}if(typeof e==\"object\")switch(e.$$typeof){case G:return Ye(e.render);case H:return T(e.type,t,i);case A:{var d=e,l=d._payload,s=d._init;try{return T(s(l),t,i)}catch{}}}return\"\"}var R=Object.prototype.hasOwnProperty,se={},ae=j.ReactDebugCurrentFrame;function S(e){if(e){var t=e._owner,i=T(e.type,e._source,t?t.type:null);ae.setExtraStackFrame(i)}else ae.setExtraStackFrame(null)}function Ve(e,t,i,d,l){{var s=Function.call.bind(R);for(var u in e)if(s(e,u)){var o=void 0;try{if(typeof e[u]!=\"function\"){var c=Error((d||\"React class\")+\": \"+i+\" type `\"+u+\"` is invalid; it must be a function, usually from the `prop-types` package, but received `\"+typeof e[u]+\"`.This often happens because of typos such as `PropTypes.function` instead of `PropTypes.func`.\");throw c.name=\"Invariant Violation\",c}o=e[u](t,u,d,i,null,\"SECRET_DO_NOT_PASS_THIS_OR_YOU_WILL_BE_FIRED\")}catch(b){o=b}o&&!(o instanceof Error)&&(S(l),f(\"%s: type specification of %s `%s` is invalid; the type checker function must return `null` or an `Error` but returned a %s. You may have forgotten to pass an argument to the type checker creator (arrayOf, instanceOf, objectOf, oneOf, oneOfType, and shape all require an argument).\",d||\"React class\",i,u,typeof o),S(null)),o instanceof Error&&!(o.message in se)&&(se[o.message]=!0,S(l),f(\"Failed %s type: %s\",i,o.message),S(null))}}}var $e=Array.isArray;function W(e){return $e(e)}function qe(e){{var t=typeof Symbol==\"function\"&&Symbol.toStringTag,i=t&&e[Symbol.toStringTag]||e.constructor.name||\"Object\";return i}}function ze(e){try{return be(e),!1}catch{return!0}}function be(e){return\"\"+e}function me(e){if(ze(e))return f(\"The provided key is an unsupported type %s. This value must be coerced to a string before before using it here.\",qe(e)),be(e)}var U=j.ReactCurrentOwner,Be={key:!0,ref:!0,__self:!0,__source:!0},fe,ce,Y;Y={};function Ke(e){if(R.call(e,\"ref\")){var t=Object.getOwnPropertyDescriptor(e,\"ref\").get;if(t&&t.isReactWarning)return!1}return e.ref!==void 0}function Xe(e){if(R.call(e,\"key\")){var t=Object.getOwnPropertyDescriptor(e,\"key\").get;if(t&&t.isReactWarning)return!1}return e.key!==void 0}function Qe(e,t){if(typeof e.ref==\"string\"&&U.current&&t&&U.current.stateNode!==t){var i=p(U.current.type);Y[i]||(f('Component \"%s\" contains the string ref \"%s\". Support for string refs will be removed in a future major release. This case cannot be automatically converted to an arrow function. We ask you to manually fix this case by using useRef() or createRef() instead. Learn more about using refs safely here: https://reactjs.org/link/strict-mode-string-ref',p(U.current.type),e.ref),Y[i]=!0)}}function Je(e,t){{var i=function(){fe||(fe=!0,f(\"%s: `key` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\",t))};i.isReactWarning=!0,Object.defineProperty(e,\"key\",{get:i,configurable:!0})}}function Ze(e,t){{var i=function(){ce||(ce=!0,f(\"%s: `ref` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\",t))};i.isReactWarning=!0,Object.defineProperty(e,\"ref\",{get:i,configurable:!0})}}var en=function(e,t,i,d,l,s,u){var o={$$typeof:n,type:e,key:t,ref:i,props:u,_owner:s};return o._store={},Object.defineProperty(o._store,\"validated\",{configurable:!1,enumerable:!1,writable:!0,value:!1}),Object.defineProperty(o,\"_self\",{configurable:!1,enumerable:!1,writable:!1,value:d}),Object.defineProperty(o,\"_source\",{configurable:!1,enumerable:!1,writable:!1,value:l}),Object.freeze&&(Object.freeze(o.props),Object.freeze(o)),o};function nn(e,t,i,d,l){{var s,u={},o=null,c=null;i!==void 0&&(me(i),o=\"\"+i),Xe(t)&&(me(t.key),o=\"\"+t.key),Ke(t)&&(c=t.ref,Qe(t,l));for(s in t)R.call(t,s)&&!Be.hasOwnProperty(s)&&(u[s]=t[s]);if(e&&e.defaultProps){var b=e.defaultProps;for(s in b)u[s]===void 0&&(u[s]=b[s])}if(o||c){var m=typeof e==\"function\"?e.displayName||e.name||\"Unknown\":e;o&&Je(u,m),c&&Ze(u,m)}return en(e,o,c,l,d,U.current,u)}}var L=j.ReactCurrentOwner,he=j.ReactDebugCurrentFrame;function k(e){if(e){var t=e._owner,i=T(e.type,e._source,t?t.type:null);he.setExtraStackFrame(i)}else he.setExtraStackFrame(null)}var V;V=!1;function $(e){return typeof e==\"object\"&&e!==null&&e.$$typeof===n}function _e(){{if(L.current){var e=p(L.current.type);if(e)return`\n\nCheck the render method of \\``+e+\"`.\"}return\"\"}}function rn(e){{if(e!==void 0){var t=e.fileName.replace(/^.*[\\\\\\/]/,\"\"),i=e.lineNumber;return`\n\nCheck your code at `+t+\":\"+i+\".\"}return\"\"}}var pe={};function tn(e){{var t=_e();if(!t){var i=typeof e==\"string\"?e:e.displayName||e.name;i&&(t=`\n\nCheck the top-level render call using <`+i+\">.\")}return t}}function ge(e,t){{if(!e._store||e._store.validated||e.key!=null)return;e._store.validated=!0;var i=tn(t);if(pe[i])return;pe[i]=!0;var d=\"\";e&&e._owner&&e._owner!==L.current&&(d=\" It was passed a child from \"+p(e._owner.type)+\".\"),k(e),f('Each child in a list should have a unique \"key\" prop.%s%s See https://reactjs.org/link/warning-keys for more information.',i,d),k(null)}}function ye(e,t){{if(typeof e!=\"object\")return;if(W(e))for(var i=0;i<e.length;i++){var d=e[i];$(d)&&ge(d,t)}else if($(e))e._store&&(e._store.validated=!0);else if(e){var l=Ee(e);if(typeof l==\"function\"&&l!==e.entries)for(var s=l.call(e),u;!(u=s.next()).done;)$(u.value)&&ge(u.value,t)}}}function dn(e){{var t=e.type;if(t==null||typeof t==\"string\")return;var i;if(typeof t==\"function\")i=t.propTypes;else if(typeof t==\"object\"&&(t.$$typeof===G||t.$$typeof===H))i=t.propTypes;else return;if(i){var d=p(t);Ve(i,e.props,\"prop\",d,e)}else if(t.PropTypes!==void 0&&!V){V=!0;var l=p(t);f(\"Component %s declared `PropTypes` instead of `propTypes`. Did you misspell the property assignment?\",l||\"Unknown\")}typeof t.getDefaultProps==\"function\"&&!t.getDefaultProps.isReactClassApproved&&f(\"getDefaultProps is only used on classic React.createClass definitions. Use a static property named `defaultProps` instead.\")}}function on(e){{for(var t=Object.keys(e.props),i=0;i<t.length;i++){var d=t[i];if(d!==\"children\"&&d!==\"key\"){k(e),f(\"Invalid prop `%s` supplied to `React.Fragment`. React.Fragment can only have `key` and `children` props.\",d),k(null);break}}e.ref!==null&&(k(e),f(\"Invalid attribute `ref` supplied to `React.Fragment`.\"),k(null))}}function un(e,t,i,d,l,s){{var u=Ae(e);if(!u){var o=\"\";(e===void 0||typeof e==\"object\"&&e!==null&&Object.keys(e).length===0)&&(o+=\" You likely forgot to export your component from the file it's defined in, or you might have mixed up default and named imports.\");var c=rn(l);c?o+=c:o+=_e();var b;e===null?b=\"null\":W(e)?b=\"array\":e!==void 0&&e.$$typeof===n?(b=\"<\"+(p(e.type)||\"Unknown\")+\" />\",o=\" Did you accidentally export a JSX literal instead of a component?\"):b=typeof e,f(\"React.jsx: type is invalid -- expected a string (for built-in components) or a class/function (for composite components) but got: %s.%s\",b,o)}var m=nn(e,t,i,l,s);if(m==null)return m;if(u){var h=t.children;if(h!==void 0)if(d)if(W(h)){for(var v=0;v<h.length;v++)ye(h[v],e);Object.freeze&&Object.freeze(h)}else f(\"React.jsx: Static children should always be an array. You are likely explicitly calling React.jsxs or React.jsxDEV. Use the Babel transform instead.\");else ye(h,e)}return e===N?on(m):dn(m),m}}var ln=un;z.Fragment=N,z.jsxDEV=ln})()});var Ue=q((vn,De)=>{\"use strict\";De.exports=ve()});var Nn={};cn(Nn,{default:()=>yn,frontmatter:()=>pn});var r=hn(Ue()),pn={title:\"Transformers - Attention is All You Need\",description:\"Transformers are a deep learning architecture that enhances natural language processing by using self-attention mechanisms to capture long-range dependencies and contextual relationships in text.\",image:\"../../public/blogs/transformers/screenshot.png\",publishedAt:\"2024-07-10\",updatedAt:\"2024-07-10\",author:\"junbrro\",isPublished:!0,tags:[\"Deep Learning\"]};function Ge(a){let n=Object.assign({h1:\"h1\",a:\"a\",span:\"span\",p:\"p\",strong:\"strong\",h3:\"h3\",ul:\"ul\",li:\"li\",img:\"img\",em:\"em\",ol:\"ol\"},a.components);return(0,r.jsxDEV)(r.Fragment,{children:[(0,r.jsxDEV)(n.h1,{id:\"introduction\",children:[(0,r.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#introduction\",children:(0,r.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this),\"Introduction\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:13,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.p,{children:[(0,r.jsxDEV)(n.strong,{children:\"Transformer:\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:15,columnNumber:1},this),\" entirely based on attention mechanism (no use of recurrence or convolution \\u2192 Therefore, less training time and calculations)\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:15,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.p,{children:[(0,r.jsxDEV)(n.strong,{children:\"SOTA:\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:17,columnNumber:1},this),\" State of the Art (The highest result)\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:17,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.h3,{id:\"limit-of-recurrent-model\",children:[(0,r.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#limit-of-recurrent-model\",children:(0,r.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this),\"Limit of Recurrent Model\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:19,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.ul,{children:[`\n`,(0,r.jsxDEV)(n.li,{children:\"Parallelization is impossible (all sequences should be processed sequently)\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:21,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.li,{children:\"Extremely low ability when long sequence length. (The longer, the worse)\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:22,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:21,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.p,{children:\"\\u2192 \\u201CThe fundamental constraint of sequential computation\\u201D remains in case of recurrent model.\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:24,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.p,{children:\"\\u2192 Transformers model is proposed, based on attention mechanism.\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:26,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.h1,{id:\"model-architecture\",children:[(0,r.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#model-architecture\",children:(0,r.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this),\"Model Architecture\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:28,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/81e6ddca-ceca-4905-ac7a-dfa0d595e461\",alt:\"\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:30,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:30,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.h3,{id:\"encoder\",children:[(0,r.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#encoder\",children:(0,r.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this),\"Encoder\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:32,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.ul,{children:[`\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\"Structure\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:34,columnNumber:3},this),\": Consists of 6 identical layers, each with two sub-layers: a multi-head self-attention mechanism and a positionwise fully connected feed-forward network.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:34,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\"Residual Connections and Normalization\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:35,columnNumber:3},this),\": Each sub-layer's output is added to its input (residual connection) and normalized, enhancing training stability.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:35,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\"Dimensionality\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:36,columnNumber:3},this),\": Outputs from sub-layers and embedding layers are dimensionally consistent at \",(0,r.jsxDEV)(n.em,{children:\"d\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:36,columnNumber:100},this),\"model=512.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:36,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:34,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.h3,{id:\"decoder\",children:[(0,r.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#decoder\",children:(0,r.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this),\"Decoder\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:38,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.ul,{children:[`\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\"Structure\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:40,columnNumber:3},this),\": Mirrors the encoder with 6 identical layers, but includes an additional third sub-layer for multi-head attention over the encoder's output.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:40,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\"Masked Self-Attention\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:41,columnNumber:3},this),\": The first sub-layer uses masked self-attention to ensure predictions for a position depend only on earlier positions.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:41,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\"Consistency and Adaptations\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:42,columnNumber:3},this),\": Applies residual connections and layer normalization like the encoder, maintaining output dimensionality consistency.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:42,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:40,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.h3,{id:\"attention-scaled-dot-product-attention\",children:[(0,r.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#attention-scaled-dot-product-attention\",children:(0,r.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this),\"Attention (Scaled Dot-Product Attention)\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:44,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.p,{children:[(0,r.jsxDEV)(n.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/801463d1-7795-4fe9-8c01-8b8b8a105f3c\",alt:\"\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:46,columnNumber:1},this),`\nAn attention mechanism maps a query along with a collection of key-value pairs to an output, with the query, keys, values, and the output all being represented as vectors.`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:46,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.p,{children:\"Attention = Mapping Query & Key-Value pair to output (Output is calculated as weighted sum of values)\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:49,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.p,{children:[(0,r.jsxDEV)(n.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/80646f25-8912-431f-86c4-f83e9ddd4691\",alt:\"\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:51,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.strong,{children:\"Additive Attention:\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:52,columnNumber:1},this),\" Calculate the compatibility function using a feed-forward layer network with a single hidden layer\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:51,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.p,{children:[(0,r.jsxDEV)(n.strong,{children:\"Dot-Product Attention:\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:54,columnNumber:1},this),\" The Attention method used in this study\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:54,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.h3,{id:\"multi-head-attention\",children:[(0,r.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#multi-head-attention\",children:(0,r.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this),\"Multi-Head Attention\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:56,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.p,{children:[(0,r.jsxDEV)(n.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/d4a77ddc-95c9-49a8-85fb-927c7e4afc33\",alt:\"\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:58,columnNumber:1},this),`\n\\u2192 Since the dimension is reduced at each head, total calculation cost is similar to that of single-head attention.`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:58,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.p,{children:[\"It was found out that linearly projecting to dk, dv, using differently trained (h-times trained) linear projections. Performing attention in parallel on these projections yields \",(0,r.jsxDEV)(n.em,{children:\"dv\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:61,columnNumber:179},this),\"-dimensional outputs, addressing the scale issue in dot products between queries and keys.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:61,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.h3,{id:\"applications-of-attentions-multi-head-attention\",children:[(0,r.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#applications-of-attentions-multi-head-attention\",children:(0,r.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this),\"Applications of Attentions (Multi-head attention)\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:63,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.ul,{children:[`\n`,(0,r.jsxDEV)(n.li,{children:'In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.'},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:65,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.li,{children:\"The encoder contains self-attention layers.\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:66,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.li,{children:\"Self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:67,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:65,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.h3,{id:\"position-wise-feedforward-networks\",children:[(0,r.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#position-wise-feedforward-networks\",children:(0,r.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this),\"Position-wise Feedforward Networks\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:69,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.p,{children:\"Although the linear transformations are same for different positions, the parameters are different at each layers.\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:71,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/46db30d9-fb54-4826-8ece-74dba40e75ec\",alt:\"\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:73,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:73,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.h3,{id:\"embeddings-and-softmax\",children:[(0,r.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#embeddings-and-softmax\",children:(0,r.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this),\"Embeddings and Softmax\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:75,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.ul,{children:[`\n`,(0,r.jsxDEV)(n.li,{children:\"Using trained embeddings, in order to convert input and ouput tokens to dmodel vector.\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:77,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.li,{children:\"In the Transformer, to convert the decoder output into probabilities for the next token prediction, a linear transformation followed by a softmax is used. The model shares the same weight matrix across the two embedding layers and the pre-softmax linear transformation, with the embedding layers' weights being scaled by dmodel.\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:78,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:77,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/815a5759-166a-4552-9f4c-494eae2e02fe\",alt:\"\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:80,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:80,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.h3,{id:\"additional-embeddings-vs-encodings\",children:[(0,r.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#additional-embeddings-vs-encodings\",children:(0,r.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this),\"[Additional] Embeddings vs Encodings\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:82,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.ul,{children:[`\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\"Embeddings\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:84,columnNumber:3},this),\":\",`\n`,(0,r.jsxDEV)(n.ul,{children:[`\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\"Learnable\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:85,columnNumber:5},this),\": Embeddings are vectors learned from data during the model training process. They can be optimized for specific tasks to learn semantic representations.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:85,columnNumber:3},this),`\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\"Semantic Relationship Learning\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:86,columnNumber:5},this),\": Embedding vectors are trained such that words or entities with similar meanings are located close to each other in the vector space.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:86,columnNumber:3},this),`\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\"High-Dimensional Dense Vectors\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:87,columnNumber:5},this),\": Embeddings typically represent data in dense vectors of several hundred to thousands of dimensions. Each dimension can represent specific semantic attributes.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:87,columnNumber:3},this),`\n`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:85,columnNumber:3},this),`\n`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:84,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\"Encodings\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:88,columnNumber:3},this),\":\",`\n`,(0,r.jsxDEV)(n.ul,{children:[`\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\"Fixed Representations\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:89,columnNumber:5},this),\": Encodings often transform data into vectors using predefined methods. They tend to remain unchanged during the training process (e.g., one-hot encoding).\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:89,columnNumber:3},this),`\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\"Structural/Positional Information\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:90,columnNumber:5},this),\": Encodings can provide structural or positional information to the model.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:90,columnNumber:3},this),`\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\"Sparse or Dense Vectors\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:91,columnNumber:5},this),\": Encodings can take the form of sparse vectors (e.g., one-hot encoding) or dense vectors (e.g., positional encodings), depending on the encoding method used.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:91,columnNumber:3},this),`\n`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:89,columnNumber:3},this),`\n`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:88,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:84,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.a,{href:\"https://www.linkedin.com/pulse/understanding-differences-between-encoding-embedding-mba-ms-phd/\",children:\"Understanding Differences Between Encoding and Embedding\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:93,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:93,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.h3,{id:\"positional-encoding\",children:[(0,r.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#positional-encoding\",children:(0,r.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this),\"Positional Encoding\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:95,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.ul,{children:[`\n`,(0,r.jsxDEV)(n.li,{children:\"Since Transformers does not use any of recurrent process or convolution, it is necessary to input the \\u2018positional data\\u2019 to train the order of sequences.\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:97,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:97,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/1f65ac3b-e113-49ec-88e7-4888914b87f4\",alt:\"\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:99,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:99,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.strong,{children:\"Question: Why using sin, cos functions for positional encoding?\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:101,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:101,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.ul,{children:[`\n`,(0,r.jsxDEV)(n.li,{children:\"The position vector must not explode, to prevent data deterioration. Sin&cos function value is from -1 to 1, therefore satisfying the condition.\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:103,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.li,{children:\"Sine and cosine functions are periodic functions. For the sigmoid function, with long sequences of sentences, the difference in position vector values can become negligible. However, for trigonometric functions (sine & cosine), since they oscillate periodically between -1 and 1, even with long sequences of sentences, the difference in position vector values can be maintained significantly.\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:104,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:103,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/b58e91fa-ac46-4e5e-810d-1262d3e2fe38\",alt:\"\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:106,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:106,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.ul,{children:[`\n`,(0,r.jsxDEV)(n.li,{children:\"If there is only a single sine or cosine function, the position vector value might be equal, for each different token. To prevent this case, sine and cosine functions are used with various periods. Therefore, the word vector can have different position vector for each dimensions.\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:108,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:108,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/45f062d0-17f7-4478-98f9-441b86f28cb8\",alt:\"\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:110,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:110,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.a,{href:\"https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding\",children:\"\\uD2B8\\uB79C\\uC2A4\\uD3EC\\uBA38(Transformer) \\uD30C\\uD5E4\\uCE58\\uAE30\\u20141. Positional Encoding\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:112,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:112,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.h1,{id:\"why-self-attention\",children:[(0,r.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#why-self-attention\",children:(0,r.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this),\"Why Self-Attention?\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:114,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.ol,{children:[`\n`,(0,r.jsxDEV)(n.li,{children:[\"Total Computational complexity per layer\",`\n`,(0,r.jsxDEV)(n.ul,{children:[`\n`,(0,r.jsxDEV)(n.li,{children:\"Self-Attention Layers lower total computational complexity per layer. This happens especially when sequence length is less than the dimensionality of the representations.\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:117,columnNumber:4},this),`\n`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:117,columnNumber:4},this),`\n`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:116,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.li,{children:[\"Amount of computation that can be parallelized\",`\n`,(0,r.jsxDEV)(n.ul,{children:[`\n`,(0,r.jsxDEV)(n.li,{children:\"Self-attention allows for greater amount of computations that can be parallelized, minimizing the number of required sequential operations and reducing training & inference time.\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:119,columnNumber:4},this),`\n`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:119,columnNumber:4},this),`\n`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:118,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.li,{children:[\"The path length between long-range dependencies in the network\",`\n`,(0,r.jsxDEV)(n.ul,{children:[`\n`,(0,r.jsxDEV)(n.li,{children:\"Self-attention mechanism shorten the path length, facilitating easier, more efficient learning of dependencies.\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:121,columnNumber:4},this),`\n`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:121,columnNumber:4},this),`\n`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:120,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:116,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.h1,{id:\"training--result\",children:[(0,r.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#training--result\",children:(0,r.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this),\"Training / Result\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:123,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.h3,{id:\"training-setting\",children:[(0,r.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#training-setting\",children:(0,r.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this),\"Training Setting\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:125,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.ul,{children:[`\n`,(0,r.jsxDEV)(n.li,{children:\"Optimizer: Adam\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:127,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:127,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/f267f567-6357-48ad-98b3-1304ec558284\",alt:\"\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:129,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:129,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.ul,{children:[`\n`,(0,r.jsxDEV)(n.li,{children:[\"Three types of Regularization\",`\n`,(0,r.jsxDEV)(n.ul,{children:[`\n`,(0,r.jsxDEV)(n.li,{children:[\"Residual Dropout\",`\n`,(0,r.jsxDEV)(n.ul,{children:[`\n`,(0,r.jsxDEV)(n.li,{children:\"Applying Dropout to outputs of each sublayers\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:133,columnNumber:5},this),`\n`,(0,r.jsxDEV)(n.li,{children:\"Applying Dropout to the sum of embeddings and positional encoding\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:134,columnNumber:5},this),`\n`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:133,columnNumber:5},this),`\n`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:132,columnNumber:3},this),`\n`,(0,r.jsxDEV)(n.li,{children:\"Label Smoothing\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:135,columnNumber:3},this),`\n`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:132,columnNumber:3},this),`\n`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:131,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:131,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/5aa0389a-e172-44e7-86ae-078fcb95fac4\",alt:\"\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:137,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:137,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.h3,{id:\"result\",children:[(0,r.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#result\",children:(0,r.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this),\"Result\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:139,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.p,{children:[(0,r.jsxDEV)(n.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/c47159de-d168-4e12-b208-4165d10ee2e7\",alt:\"\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:141,columnNumber:1},this),`\n`,(0,r.jsxDEV)(n.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/914a0020-0bc7-4401-814e-cdaa2dfca022\",alt:\"\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:142,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:141,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\",lineNumber:1,columnNumber:1},this)}function gn(a={}){let{wrapper:n}=a.components||{};return n?(0,r.jsxDEV)(n,Object.assign({},a,{children:(0,r.jsxDEV)(Ge,a,void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this)}),void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-969f525d-1db6-406d-8497-9d73b1f85571.mdx\"},this):Ge(a)}var yn=gn;return _n(Nn);})();\n/*! Bundled license information:\n\nreact/cjs/react-jsx-dev-runtime.development.js:\n  (**\n   * @license React\n   * react-jsx-dev-runtime.development.js\n   *\n   * Copyright (c) Facebook, Inc. and its affiliates.\n   *\n   * This source code is licensed under the MIT license found in the\n   * LICENSE file in the root directory of this source tree.\n   *)\n*/\n;return Component;"
  },
  "_id": "transformers/index.mdx",
  "_raw": {
    "sourceFilePath": "transformers/index.mdx",
    "sourceFileName": "index.mdx",
    "sourceFileDir": "transformers",
    "contentType": "mdx",
    "flattenedPath": "transformers"
  },
  "type": "Blog",
  "url": "/blogs/transformers",
  "readingTime": {
    "text": "5 min read",
    "minutes": 4.835,
    "time": 290100,
    "words": 967
  },
  "toc": [
    {
      "level": "one",
      "text": "Introduction",
      "slug": "introduction"
    },
    {
      "level": "three",
      "text": "Limit of Recurrent Model",
      "slug": "limit-of-recurrent-model"
    },
    {
      "level": "one",
      "text": "Model Architecture",
      "slug": "model-architecture"
    },
    {
      "level": "three",
      "text": "Encoder",
      "slug": "encoder"
    },
    {
      "level": "three",
      "text": "Decoder",
      "slug": "decoder"
    },
    {
      "level": "three",
      "text": "Attention (Scaled Dot-Product Attention)",
      "slug": "attention-scaled-dot-product-attention"
    },
    {
      "level": "three",
      "text": "Multi-Head Attention",
      "slug": "multi-head-attention"
    },
    {
      "level": "three",
      "text": "Applications of Attentions (Multi-head attention)",
      "slug": "applications-of-attentions-multi-head-attention"
    },
    {
      "level": "three",
      "text": "Position-wise Feedforward Networks",
      "slug": "position-wise-feedforward-networks"
    },
    {
      "level": "three",
      "text": "Embeddings and Softmax",
      "slug": "embeddings-and-softmax"
    },
    {
      "level": "three",
      "text": "[Additional] Embeddings vs Encodings",
      "slug": "additional-embeddings-vs-encodings"
    },
    {
      "level": "three",
      "text": "Positional Encoding",
      "slug": "positional-encoding"
    },
    {
      "level": "one",
      "text": "Why Self-Attention?",
      "slug": "why-self-attention"
    },
    {
      "level": "one",
      "text": "Training / Result",
      "slug": "training--result"
    },
    {
      "level": "three",
      "text": "Training Setting",
      "slug": "training-setting"
    },
    {
      "level": "three",
      "text": "Result",
      "slug": "result"
    }
  ]
}