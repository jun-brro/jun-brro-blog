{
  "title": "[Paper Review] Seq2Seq",
  "publishedAt": "2024-07-08T00:00:00.000Z",
  "updatedAt": "2024-07-08T00:00:00.000Z",
  "description": "Seq2Seq (Sequence to Sequence) is a deep learning model designed for transforming sequences, such as translating sentences, by encoding an input sequence and decoding it into an output sequence.",
  "image": {
    "filePath": "../public/blogs/seq2seq/image.png",
    "relativeFilePath": "../../public/blogs/seq2seq/image.png",
    "format": "png",
    "height": 864,
    "width": 1754,
    "aspectRatio": 2.0300925925925926,
    "blurhashDataUrl": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAP1BMVEX////EzqmdxojFspvK2u/rxrSSu+T4+vnnubj8/fza2Nvx0sfLwM7Cs8Dl5eXX1drW3b2jwuyvz5nVybLvy8yRsQAzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAP0lEQVR4nB3LSRLAIAgAwRFBIGrM9v+3pspbXxoAd3cgOdc6NrqZoap6lVJord2PiDDmHG+tlYjoX0Ts6p75AzVnAbY6mBSRAAAAAElFTkSuQmCC"
  },
  "isPublished": true,
  "author": "junbrro",
  "tags": [
    "Deep Learning"
  ],
  "body": {
    "raw": "\n# Introduction: Limitation of traditional DNN\n\nDNN-based researches achieved steady success in terms of voice recognition and visual detection. However, there was a limitation that the sequential problem could not be properly solved because **the input size was fixed**. Due to the nature of language, which **inevitably has variable length**, it was difficult to solve this problem. Seq2Seq aims to solve this problem that the length of the same sentence varies depending on the language.\n\n# Seq2Seq Model Structure\n\n**Sequent usage of LSTM!**\n\n![](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/4df5a6cb-dd25-4794-a9de-7ff72fa06552)\nA model consisting of two LSTM-based structures, an Encoder and a Decoder, first reads the language through the Encoder and creates a **fixed-length Context Vector**. The end of each sentences are detected by using EOS(End of Sentence) token. Therefore, the input and output length can vary. When reading sentences through an encoder, they are read in reverse order. (It was way better than reading in forward order, proved by experimental result) The decoder produces an output that maximizes the conditional probability by considering the input, latent variable H, and context C.\n\n### Question: How the length of context vector is fixed?\n\nAnswer: Since the context vector, which is the output of Encoder, also works as input of Decoder, the size should be fixed when building a model. Often use 256, 512, 1024, etc.\n\n[Attention 메커니즘의 이해](https://gaussian37.github.io/dl-concept-attention/)\n\nThere was some trials to use RNN to handle sequence data.\n\n![](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/ee71c941-4c14-4116-ad12-235cc949f27d)\nRNNs work well when they know how inputs and outputs match up, especially if they are the same length. But, when inputs and outputs are different lengths and their connection isn't straightforward, it's hard to use RNNs. Furthermore, RNN is vulnerable to long term dependency, because Vanishing Gradient damages long-term dependency.\n\nTherefore, LSTM is used to handle long-term dependency and get higher ability of the model.\n\n![](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/aa8c98ce-fac8-44f2-b364-199fe7e1d6f2)\nThe LSTM achieves this by first generating a fixed-dimensional representation (v) of the input sequence (x_1 … x_T), represented by the final hidden state of the LSTM. Following that, it uses (v) as the initial hidden state for a standard LSTM-based language model (LSTM-LM) to calculate the probability of the output sequence (y_1 … y_T')\n\n# Experiments\n\n![](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/f8722669-4238-46b7-ac40-c421f665d780)\n![](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/fbada0bb-7e93-420a-acb4-41fb8d8f5e66)\nThe target function we aim to optimize is the maximization of the log likelihood, which translates into maximizing conditional probability. This maximization involves dividing the sum of these probabilities by 1/∣*S*∣, where ∣*S*∣ represents the size of the entire training set. This approach indicates that the objective is to develop a model that performs better on average, rather than one that fits a few specific sentences too closely.\n\n### Beam Search\n\nWhen creating target sentence using decoder, beam search method is used.\n\n![](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/677cbf2a-f62d-44c7-b6e2-4b4e021f5622)\n\nBeam search is a search algorithm used for finding sequences with the highest probabilities. It maintains a fixed number of best options, called the \"beam width,\" at each step, allowing it to explore more possibilities than a simple greedy search, which only keeps the single best option.\n\nWhen creating the sentence, the seq2seq decoder select nodes with highest probability in descending order. Here, All probabilities considered in beam search are cumulative probabilities. Even if some child nodes have the same probability, the cumulative probability varies depending on which beam they come from.\n\n### GPU Parallel Calculation\n\nThe calculation speed of LSTM is way slow to get meaningful result. Therefore, the study adapts parallelizing 8 GPU machines, to reduce total calculation time. _(Each layer of the LSTM was executed on a different GPU and communicated its activations to the next GPU / layer as soon as they were computed.)_\n\n### BLEU Score**(Bilingual Evaluation Understudy Score)**\n\n[14-03 BLEU Score(Bilingual Evaluation Understudy Score)](https://wikidocs.net/31695)\n\nBLEU measures the correspondence between a machine's output and that of a human reference translation, based on the precision of n-grams (contiguous sequences of _n_ items from a given sample of text) in the translated text compared to the reference.\n\n![](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/a4772dc3-33ed-4a36-9ecb-7e75ebb4a100)\n\n### Experiment Results (Reverse Reading)\n\n![](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/195041e1-0c80-4eab-a37c-be7fb08edd8b)\nNotable thing is that the model's performance is better when the words in the sentence are entered in reverse order. The paper assumes that reversing the word improves backpropagation. Reversing the source sentence doesn't change the average distance between corresponding words but significantly reduces this time lag for the initial words. This reduction makes it easier for backpropagation to link the source and target sentences, leading to notably better performance.\n\n### Experiment Results (Long-term Dependency)\n\n![](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/f03cfd79-912e-41d4-9f67-a76df8fc41cf)\n![](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/5c534d7f-7ab0-4749-a838-963b6c9191fd)\n",
    "code": "var Component=(()=>{var cn=Object.create;var C=Object.defineProperty;var dn=Object.getOwnPropertyDescriptor;var ln=Object.getOwnPropertyNames;var mn=Object.getPrototypeOf,fn=Object.prototype.hasOwnProperty;var V=(d,r)=>()=>(r||d((r={exports:{}}).exports,r),r.exports),bn=(d,r)=>{for(var p in r)C(d,p,{get:r[p],enumerable:!0})},xe=(d,r,p,v)=>{if(r&&typeof r==\"object\"||typeof r==\"function\")for(let y of ln(r))!fn.call(d,y)&&y!==p&&C(d,y,{get:()=>r[y],enumerable:!(v=dn(r,y))||v.enumerable});return d};var hn=(d,r,p)=>(p=d!=null?cn(mn(d)):{},xe(r||!d||!d.__esModule?C(p,\"default\",{value:d,enumerable:!0}):p,d)),pn=d=>xe(C({},\"__esModule\",{value:!0}),d);var je=V((Nn,Ne)=>{Ne.exports=React});var ke=V(B=>{\"use strict\";(function(){\"use strict\";var d=je(),r=Symbol.for(\"react.element\"),p=Symbol.for(\"react.portal\"),v=Symbol.for(\"react.fragment\"),y=Symbol.for(\"react.strict_mode\"),z=Symbol.for(\"react.profiler\"),X=Symbol.for(\"react.provider\"),K=Symbol.for(\"react.context\"),w=Symbol.for(\"react.forward_ref\"),P=Symbol.for(\"react.suspense\"),O=Symbol.for(\"react.suspense_list\"),R=Symbol.for(\"react.memo\"),A=Symbol.for(\"react.lazy\"),Re=Symbol.for(\"react.offscreen\"),J=Symbol.iterator,Te=\"@@iterator\";function Ue(e){if(e===null||typeof e!=\"object\")return null;var n=J&&e[J]||e[Te];return typeof n==\"function\"?n:null}var N=d.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED;function f(e){{for(var n=arguments.length,a=new Array(n>1?n-1:0),i=1;i<n;i++)a[i-1]=arguments[i];Se(\"error\",e,a)}}function Se(e,n,a){{var i=N.ReactDebugCurrentFrame,s=i.getStackAddendum();s!==\"\"&&(n+=\"%s\",a=a.concat([s]));var c=a.map(function(u){return String(u)});c.unshift(\"Warning: \"+n),Function.prototype.apply.call(console[e],console,c)}}var Ge=!1,He=!1,Ce=!1,Pe=!1,Oe=!1,Q;Q=Symbol.for(\"react.module.reference\");function Ae(e){return!!(typeof e==\"string\"||typeof e==\"function\"||e===v||e===z||Oe||e===y||e===P||e===O||Pe||e===Re||Ge||He||Ce||typeof e==\"object\"&&e!==null&&(e.$$typeof===A||e.$$typeof===R||e.$$typeof===X||e.$$typeof===K||e.$$typeof===w||e.$$typeof===Q||e.getModuleId!==void 0))}function Fe(e,n,a){var i=e.displayName;if(i)return i;var s=n.displayName||n.name||\"\";return s!==\"\"?a+\"(\"+s+\")\":a}function Z(e){return e.displayName||\"Context\"}function g(e){if(e==null)return null;if(typeof e.tag==\"number\"&&f(\"Received an unexpected object in getComponentNameFromType(). This is likely a bug in React. Please file an issue.\"),typeof e==\"function\")return e.displayName||e.name||null;if(typeof e==\"string\")return e;switch(e){case v:return\"Fragment\";case p:return\"Portal\";case z:return\"Profiler\";case y:return\"StrictMode\";case P:return\"Suspense\";case O:return\"SuspenseList\"}if(typeof e==\"object\")switch(e.$$typeof){case K:var n=e;return Z(n)+\".Consumer\";case X:var a=e;return Z(a._context)+\".Provider\";case w:return Fe(e,e.render,\"ForwardRef\");case R:var i=e.displayName||null;return i!==null?i:g(e.type)||\"Memo\";case A:{var s=e,c=s._payload,u=s._init;try{return g(u(c))}catch{return null}}}return null}var x=Object.assign,E=0,ee,ne,re,te,ae,ie,oe;function ue(){}ue.__reactDisabledLog=!0;function Ie(){{if(E===0){ee=console.log,ne=console.info,re=console.warn,te=console.error,ae=console.group,ie=console.groupCollapsed,oe=console.groupEnd;var e={configurable:!0,enumerable:!0,value:ue,writable:!0};Object.defineProperties(console,{info:e,log:e,warn:e,error:e,group:e,groupCollapsed:e,groupEnd:e})}E++}}function qe(){{if(E--,E===0){var e={configurable:!0,enumerable:!0,writable:!0};Object.defineProperties(console,{log:x({},e,{value:ee}),info:x({},e,{value:ne}),warn:x({},e,{value:re}),error:x({},e,{value:te}),group:x({},e,{value:ae}),groupCollapsed:x({},e,{value:ie}),groupEnd:x({},e,{value:oe})})}E<0&&f(\"disabledDepth fell below zero. This is a bug in React. Please file an issue.\")}}var F=N.ReactCurrentDispatcher,I;function T(e,n,a){{if(I===void 0)try{throw Error()}catch(s){var i=s.stack.trim().match(/\\n( *(at )?)/);I=i&&i[1]||\"\"}return`\n`+I+e}}var q=!1,U;{var Le=typeof WeakMap==\"function\"?WeakMap:Map;U=new Le}function se(e,n){if(!e||q)return\"\";{var a=U.get(e);if(a!==void 0)return a}var i;q=!0;var s=Error.prepareStackTrace;Error.prepareStackTrace=void 0;var c;c=F.current,F.current=null,Ie();try{if(n){var u=function(){throw Error()};if(Object.defineProperty(u.prototype,\"props\",{set:function(){throw Error()}}),typeof Reflect==\"object\"&&Reflect.construct){try{Reflect.construct(u,[])}catch(_){i=_}Reflect.construct(e,[],u)}else{try{u.call()}catch(_){i=_}e.call(u.prototype)}}else{try{throw Error()}catch(_){i=_}e()}}catch(_){if(_&&i&&typeof _.stack==\"string\"){for(var o=_.stack.split(`\n`),b=i.stack.split(`\n`),l=o.length-1,m=b.length-1;l>=1&&m>=0&&o[l]!==b[m];)m--;for(;l>=1&&m>=0;l--,m--)if(o[l]!==b[m]){if(l!==1||m!==1)do if(l--,m--,m<0||o[l]!==b[m]){var h=`\n`+o[l].replace(\" at new \",\" at \");return e.displayName&&h.includes(\"<anonymous>\")&&(h=h.replace(\"<anonymous>\",e.displayName)),typeof e==\"function\"&&U.set(e,h),h}while(l>=1&&m>=0);break}}}finally{q=!1,F.current=c,qe(),Error.prepareStackTrace=s}var k=e?e.displayName||e.name:\"\",ve=k?T(k):\"\";return typeof e==\"function\"&&U.set(e,ve),ve}function Me(e,n,a){return se(e,!1)}function We(e){var n=e.prototype;return!!(n&&n.isReactComponent)}function S(e,n,a){if(e==null)return\"\";if(typeof e==\"function\")return se(e,We(e));if(typeof e==\"string\")return T(e);switch(e){case P:return T(\"Suspense\");case O:return T(\"SuspenseList\")}if(typeof e==\"object\")switch(e.$$typeof){case w:return Me(e.render);case R:return S(e.type,n,a);case A:{var i=e,s=i._payload,c=i._init;try{return S(c(s),n,a)}catch{}}}return\"\"}var G=Object.prototype.hasOwnProperty,ce={},de=N.ReactDebugCurrentFrame;function H(e){if(e){var n=e._owner,a=S(e.type,e._source,n?n.type:null);de.setExtraStackFrame(a)}else de.setExtraStackFrame(null)}function Ye(e,n,a,i,s){{var c=Function.call.bind(G);for(var u in e)if(c(e,u)){var o=void 0;try{if(typeof e[u]!=\"function\"){var b=Error((i||\"React class\")+\": \"+a+\" type `\"+u+\"` is invalid; it must be a function, usually from the `prop-types` package, but received `\"+typeof e[u]+\"`.This often happens because of typos such as `PropTypes.function` instead of `PropTypes.func`.\");throw b.name=\"Invariant Violation\",b}o=e[u](n,u,i,a,null,\"SECRET_DO_NOT_PASS_THIS_OR_YOU_WILL_BE_FIRED\")}catch(l){o=l}o&&!(o instanceof Error)&&(H(s),f(\"%s: type specification of %s `%s` is invalid; the type checker function must return `null` or an `Error` but returned a %s. You may have forgotten to pass an argument to the type checker creator (arrayOf, instanceOf, objectOf, oneOf, oneOfType, and shape all require an argument).\",i||\"React class\",a,u,typeof o),H(null)),o instanceof Error&&!(o.message in ce)&&(ce[o.message]=!0,H(s),f(\"Failed %s type: %s\",a,o.message),H(null))}}}var $e=Array.isArray;function L(e){return $e(e)}function Ve(e){{var n=typeof Symbol==\"function\"&&Symbol.toStringTag,a=n&&e[Symbol.toStringTag]||e.constructor.name||\"Object\";return a}}function Be(e){try{return le(e),!1}catch{return!0}}function le(e){return\"\"+e}function me(e){if(Be(e))return f(\"The provided key is an unsupported type %s. This value must be coerced to a string before before using it here.\",Ve(e)),le(e)}var D=N.ReactCurrentOwner,ze={key:!0,ref:!0,__self:!0,__source:!0},fe,be,M;M={};function Xe(e){if(G.call(e,\"ref\")){var n=Object.getOwnPropertyDescriptor(e,\"ref\").get;if(n&&n.isReactWarning)return!1}return e.ref!==void 0}function Ke(e){if(G.call(e,\"key\")){var n=Object.getOwnPropertyDescriptor(e,\"key\").get;if(n&&n.isReactWarning)return!1}return e.key!==void 0}function Je(e,n){if(typeof e.ref==\"string\"&&D.current&&n&&D.current.stateNode!==n){var a=g(D.current.type);M[a]||(f('Component \"%s\" contains the string ref \"%s\". Support for string refs will be removed in a future major release. This case cannot be automatically converted to an arrow function. We ask you to manually fix this case by using useRef() or createRef() instead. Learn more about using refs safely here: https://reactjs.org/link/strict-mode-string-ref',g(D.current.type),e.ref),M[a]=!0)}}function Qe(e,n){{var a=function(){fe||(fe=!0,f(\"%s: `key` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\",n))};a.isReactWarning=!0,Object.defineProperty(e,\"key\",{get:a,configurable:!0})}}function Ze(e,n){{var a=function(){be||(be=!0,f(\"%s: `ref` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\",n))};a.isReactWarning=!0,Object.defineProperty(e,\"ref\",{get:a,configurable:!0})}}var en=function(e,n,a,i,s,c,u){var o={$$typeof:r,type:e,key:n,ref:a,props:u,_owner:c};return o._store={},Object.defineProperty(o._store,\"validated\",{configurable:!1,enumerable:!1,writable:!0,value:!1}),Object.defineProperty(o,\"_self\",{configurable:!1,enumerable:!1,writable:!1,value:i}),Object.defineProperty(o,\"_source\",{configurable:!1,enumerable:!1,writable:!1,value:s}),Object.freeze&&(Object.freeze(o.props),Object.freeze(o)),o};function nn(e,n,a,i,s){{var c,u={},o=null,b=null;a!==void 0&&(me(a),o=\"\"+a),Ke(n)&&(me(n.key),o=\"\"+n.key),Xe(n)&&(b=n.ref,Je(n,s));for(c in n)G.call(n,c)&&!ze.hasOwnProperty(c)&&(u[c]=n[c]);if(e&&e.defaultProps){var l=e.defaultProps;for(c in l)u[c]===void 0&&(u[c]=l[c])}if(o||b){var m=typeof e==\"function\"?e.displayName||e.name||\"Unknown\":e;o&&Qe(u,m),b&&Ze(u,m)}return en(e,o,b,s,i,D.current,u)}}var W=N.ReactCurrentOwner,he=N.ReactDebugCurrentFrame;function j(e){if(e){var n=e._owner,a=S(e.type,e._source,n?n.type:null);he.setExtraStackFrame(a)}else he.setExtraStackFrame(null)}var Y;Y=!1;function $(e){return typeof e==\"object\"&&e!==null&&e.$$typeof===r}function pe(){{if(W.current){var e=g(W.current.type);if(e)return`\n\nCheck the render method of \\``+e+\"`.\"}return\"\"}}function rn(e){{if(e!==void 0){var n=e.fileName.replace(/^.*[\\\\\\/]/,\"\"),a=e.lineNumber;return`\n\nCheck your code at `+n+\":\"+a+\".\"}return\"\"}}var ge={};function tn(e){{var n=pe();if(!n){var a=typeof e==\"string\"?e:e.displayName||e.name;a&&(n=`\n\nCheck the top-level render call using <`+a+\">.\")}return n}}function _e(e,n){{if(!e._store||e._store.validated||e.key!=null)return;e._store.validated=!0;var a=tn(n);if(ge[a])return;ge[a]=!0;var i=\"\";e&&e._owner&&e._owner!==W.current&&(i=\" It was passed a child from \"+g(e._owner.type)+\".\"),j(e),f('Each child in a list should have a unique \"key\" prop.%s%s See https://reactjs.org/link/warning-keys for more information.',a,i),j(null)}}function ye(e,n){{if(typeof e!=\"object\")return;if(L(e))for(var a=0;a<e.length;a++){var i=e[a];$(i)&&_e(i,n)}else if($(e))e._store&&(e._store.validated=!0);else if(e){var s=Ue(e);if(typeof s==\"function\"&&s!==e.entries)for(var c=s.call(e),u;!(u=c.next()).done;)$(u.value)&&_e(u.value,n)}}}function an(e){{var n=e.type;if(n==null||typeof n==\"string\")return;var a;if(typeof n==\"function\")a=n.propTypes;else if(typeof n==\"object\"&&(n.$$typeof===w||n.$$typeof===R))a=n.propTypes;else return;if(a){var i=g(n);Ye(a,e.props,\"prop\",i,e)}else if(n.PropTypes!==void 0&&!Y){Y=!0;var s=g(n);f(\"Component %s declared `PropTypes` instead of `propTypes`. Did you misspell the property assignment?\",s||\"Unknown\")}typeof n.getDefaultProps==\"function\"&&!n.getDefaultProps.isReactClassApproved&&f(\"getDefaultProps is only used on classic React.createClass definitions. Use a static property named `defaultProps` instead.\")}}function on(e){{for(var n=Object.keys(e.props),a=0;a<n.length;a++){var i=n[a];if(i!==\"children\"&&i!==\"key\"){j(e),f(\"Invalid prop `%s` supplied to `React.Fragment`. React.Fragment can only have `key` and `children` props.\",i),j(null);break}}e.ref!==null&&(j(e),f(\"Invalid attribute `ref` supplied to `React.Fragment`.\"),j(null))}}function un(e,n,a,i,s,c){{var u=Ae(e);if(!u){var o=\"\";(e===void 0||typeof e==\"object\"&&e!==null&&Object.keys(e).length===0)&&(o+=\" You likely forgot to export your component from the file it's defined in, or you might have mixed up default and named imports.\");var b=rn(s);b?o+=b:o+=pe();var l;e===null?l=\"null\":L(e)?l=\"array\":e!==void 0&&e.$$typeof===r?(l=\"<\"+(g(e.type)||\"Unknown\")+\" />\",o=\" Did you accidentally export a JSX literal instead of a component?\"):l=typeof e,f(\"React.jsx: type is invalid -- expected a string (for built-in components) or a class/function (for composite components) but got: %s.%s\",l,o)}var m=nn(e,n,a,s,c);if(m==null)return m;if(u){var h=n.children;if(h!==void 0)if(i)if(L(h)){for(var k=0;k<h.length;k++)ye(h[k],e);Object.freeze&&Object.freeze(h)}else f(\"React.jsx: Static children should always be an array. You are likely explicitly calling React.jsxs or React.jsxDEV. Use the Babel transform instead.\");else ye(h,e)}return e===v?on(m):an(m),m}}var sn=un;B.Fragment=v,B.jsxDEV=sn})()});var De=V((kn,Ee)=>{\"use strict\";Ee.exports=ke()});var vn={};bn(vn,{default:()=>yn,frontmatter:()=>gn});var t=hn(De()),gn={title:\"[Paper Review] Seq2Seq\",description:\"Seq2Seq (Sequence to Sequence) is a deep learning model designed for transforming sequences, such as translating sentences, by encoding an input sequence and decoding it into an output sequence.\",image:\"../../public/blogs/seq2seq/image.png\",publishedAt:\"2024-07-08\",updatedAt:\"2024-07-08\",author:\"junbrro\",isPublished:!0,tags:[\"Deep Learning\"]};function we(d){let r=Object.assign({h1:\"h1\",a:\"a\",span:\"span\",p:\"p\",strong:\"strong\",img:\"img\",h3:\"h3\",em:\"em\"},d.components);return(0,t.jsxDEV)(t.Fragment,{children:[(0,t.jsxDEV)(r.h1,{id:\"introduction-limitation-of-traditional-dnn\",children:[(0,t.jsxDEV)(r.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#introduction-limitation-of-traditional-dnn\",children:(0,t.jsxDEV)(r.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\"},this),\"Introduction: Limitation of traditional DNN\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:13,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:[\"DNN-based researches achieved steady success in terms of voice recognition and visual detection. However, there was a limitation that the sequential problem could not be properly solved because \",(0,t.jsxDEV)(r.strong,{children:\"the input size was fixed\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:15,columnNumber:195},this),\". Due to the nature of language, which \",(0,t.jsxDEV)(r.strong,{children:\"inevitably has variable length\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:15,columnNumber:262},this),\", it was difficult to solve this problem. Seq2Seq aims to solve this problem that the length of the same sentence varies depending on the language.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:15,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.h1,{id:\"seq2seq-model-structure\",children:[(0,t.jsxDEV)(r.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#seq2seq-model-structure\",children:(0,t.jsxDEV)(r.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\"},this),\"Seq2Seq Model Structure\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:17,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:(0,t.jsxDEV)(r.strong,{children:\"Sequent usage of LSTM!\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:19,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:19,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:[(0,t.jsxDEV)(r.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/4df5a6cb-dd25-4794-a9de-7ff72fa06552\",alt:\"\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:21,columnNumber:1},this),`\nA model consisting of two LSTM-based structures, an Encoder and a Decoder, first reads the language through the Encoder and creates a `,(0,t.jsxDEV)(r.strong,{children:\"fixed-length Context Vector\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:22,columnNumber:135},this),\". The end of each sentences are detected by using EOS(End of Sentence) token. Therefore, the input and output length can vary. When reading sentences through an encoder, they are read in reverse order. (It was way better than reading in forward order, proved by experimental result) The decoder produces an output that maximizes the conditional probability by considering the input, latent variable H, and context C.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:21,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.h3,{id:\"question-how-the-length-of-context-vector-is-fixed\",children:[(0,t.jsxDEV)(r.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#question-how-the-length-of-context-vector-is-fixed\",children:(0,t.jsxDEV)(r.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\"},this),\"Question: How the length of context vector is fixed?\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:24,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"Answer: Since the context vector, which is the output of Encoder, also works as input of Decoder, the size should be fixed when building a model. Often use 256, 512, 1024, etc.\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:26,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:(0,t.jsxDEV)(r.a,{href:\"https://gaussian37.github.io/dl-concept-attention/\",children:\"Attention \\uBA54\\uCEE4\\uB2C8\\uC998\\uC758 \\uC774\\uD574\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:28,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:28,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"There was some trials to use RNN to handle sequence data.\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:30,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:[(0,t.jsxDEV)(r.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/ee71c941-4c14-4116-ad12-235cc949f27d\",alt:\"\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:32,columnNumber:1},this),`\nRNNs work well when they know how inputs and outputs match up, especially if they are the same length. But, when inputs and outputs are different lengths and their connection isn't straightforward, it's hard to use RNNs. Furthermore, RNN is vulnerable to long term dependency, because Vanishing Gradient damages long-term dependency.`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:32,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"Therefore, LSTM is used to handle long-term dependency and get higher ability of the model.\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:35,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:[(0,t.jsxDEV)(r.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/aa8c98ce-fac8-44f2-b364-199fe7e1d6f2\",alt:\"\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:37,columnNumber:1},this),`\nThe LSTM achieves this by first generating a fixed-dimensional representation (v) of the input sequence (x_1 \\u2026 x_T), represented by the final hidden state of the LSTM. Following that, it uses (v) as the initial hidden state for a standard LSTM-based language model (LSTM-LM) to calculate the probability of the output sequence (y_1 \\u2026 y_T')`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:37,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.h1,{id:\"experiments\",children:[(0,t.jsxDEV)(r.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#experiments\",children:(0,t.jsxDEV)(r.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\"},this),\"Experiments\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:40,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:[(0,t.jsxDEV)(r.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/f8722669-4238-46b7-ac40-c421f665d780\",alt:\"\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:42,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/fbada0bb-7e93-420a-acb4-41fb8d8f5e66\",alt:\"\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:43,columnNumber:1},this),`\nThe target function we aim to optimize is the maximization of the log likelihood, which translates into maximizing conditional probability. This maximization involves dividing the sum of these probabilities by 1/\\u2223`,(0,t.jsxDEV)(r.em,{children:\"S\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:44,columnNumber:214},this),\"\\u2223, where \\u2223\",(0,t.jsxDEV)(r.em,{children:\"S\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:44,columnNumber:227},this),\"\\u2223 represents the size of the entire training set. This approach indicates that the objective is to develop a model that performs better on average, rather than one that fits a few specific sentences too closely.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:42,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.h3,{id:\"beam-search\",children:[(0,t.jsxDEV)(r.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#beam-search\",children:(0,t.jsxDEV)(r.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\"},this),\"Beam Search\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:46,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"When creating target sentence using decoder, beam search method is used.\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:48,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:(0,t.jsxDEV)(r.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/677cbf2a-f62d-44c7-b6e2-4b4e021f5622\",alt:\"\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:50,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:50,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:'Beam search is a search algorithm used for finding sequences with the highest probabilities. It maintains a fixed number of best options, called the \"beam width,\" at each step, allowing it to explore more possibilities than a simple greedy search, which only keeps the single best option.'},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:52,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"When creating the sentence, the seq2seq decoder select nodes with highest probability in descending order. Here, All probabilities considered in beam search are cumulative probabilities. Even if some child nodes have the same probability, the cumulative probability varies depending on which beam they come from.\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:54,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.h3,{id:\"gpu-parallel-calculation\",children:[(0,t.jsxDEV)(r.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#gpu-parallel-calculation\",children:(0,t.jsxDEV)(r.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\"},this),\"GPU Parallel Calculation\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:56,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:[\"The calculation speed of LSTM is way slow to get meaningful result. Therefore, the study adapts parallelizing 8 GPU machines, to reduce total calculation time. \",(0,t.jsxDEV)(r.em,{children:\"(Each layer of the LSTM was executed on a different GPU and communicated its activations to the next GPU / layer as soon as they were computed.)\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:58,columnNumber:161},this)]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:58,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.h3,{id:\"bleu-scorebilingual-evaluation-understudy-score\",children:[(0,t.jsxDEV)(r.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#bleu-scorebilingual-evaluation-understudy-score\",children:(0,t.jsxDEV)(r.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\"},this),\"BLEU Score**(Bilingual Evaluation Understudy Score)**\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:60,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:(0,t.jsxDEV)(r.a,{href:\"https://wikidocs.net/31695\",children:\"14-03 BLEU Score(Bilingual Evaluation Understudy Score)\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:62,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:62,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:[\"BLEU measures the correspondence between a machine's output and that of a human reference translation, based on the precision of n-grams (contiguous sequences of \",(0,t.jsxDEV)(r.em,{children:\"n\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:64,columnNumber:163},this),\" items from a given sample of text) in the translated text compared to the reference.\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:64,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:(0,t.jsxDEV)(r.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/a4772dc3-33ed-4a36-9ecb-7e75ebb4a100\",alt:\"\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:66,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:66,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.h3,{id:\"experiment-results-reverse-reading\",children:[(0,t.jsxDEV)(r.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#experiment-results-reverse-reading\",children:(0,t.jsxDEV)(r.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\"},this),\"Experiment Results (Reverse Reading)\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:68,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:[(0,t.jsxDEV)(r.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/195041e1-0c80-4eab-a37c-be7fb08edd8b\",alt:\"\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:70,columnNumber:1},this),`\nNotable thing is that the model's performance is better when the words in the sentence are entered in reverse order. The paper assumes that reversing the word improves backpropagation. Reversing the source sentence doesn't change the average distance between corresponding words but significantly reduces this time lag for the initial words. This reduction makes it easier for backpropagation to link the source and target sentences, leading to notably better performance.`]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:70,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.h3,{id:\"experiment-results-long-term-dependency\",children:[(0,t.jsxDEV)(r.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#experiment-results-long-term-dependency\",children:(0,t.jsxDEV)(r.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\"},this)},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\"},this),\"Experiment Results (Long-term Dependency)\"]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:73,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:[(0,t.jsxDEV)(r.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/f03cfd79-912e-41d4-9f67-a76df8fc41cf\",alt:\"\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:75,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.img,{src:\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/5c534d7f-7ab0-4749-a838-963b6c9191fd\",alt:\"\"},void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:76,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:75,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\",lineNumber:1,columnNumber:1},this)}function _n(d={}){let{wrapper:r}=d.components||{};return r?(0,t.jsxDEV)(r,Object.assign({},d,{children:(0,t.jsxDEV)(we,d,void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\"},this)}),void 0,!1,{fileName:\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-1832d38a-8426-4972-8513-0d554ca51c7c.mdx\"},this):we(d)}var yn=_n;return pn(vn);})();\n/*! Bundled license information:\n\nreact/cjs/react-jsx-dev-runtime.development.js:\n  (**\n   * @license React\n   * react-jsx-dev-runtime.development.js\n   *\n   * Copyright (c) Facebook, Inc. and its affiliates.\n   *\n   * This source code is licensed under the MIT license found in the\n   * LICENSE file in the root directory of this source tree.\n   *)\n*/\n;return Component;"
  },
  "_id": "seq2seq/index.mdx",
  "_raw": {
    "sourceFilePath": "seq2seq/index.mdx",
    "sourceFileName": "index.mdx",
    "sourceFileDir": "seq2seq",
    "contentType": "mdx",
    "flattenedPath": "seq2seq"
  },
  "type": "Blog",
  "url": "/blogs/seq2seq",
  "readingTime": {
    "text": "4 min read",
    "minutes": 3.655,
    "time": 219300,
    "words": 731
  },
  "toc": [
    {
      "level": "one",
      "text": "Introduction: Limitation of traditional DNN",
      "slug": "introduction-limitation-of-traditional-dnn"
    },
    {
      "level": "one",
      "text": "Seq2Seq Model Structure",
      "slug": "seq2seq-model-structure"
    },
    {
      "level": "three",
      "text": "Question: How the length of context vector is fixed?",
      "slug": "question-how-the-length-of-context-vector-is-fixed"
    },
    {
      "level": "one",
      "text": "Experiments",
      "slug": "experiments"
    },
    {
      "level": "three",
      "text": "Beam Search",
      "slug": "beam-search"
    },
    {
      "level": "three",
      "text": "GPU Parallel Calculation",
      "slug": "gpu-parallel-calculation"
    },
    {
      "level": "three",
      "text": "BLEU Score**(Bilingual Evaluation Understudy Score)**",
      "slug": "bleu-scorebilingual-evaluation-understudy-score"
    },
    {
      "level": "three",
      "text": "Experiment Results (Reverse Reading)",
      "slug": "experiment-results-reverse-reading"
    },
    {
      "level": "three",
      "text": "Experiment Results (Long-term Dependency)",
      "slug": "experiment-results-long-term-dependency"
    }
  ]
}