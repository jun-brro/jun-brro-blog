"use strict";
/*
 * ATTENTION: An "eval-source-map" devtool has been used.
 * This devtool is neither made for production nor for readable output files.
 * It uses "eval()" calls to create a separate source file with attached SourceMaps in the browser devtools.
 * If you are trying to read the output file, select a different devtool (https://webpack.js.org/configuration/devtool/)
 * or disable the default devtool with "devtool: false".
 * If you are looking for production-ready output files, see mode: "production" (https://webpack.js.org/configuration/mode/).
 */
self["webpackHotUpdate_N_E"]("app/page",{

/***/ "(app-pages-browser)/./.contentlayer/generated/Blog/lstm-and-gru__index.mdx.json":
/*!*******************************************************************!*\
  !*** ./.contentlayer/generated/Blog/lstm-and-gru__index.mdx.json ***!
  \*******************************************************************/
/***/ (function(module, __unused_webpack_exports, __webpack_require__) {

module.exports = JSON.parse('{"title":"[Paper Review] LSTM & GRU","publishedAt":"2024-07-10T00:00:00.000Z","updatedAt":"2024-07-10T00:00:00.000Z","description":"DDIM (Denoising Diffusion Implicit Models) is a deep learning model for efficient image generation through a refined diffusion denoising process.","image":{"filePath":"../public/blogs/lstm-and-gru/screenshot.png","relativeFilePath":"../../public/blogs/lstm-and-gru/screenshot.png","format":"png","height":341,"width":685,"aspectRatio":2.0087976539589443,"blurhashDataUrl":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAS1BMVEX///+5ucO3yuj8+fD+9eLBx9X7/v+8yeCvvdi+t7f6+/3o7PXj5Or84u/T2OPFtKjzu9bxy3777M3037a3wdvLxcnz1On1oMr+/fqvSJ4mAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAQElEQVR4nBXMhxHAIAwEsAfcMD19/0lzaAChYZsvmri7PBMozHqzolczyyOg1zByDgaUS1VjRKIzQeQDHYv28ANMcgIDngOMWwAAAABJRU5ErkJggg=="},"isPublished":true,"author":"junbrro","tags":["Deep Learning"],"body":{"raw":"\\nThis post is reviewing the LSTM & GRU model structure, based on RNN basics.\\n\\nThumbnail: [SpringerLink](https://link.springer.com/article/10.1007/s11269-022-03397-6)\\n\\n# Background: RNN (Recurrent Neural Network)\\n\\n**Recurrent Neural Networks (RNNs)** are a class of neural networks designed to process sequential data of varying lengths. Unlike traditional feedforward neural networks, **RNNs maintain a form of internal memory through their recurrent hidden states**, allowing them to exhibit dynamic temporal behavior. This feature makes RNNs particularly suitable for tasks where the current input is dependent on prior inputs, such as natural language processing or time series analysis.\\n\\nFor any given sequence X=(x1,x2,x3,...,xr), the RNN updates its hidden state ht at each time step t, which is a function of the current input xt and the previous hidden state ht-1. Mathematically, this can be represented as:\\n\\n![RNN Formula](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/6893aacc-b65e-4b1e-befa-12ec056af38b)\\n\\nwhere f is a nonlinear activation function, xt is the input at time step t, and ht-1 is the hidden state from the previous time step.\\n\\n![RNN Diagram](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/7bd51890-4470-438f-8c86-698e66c84a7d)\\n\\n![RNN Structure](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/6407a265-e076-435f-a5da-4630cb5a90f2)\\n\\n![RNN Overview](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/a5f693c6-e0f6-4b1c-ba08-f31a2e57791e)\\n\\n**Cons:** RNNs are hard to train with long input sequences due to several issues:\\n\\n- **Too many calculations**\\n- **Gradient vanishing**\\n- **Gradient exploding**\\n\\n**Solutions:** Clipping Gradient, Using Gating Unit\\n\\n### Clipping Gradient\\n\\n![Gradient Clipping](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/337908c7-37ba-410e-ad30-b97b45eb6971)\\n\\n**Gradient clipping** involves limiting (or \\"clipping\\") the gradients to a defined range or threshold during the backpropagation process to ensure they do not become too large (exploding) or too small (vanishing).\\n\\n**How It Works**:\\n\\n- By Value: Gradient values are clipped directly if they exceed a predefined threshold. For example, all gradient components greater than a value are set to that value, and all those less than a negative of that value are set to its negative.\\n- By Norm: The gradients are scaled down proportionally if the norm of the gradient vector exceeds a specified threshold. This approach keeps the direction of the gradient but reduces its magnitude to avoid exploding gradients.\\n\\n### Using Gating Unit\\n\\n![Gating Unit](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/9a6bc2f6-1971-4c1c-907a-c39e2b1d2aa3)\\n\\nThis paper mainly explains about GRU, comparing the basic structures with LSTM.\\n\\n# LSTM (Long Short-Term Memory)\\n\\n![LSTM Diagram](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/34029be4-ee80-4afa-b3c7-ab59d217604d)\\n\\n- **Forget Gate**: Decides what information to discard from the cell state, using a sigmoid function to assign values close to 0 for information to forget and values close to 1 for information to retain.\\n- **Input Gate**: Determines which new information to update in the cell state, combining a sigmoid layer to select values and a tanh layer to create a vector of candidate values.\\n- **Cell State**: Serves as the LSTM\'s memory, carrying relevant information across the sequence, and is updated based on inputs from the forget and input gates.\\n- **Output Gate**: Controls what part of the cell state is passed to the output, using a sigmoid layer to select parts of the cell state and a tanh layer to scale these selected parts before producing the final output.\\n\\n![LSTM Output Calculation](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/ab2c532a-07b2-46ad-98c7-4fb671b4a7f7)\\n\\no_jt is the total sum of memory content exposure (output gate).\\n\\n![Memory Content Exposure](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/6ec0bc8a-34dd-4535-8936-649abd49ad45)\\n\\nV0 is a diagonal matrix.\\n\\nThe memory cell state c_Bar\\\\*jt forgets some of the previous content and adds new memory content cbar_jt.\\n\\n![Memory Cell State](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/4b8b2521-d99d-4d46-9454-d245bcb0dda6)\\n\\n![New Memory Content](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/81655588-08a9-40f2-8ed2-b6e51bf28363)\\n\\nThe forget gate calculation f_jt determines how much of the new content will be added, which is controlled by the input gate i_jt.\\n\\n![Forget Gate Calculation](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/4aa734d4-8fa6-442e-8d8b-dca3bc65cecf)\\n\\nHere, Vf is a diagonal matrix of Vi.\\n\\n# GRU (Gated Recurrent Unit)\\n\\n![GRU Diagram](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/20737357-da6c-428b-918f-2dd14eaa1489)\\n\\n- **Update Gate:** Determines the amount of past information to keep versus new information to add, using a sigmoid function to balance between the previous activation and the potential new candidate activation.\\n- **Reset Gate:** Decides how much of the past information to forget, allowing the model to drop irrelevant information from the previous steps for the current prediction.\\n- **Candidate Activation:** Combines the current input with the past memory, influenced by the reset gate, to create a candidate for the new hidden state, blending old and new information with the potential to update the model’s memory.\\n\\nThe linear interpolation between h\\\\*jt-1 and hbar_jt works as the activation of GRU.\\n\\n![GRU Activation](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/1c02cfa7-234d-4512-b10d-201b9afee926)\\n\\nHere, the update gate z_jt is the value of updated content.\\n\\n![Update Gate](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/3a59ab74-d0b3-4418-b708-647bf4c639c5)\\n\\nThese processes to get the linear sum of the existing state and the new state (candidate activation) are similar to LSTM calculations. However, the difference is that GRU cannot adjust the amount of state exposure. \\\\( \\\\tilde{h}\\\\_{jt} \\\\) is calculated as below.\\n\\n![Candidate Activation Calculation](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/3e01a169-d3fb-4834-abd1-10d4a03f7495)\\n\\nThe reset gate has a similar process.\\n\\n![Reset Gate](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/81e4267d-557a-4ecf-8ad8-412109396f74)\\n\\n# Experiment (Validation)\\n\\n**Sequence Modeling** aims at learning a probability distribution over sequences.\\n\\n![Sequence Modeling](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/4c12254a-97bc-4ad6-8388-c52fe9a6bf0b)\\n\\nTraining three different units (LSTM, GRU, tanh unit). Used RMSProp as the optimizer, and weight noise is fixed at 0.075 (standard deviation). The magnitude of the gradient’s norm cannot exceed 1 to prevent gradient exploding.\\n\\n![Training Units](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/06d04510-578b-4115-8cf1-f85036d20bd6)\\n\\n![Training Results](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/de7b170b-82f1-4439-8ae4-7e86b9dafc9e)\\n\\n# Result\\n\\n![Experiment Result](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/80cf294a-70ea-4726-a510-43a946afb614)\\n\\n**In the case of GRU**, LSTM’s four gates are reduced to two gates (update, reset gate). Also, all states are provided at once (single hidden state). Since the number of gates is smaller, the number of parameters is also less than LSTM.\\n\\n**Actually, the ability of GRU is not dramatically higher than LSTM.** Still, GRU has the advantage of requiring fewer calculations and faster speed.\\n","code":"var Component=(()=>{var sn=Object.create;var O=Object.defineProperty;var cn=Object.getOwnPropertyDescriptor;var bn=Object.getOwnPropertyNames;var mn=Object.getPrototypeOf,fn=Object.prototype.hasOwnProperty;var B=(s,n)=>()=>(n||s((n={exports:{}}).exports,n),n.exports),hn=(s,n)=>{for(var _ in n)O(s,_,{get:n[_],enumerable:!0})},xe=(s,n,_,j)=>{if(n&&typeof n==\\"object\\"||typeof n==\\"function\\")for(let y of bn(n))!fn.call(s,y)&&y!==_&&O(s,y,{get:()=>n[y],enumerable:!(j=cn(n,y))||j.enumerable});return s};var pn=(s,n,_)=>(_=s!=null?sn(mn(s)):{},xe(n||!s||!s.__esModule?O(_,\\"default\\",{value:s,enumerable:!0}):_,s)),_n=s=>xe(O({},\\"__esModule\\",{value:!0}),s);var ke=B((vn,ve)=>{ve.exports=React});var De=B(z=>{\\"use strict\\";(function(){\\"use strict\\";var s=ke(),n=Symbol.for(\\"react.element\\"),_=Symbol.for(\\"react.portal\\"),j=Symbol.for(\\"react.fragment\\"),y=Symbol.for(\\"react.strict_mode\\"),X=Symbol.for(\\"react.profiler\\"),K=Symbol.for(\\"react.provider\\"),J=Symbol.for(\\"react.context\\"),H=Symbol.for(\\"react.forward_ref\\"),P=Symbol.for(\\"react.suspense\\"),F=Symbol.for(\\"react.suspense_list\\"),w=Symbol.for(\\"react.memo\\"),M=Symbol.for(\\"react.lazy\\"),we=Symbol.for(\\"react.offscreen\\"),Z=Symbol.iterator,Re=\\"@@iterator\\";function Ee(e){if(e===null||typeof e!=\\"object\\")return null;var t=Z&&e[Z]||e[Re];return typeof t==\\"function\\"?t:null}var v=s.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED;function m(e){{for(var t=arguments.length,i=new Array(t>1?t-1:0),u=1;u<t;u++)i[u-1]=arguments[u];Te(\\"error\\",e,i)}}function Te(e,t,i){{var u=v.ReactDebugCurrentFrame,d=u.getStackAddendum();d!==\\"\\"&&(t+=\\"%s\\",i=i.concat([d]));var l=i.map(function(a){return String(a)});l.unshift(\\"Warning: \\"+t),Function.prototype.apply.call(console[e],console,l)}}var Se=!1,Ce=!1,Oe=!1,Pe=!1,Fe=!1,Q;Q=Symbol.for(\\"react.module.reference\\");function Me(e){return!!(typeof e==\\"string\\"||typeof e==\\"function\\"||e===j||e===X||Fe||e===y||e===P||e===F||Pe||e===we||Se||Ce||Oe||typeof e==\\"object\\"&&e!==null&&(e.$$typeof===M||e.$$typeof===w||e.$$typeof===K||e.$$typeof===J||e.$$typeof===H||e.$$typeof===Q||e.getModuleId!==void 0))}function Ae(e,t,i){var u=e.displayName;if(u)return u;var d=t.displayName||t.name||\\"\\";return d!==\\"\\"?i+\\"(\\"+d+\\")\\":i}function ee(e){return e.displayName||\\"Context\\"}function g(e){if(e==null)return null;if(typeof e.tag==\\"number\\"&&m(\\"Received an unexpected object in getComponentNameFromType(). This is likely a bug in React. Please file an issue.\\"),typeof e==\\"function\\")return e.displayName||e.name||null;if(typeof e==\\"string\\")return e;switch(e){case j:return\\"Fragment\\";case _:return\\"Portal\\";case X:return\\"Profiler\\";case y:return\\"StrictMode\\";case P:return\\"Suspense\\";case F:return\\"SuspenseList\\"}if(typeof e==\\"object\\")switch(e.$$typeof){case J:var t=e;return ee(t)+\\".Consumer\\";case K:var i=e;return ee(i._context)+\\".Provider\\";case H:return Ae(e,e.render,\\"ForwardRef\\");case w:var u=e.displayName||null;return u!==null?u:g(e.type)||\\"Memo\\";case M:{var d=e,l=d._payload,a=d._init;try{return g(a(l))}catch{return null}}}return null}var x=Object.assign,G=0,ne,re,te,ie,ue,oe,ae;function de(){}de.__reactDisabledLog=!0;function Ie(){{if(G===0){ne=console.log,re=console.info,te=console.warn,ie=console.error,ue=console.group,oe=console.groupCollapsed,ae=console.groupEnd;var e={configurable:!0,enumerable:!0,value:de,writable:!0};Object.defineProperties(console,{info:e,log:e,warn:e,error:e,group:e,groupCollapsed:e,groupEnd:e})}G++}}function Le(){{if(G--,G===0){var e={configurable:!0,enumerable:!0,writable:!0};Object.defineProperties(console,{log:x({},e,{value:ne}),info:x({},e,{value:re}),warn:x({},e,{value:te}),error:x({},e,{value:ie}),group:x({},e,{value:ue}),groupCollapsed:x({},e,{value:oe}),groupEnd:x({},e,{value:ae})})}G<0&&m(\\"disabledDepth fell below zero. This is a bug in React. Please file an issue.\\")}}var A=v.ReactCurrentDispatcher,I;function R(e,t,i){{if(I===void 0)try{throw Error()}catch(d){var u=d.stack.trim().match(/\\\\n( *(at )?)/);I=u&&u[1]||\\"\\"}return`\\n`+I+e}}var L=!1,E;{var Ve=typeof WeakMap==\\"function\\"?WeakMap:Map;E=new Ve}function le(e,t){if(!e||L)return\\"\\";{var i=E.get(e);if(i!==void 0)return i}var u;L=!0;var d=Error.prepareStackTrace;Error.prepareStackTrace=void 0;var l;l=A.current,A.current=null,Ie();try{if(t){var a=function(){throw Error()};if(Object.defineProperty(a.prototype,\\"props\\",{set:function(){throw Error()}}),typeof Reflect==\\"object\\"&&Reflect.construct){try{Reflect.construct(a,[])}catch(N){u=N}Reflect.construct(e,[],a)}else{try{a.call()}catch(N){u=N}e.call(a.prototype)}}else{try{throw Error()}catch(N){u=N}e()}}catch(N){if(N&&u&&typeof N.stack==\\"string\\"){for(var o=N.stack.split(`\\n`),f=u.stack.split(`\\n`),c=o.length-1,b=f.length-1;c>=1&&b>=0&&o[c]!==f[b];)b--;for(;c>=1&&b>=0;c--,b--)if(o[c]!==f[b]){if(c!==1||b!==1)do if(c--,b--,b<0||o[c]!==f[b]){var p=`\\n`+o[c].replace(\\" at new \\",\\" at \\");return e.displayName&&p.includes(\\"<anonymous>\\")&&(p=p.replace(\\"<anonymous>\\",e.displayName)),typeof e==\\"function\\"&&E.set(e,p),p}while(c>=1&&b>=0);break}}}finally{L=!1,A.current=l,Le(),Error.prepareStackTrace=d}var D=e?e.displayName||e.name:\\"\\",je=D?R(D):\\"\\";return typeof e==\\"function\\"&&E.set(e,je),je}function We(e,t,i){return le(e,!1)}function Ye(e){var t=e.prototype;return!!(t&&t.isReactComponent)}function T(e,t,i){if(e==null)return\\"\\";if(typeof e==\\"function\\")return le(e,Ye(e));if(typeof e==\\"string\\")return R(e);switch(e){case P:return R(\\"Suspense\\");case F:return R(\\"SuspenseList\\")}if(typeof e==\\"object\\")switch(e.$$typeof){case H:return We(e.render);case w:return T(e.type,t,i);case M:{var u=e,d=u._payload,l=u._init;try{return T(l(d),t,i)}catch{}}}return\\"\\"}var S=Object.prototype.hasOwnProperty,se={},ce=v.ReactDebugCurrentFrame;function C(e){if(e){var t=e._owner,i=T(e.type,e._source,t?t.type:null);ce.setExtraStackFrame(i)}else ce.setExtraStackFrame(null)}function $e(e,t,i,u,d){{var l=Function.call.bind(S);for(var a in e)if(l(e,a)){var o=void 0;try{if(typeof e[a]!=\\"function\\"){var f=Error((u||\\"React class\\")+\\": \\"+i+\\" type `\\"+a+\\"` is invalid; it must be a function, usually from the `prop-types` package, but received `\\"+typeof e[a]+\\"`.This often happens because of typos such as `PropTypes.function` instead of `PropTypes.func`.\\");throw f.name=\\"Invariant Violation\\",f}o=e[a](t,a,u,i,null,\\"SECRET_DO_NOT_PASS_THIS_OR_YOU_WILL_BE_FIRED\\")}catch(c){o=c}o&&!(o instanceof Error)&&(C(d),m(\\"%s: type specification of %s `%s` is invalid; the type checker function must return `null` or an `Error` but returned a %s. You may have forgotten to pass an argument to the type checker creator (arrayOf, instanceOf, objectOf, oneOf, oneOfType, and shape all require an argument).\\",u||\\"React class\\",i,a,typeof o),C(null)),o instanceof Error&&!(o.message in se)&&(se[o.message]=!0,C(d),m(\\"Failed %s type: %s\\",i,o.message),C(null))}}}var qe=Array.isArray;function V(e){return qe(e)}function Be(e){{var t=typeof Symbol==\\"function\\"&&Symbol.toStringTag,i=t&&e[Symbol.toStringTag]||e.constructor.name||\\"Object\\";return i}}function ze(e){try{return be(e),!1}catch{return!0}}function be(e){return\\"\\"+e}function me(e){if(ze(e))return m(\\"The provided key is an unsupported type %s. This value must be coerced to a string before before using it here.\\",Be(e)),be(e)}var U=v.ReactCurrentOwner,Xe={key:!0,ref:!0,__self:!0,__source:!0},fe,he,W;W={};function Ke(e){if(S.call(e,\\"ref\\")){var t=Object.getOwnPropertyDescriptor(e,\\"ref\\").get;if(t&&t.isReactWarning)return!1}return e.ref!==void 0}function Je(e){if(S.call(e,\\"key\\")){var t=Object.getOwnPropertyDescriptor(e,\\"key\\").get;if(t&&t.isReactWarning)return!1}return e.key!==void 0}function Ze(e,t){if(typeof e.ref==\\"string\\"&&U.current&&t&&U.current.stateNode!==t){var i=g(U.current.type);W[i]||(m(\'Component \\"%s\\" contains the string ref \\"%s\\". Support for string refs will be removed in a future major release. This case cannot be automatically converted to an arrow function. We ask you to manually fix this case by using useRef() or createRef() instead. Learn more about using refs safely here: https://reactjs.org/link/strict-mode-string-ref\',g(U.current.type),e.ref),W[i]=!0)}}function Qe(e,t){{var i=function(){fe||(fe=!0,m(\\"%s: `key` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\\",t))};i.isReactWarning=!0,Object.defineProperty(e,\\"key\\",{get:i,configurable:!0})}}function en(e,t){{var i=function(){he||(he=!0,m(\\"%s: `ref` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\\",t))};i.isReactWarning=!0,Object.defineProperty(e,\\"ref\\",{get:i,configurable:!0})}}var nn=function(e,t,i,u,d,l,a){var o={$$typeof:n,type:e,key:t,ref:i,props:a,_owner:l};return o._store={},Object.defineProperty(o._store,\\"validated\\",{configurable:!1,enumerable:!1,writable:!0,value:!1}),Object.defineProperty(o,\\"_self\\",{configurable:!1,enumerable:!1,writable:!1,value:u}),Object.defineProperty(o,\\"_source\\",{configurable:!1,enumerable:!1,writable:!1,value:d}),Object.freeze&&(Object.freeze(o.props),Object.freeze(o)),o};function rn(e,t,i,u,d){{var l,a={},o=null,f=null;i!==void 0&&(me(i),o=\\"\\"+i),Je(t)&&(me(t.key),o=\\"\\"+t.key),Ke(t)&&(f=t.ref,Ze(t,d));for(l in t)S.call(t,l)&&!Xe.hasOwnProperty(l)&&(a[l]=t[l]);if(e&&e.defaultProps){var c=e.defaultProps;for(l in c)a[l]===void 0&&(a[l]=c[l])}if(o||f){var b=typeof e==\\"function\\"?e.displayName||e.name||\\"Unknown\\":e;o&&Qe(a,b),f&&en(a,b)}return nn(e,o,f,d,u,U.current,a)}}var Y=v.ReactCurrentOwner,pe=v.ReactDebugCurrentFrame;function k(e){if(e){var t=e._owner,i=T(e.type,e._source,t?t.type:null);pe.setExtraStackFrame(i)}else pe.setExtraStackFrame(null)}var $;$=!1;function q(e){return typeof e==\\"object\\"&&e!==null&&e.$$typeof===n}function _e(){{if(Y.current){var e=g(Y.current.type);if(e)return`\\n\\nCheck the render method of \\\\``+e+\\"`.\\"}return\\"\\"}}function tn(e){{if(e!==void 0){var t=e.fileName.replace(/^.*[\\\\\\\\\\\\/]/,\\"\\"),i=e.lineNumber;return`\\n\\nCheck your code at `+t+\\":\\"+i+\\".\\"}return\\"\\"}}var ge={};function un(e){{var t=_e();if(!t){var i=typeof e==\\"string\\"?e:e.displayName||e.name;i&&(t=`\\n\\nCheck the top-level render call using <`+i+\\">.\\")}return t}}function Ne(e,t){{if(!e._store||e._store.validated||e.key!=null)return;e._store.validated=!0;var i=un(t);if(ge[i])return;ge[i]=!0;var u=\\"\\";e&&e._owner&&e._owner!==Y.current&&(u=\\" It was passed a child from \\"+g(e._owner.type)+\\".\\"),k(e),m(\'Each child in a list should have a unique \\"key\\" prop.%s%s See https://reactjs.org/link/warning-keys for more information.\',i,u),k(null)}}function ye(e,t){{if(typeof e!=\\"object\\")return;if(V(e))for(var i=0;i<e.length;i++){var u=e[i];q(u)&&Ne(u,t)}else if(q(e))e._store&&(e._store.validated=!0);else if(e){var d=Ee(e);if(typeof d==\\"function\\"&&d!==e.entries)for(var l=d.call(e),a;!(a=l.next()).done;)q(a.value)&&Ne(a.value,t)}}}function on(e){{var t=e.type;if(t==null||typeof t==\\"string\\")return;var i;if(typeof t==\\"function\\")i=t.propTypes;else if(typeof t==\\"object\\"&&(t.$$typeof===H||t.$$typeof===w))i=t.propTypes;else return;if(i){var u=g(t);$e(i,e.props,\\"prop\\",u,e)}else if(t.PropTypes!==void 0&&!$){$=!0;var d=g(t);m(\\"Component %s declared `PropTypes` instead of `propTypes`. Did you misspell the property assignment?\\",d||\\"Unknown\\")}typeof t.getDefaultProps==\\"function\\"&&!t.getDefaultProps.isReactClassApproved&&m(\\"getDefaultProps is only used on classic React.createClass definitions. Use a static property named `defaultProps` instead.\\")}}function an(e){{for(var t=Object.keys(e.props),i=0;i<t.length;i++){var u=t[i];if(u!==\\"children\\"&&u!==\\"key\\"){k(e),m(\\"Invalid prop `%s` supplied to `React.Fragment`. React.Fragment can only have `key` and `children` props.\\",u),k(null);break}}e.ref!==null&&(k(e),m(\\"Invalid attribute `ref` supplied to `React.Fragment`.\\"),k(null))}}function dn(e,t,i,u,d,l){{var a=Me(e);if(!a){var o=\\"\\";(e===void 0||typeof e==\\"object\\"&&e!==null&&Object.keys(e).length===0)&&(o+=\\" You likely forgot to export your component from the file it\'s defined in, or you might have mixed up default and named imports.\\");var f=tn(d);f?o+=f:o+=_e();var c;e===null?c=\\"null\\":V(e)?c=\\"array\\":e!==void 0&&e.$$typeof===n?(c=\\"<\\"+(g(e.type)||\\"Unknown\\")+\\" />\\",o=\\" Did you accidentally export a JSX literal instead of a component?\\"):c=typeof e,m(\\"React.jsx: type is invalid -- expected a string (for built-in components) or a class/function (for composite components) but got: %s.%s\\",c,o)}var b=rn(e,t,i,d,l);if(b==null)return b;if(a){var p=t.children;if(p!==void 0)if(u)if(V(p)){for(var D=0;D<p.length;D++)ye(p[D],e);Object.freeze&&Object.freeze(p)}else m(\\"React.jsx: Static children should always be an array. You are likely explicitly calling React.jsxs or React.jsxDEV. Use the Babel transform instead.\\");else ye(p,e)}return e===j?an(b):on(b),b}}var ln=dn;z.Fragment=j,z.jsxDEV=ln})()});var Ue=B((Dn,Ge)=>{\\"use strict\\";Ge.exports=De()});var jn={};hn(jn,{default:()=>yn,frontmatter:()=>gn});var r=pn(Ue()),gn={title:\\"[Paper Review] LSTM & GRU\\",description:\\"DDIM (Denoising Diffusion Implicit Models) is a deep learning model for efficient image generation through a refined diffusion denoising process.\\",image:\\"../../public/blogs/lstm-and-gru/screenshot.png\\",publishedAt:\\"2024-07-10\\",updatedAt:\\"2024-07-10\\",author:\\"junbrro\\",isPublished:!0,tags:[\\"Deep Learning\\"]};function He(s){let n=Object.assign({p:\\"p\\",a:\\"a\\",h1:\\"h1\\",span:\\"span\\",strong:\\"strong\\",img:\\"img\\",ul:\\"ul\\",li:\\"li\\",h3:\\"h3\\"},s.components);return(0,r.jsxDEV)(r.Fragment,{children:[(0,r.jsxDEV)(n.p,{children:\\"This post is reviewing the LSTM & GRU model structure, based on RNN basics.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:13,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:[\\"Thumbnail: \\",(0,r.jsxDEV)(n.a,{href:\\"https://link.springer.com/article/10.1007/s11269-022-03397-6\\",children:\\"SpringerLink\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:15,columnNumber:12},this)]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:15,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h1,{id:\\"background-rnn-recurrent-neural-network\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#background-rnn-recurrent-neural-network\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\"},this),\\"Background: RNN (Recurrent Neural Network)\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:17,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:[(0,r.jsxDEV)(n.strong,{children:\\"Recurrent Neural Networks (RNNs)\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:19,columnNumber:1},this),\\" are a class of neural networks designed to process sequential data of varying lengths. Unlike traditional feedforward neural networks, \\",(0,r.jsxDEV)(n.strong,{children:\\"RNNs maintain a form of internal memory through their recurrent hidden states\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:19,columnNumber:173},this),\\", allowing them to exhibit dynamic temporal behavior. This feature makes RNNs particularly suitable for tasks where the current input is dependent on prior inputs, such as natural language processing or time series analysis.\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:19,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:\\"For any given sequence X=(x1,x2,x3,...,xr), the RNN updates its hidden state ht at each time step t, which is a function of the current input xt and the previous hidden state ht-1. Mathematically, this can be represented as:\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:21,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/6893aacc-b65e-4b1e-befa-12ec056af38b\\",alt:\\"RNN Formula\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:23,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:23,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:\\"where f is a nonlinear activation function, xt is the input at time step t, and ht-1 is the hidden state from the previous time step.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:25,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/7bd51890-4470-438f-8c86-698e66c84a7d\\",alt:\\"RNN Diagram\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:27,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:27,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/6407a265-e076-435f-a5da-4630cb5a90f2\\",alt:\\"RNN Structure\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:29,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:29,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/a5f693c6-e0f6-4b1c-ba08-f31a2e57791e\\",alt:\\"RNN Overview\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:31,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:31,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:[(0,r.jsxDEV)(n.strong,{children:\\"Cons:\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:33,columnNumber:1},this),\\" RNNs are hard to train with long input sequences due to several issues:\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:33,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.ul,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:(0,r.jsxDEV)(n.strong,{children:\\"Too many calculations\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:35,columnNumber:3},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:35,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:(0,r.jsxDEV)(n.strong,{children:\\"Gradient vanishing\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:36,columnNumber:3},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:36,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:(0,r.jsxDEV)(n.strong,{children:\\"Gradient exploding\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:37,columnNumber:3},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:37,columnNumber:1},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:35,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:[(0,r.jsxDEV)(n.strong,{children:\\"Solutions:\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:39,columnNumber:1},this),\\" Clipping Gradient, Using Gating Unit\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:39,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h3,{id:\\"clipping-gradient\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#clipping-gradient\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\"},this),\\"Clipping Gradient\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:41,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/337908c7-37ba-410e-ad30-b97b45eb6971\\",alt:\\"Gradient Clipping\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:43,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:43,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:[(0,r.jsxDEV)(n.strong,{children:\\"Gradient clipping\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:45,columnNumber:1},this),\' involves limiting (or \\"clipping\\") the gradients to a defined range or threshold during the backpropagation process to ensure they do not become too large (exploding) or too small (vanishing).\']},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:45,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:[(0,r.jsxDEV)(n.strong,{children:\\"How It Works\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:47,columnNumber:1},this),\\":\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:47,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.ul,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:\\"By Value: Gradient values are clipped directly if they exceed a predefined threshold. For example, all gradient components greater than a value are set to that value, and all those less than a negative of that value are set to its negative.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:49,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:\\"By Norm: The gradients are scaled down proportionally if the norm of the gradient vector exceeds a specified threshold. This approach keeps the direction of the gradient but reduces its magnitude to avoid exploding gradients.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:50,columnNumber:1},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:49,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h3,{id:\\"using-gating-unit\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#using-gating-unit\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\"},this),\\"Using Gating Unit\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:52,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/9a6bc2f6-1971-4c1c-907a-c39e2b1d2aa3\\",alt:\\"Gating Unit\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:54,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:54,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:\\"This paper mainly explains about GRU, comparing the basic structures with LSTM.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:56,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h1,{id:\\"lstm-long-short-term-memory\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#lstm-long-short-term-memory\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\"},this),\\"LSTM (Long Short-Term Memory)\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:58,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/34029be4-ee80-4afa-b3c7-ab59d217604d\\",alt:\\"LSTM Diagram\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:60,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:60,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.ul,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\\"Forget Gate\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:62,columnNumber:3},this),\\": Decides what information to discard from the cell state, using a sigmoid function to assign values close to 0 for information to forget and values close to 1 for information to retain.\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:62,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\\"Input Gate\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:63,columnNumber:3},this),\\": Determines which new information to update in the cell state, combining a sigmoid layer to select values and a tanh layer to create a vector of candidate values.\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:63,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\\"Cell State\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:64,columnNumber:3},this),\\": Serves as the LSTM\'s memory, carrying relevant information across the sequence, and is updated based on inputs from the forget and input gates.\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:64,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\\"Output Gate\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:65,columnNumber:3},this),\\": Controls what part of the cell state is passed to the output, using a sigmoid layer to select parts of the cell state and a tanh layer to scale these selected parts before producing the final output.\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:65,columnNumber:1},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:62,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/ab2c532a-07b2-46ad-98c7-4fb671b4a7f7\\",alt:\\"LSTM Output Calculation\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:67,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:67,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:\\"o_jt is the total sum of memory content exposure (output gate).\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:69,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/6ec0bc8a-34dd-4535-8936-649abd49ad45\\",alt:\\"Memory Content Exposure\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:71,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:71,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:\\"V0 is a diagonal matrix.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:73,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:\\"The memory cell state c_Bar*jt forgets some of the previous content and adds new memory content cbar_jt.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:75,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/4b8b2521-d99d-4d46-9454-d245bcb0dda6\\",alt:\\"Memory Cell State\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:77,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:77,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/81655588-08a9-40f2-8ed2-b6e51bf28363\\",alt:\\"New Memory Content\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:79,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:79,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:\\"The forget gate calculation f_jt determines how much of the new content will be added, which is controlled by the input gate i_jt.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:81,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/4aa734d4-8fa6-442e-8d8b-dca3bc65cecf\\",alt:\\"Forget Gate Calculation\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:83,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:83,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:\\"Here, Vf is a diagonal matrix of Vi.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:85,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h1,{id:\\"gru-gated-recurrent-unit\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#gru-gated-recurrent-unit\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\"},this),\\"GRU (Gated Recurrent Unit)\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:87,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/20737357-da6c-428b-918f-2dd14eaa1489\\",alt:\\"GRU Diagram\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:89,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:89,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.ul,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\\"Update Gate:\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:91,columnNumber:3},this),\\" Determines the amount of past information to keep versus new information to add, using a sigmoid function to balance between the previous activation and the potential new candidate activation.\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:91,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\\"Reset Gate:\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:92,columnNumber:3},this),\\" Decides how much of the past information to forget, allowing the model to drop irrelevant information from the previous steps for the current prediction.\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:92,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\\"Candidate Activation:\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:93,columnNumber:3},this),\\" Combines the current input with the past memory, influenced by the reset gate, to create a candidate for the new hidden state, blending old and new information with the potential to update the model\\\\u2019s memory.\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:93,columnNumber:1},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:91,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:\\"The linear interpolation between h*jt-1 and hbar_jt works as the activation of GRU.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:95,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/1c02cfa7-234d-4512-b10d-201b9afee926\\",alt:\\"GRU Activation\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:97,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:97,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:\\"Here, the update gate z_jt is the value of updated content.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:99,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/3a59ab74-d0b3-4418-b708-647bf4c639c5\\",alt:\\"Update Gate\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:101,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:101,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:[\\"These processes to get the linear sum of the existing state and the new state (candidate activation) are similar to LSTM calculations. However, the difference is that GRU cannot adjust the amount of state exposure. ( \\\\\\\\tilde\\",h,\\"_\\",jt,\\" ) is calculated as below.\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:103,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/3e01a169-d3fb-4834-abd1-10d4a03f7495\\",alt:\\"Candidate Activation Calculation\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:105,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:105,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:\\"The reset gate has a similar process.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:107,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/81e4267d-557a-4ecf-8ad8-412109396f74\\",alt:\\"Reset Gate\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:109,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:109,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h1,{id:\\"experiment-validation\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#experiment-validation\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\"},this),\\"Experiment (Validation)\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:111,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:[(0,r.jsxDEV)(n.strong,{children:\\"Sequence Modeling\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:113,columnNumber:1},this),\\" aims at learning a probability distribution over sequences.\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:113,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/4c12254a-97bc-4ad6-8388-c52fe9a6bf0b\\",alt:\\"Sequence Modeling\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:115,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:115,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:\\"Training three different units (LSTM, GRU, tanh unit). Used RMSProp as the optimizer, and weight noise is fixed at 0.075 (standard deviation). The magnitude of the gradient\\\\u2019s norm cannot exceed 1 to prevent gradient exploding.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:117,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/06d04510-578b-4115-8cf1-f85036d20bd6\\",alt:\\"Training Units\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:119,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:119,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/de7b170b-82f1-4439-8ae4-7e86b9dafc9e\\",alt:\\"Training Results\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:121,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:121,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h1,{id:\\"result\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#result\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\"},this),\\"Result\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:123,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/80cf294a-70ea-4726-a510-43a946afb614\\",alt:\\"Experiment Result\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:125,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:125,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:[(0,r.jsxDEV)(n.strong,{children:\\"In the case of GRU\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:127,columnNumber:1},this),\\", LSTM\\\\u2019s four gates are reduced to two gates (update, reset gate). Also, all states are provided at once (single hidden state). Since the number of gates is smaller, the number of parameters is also less than LSTM.\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:127,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:[(0,r.jsxDEV)(n.strong,{children:\\"Actually, the ability of GRU is not dramatically higher than LSTM.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:129,columnNumber:1},this),\\" Still, GRU has the advantage of requiring fewer calculations and faster speed.\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:129,columnNumber:1},this)]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\",lineNumber:1,columnNumber:1},this)}function Nn(s={}){let{wrapper:n}=s.components||{};return n?(0,r.jsxDEV)(n,Object.assign({},s,{children:(0,r.jsxDEV)(He,s,void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\"},this)}),void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-099d8042-509d-4573-a203-dfcb30876353.mdx\\"},this):He(s)}var yn=Nn;return _n(jn);})();\\n/*! Bundled license information:\\n\\nreact/cjs/react-jsx-dev-runtime.development.js:\\n  (**\\n   * @license React\\n   * react-jsx-dev-runtime.development.js\\n   *\\n   * Copyright (c) Facebook, Inc. and its affiliates.\\n   *\\n   * This source code is licensed under the MIT license found in the\\n   * LICENSE file in the root directory of this source tree.\\n   *)\\n*/\\n;return Component;"},"_id":"lstm-and-gru/index.mdx","_raw":{"sourceFilePath":"lstm-and-gru/index.mdx","sourceFileName":"index.mdx","sourceFileDir":"lstm-and-gru","contentType":"mdx","flattenedPath":"lstm-and-gru"},"type":"Blog","url":"/blogs/lstm-and-gru","readingTime":{"text":"5 min read","minutes":4.275,"time":256500,"words":855},"toc":[{"level":"one","text":"Background: RNN (Recurrent Neural Network)","slug":"background-rnn-recurrent-neural-network"},{"level":"three","text":"Clipping Gradient","slug":"clipping-gradient"},{"level":"three","text":"Using Gating Unit","slug":"using-gating-unit"},{"level":"one","text":"LSTM (Long Short-Term Memory)","slug":"lstm-long-short-term-memory"},{"level":"one","text":"GRU (Gated Recurrent Unit)","slug":"gru-gated-recurrent-unit"},{"level":"one","text":"Experiment (Validation)","slug":"experiment-validation"},{"level":"one","text":"Result","slug":"result"}]}');

/***/ })

});