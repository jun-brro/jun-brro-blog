"use strict";
/*
 * ATTENTION: An "eval-source-map" devtool has been used.
 * This devtool is neither made for production nor for readable output files.
 * It uses "eval()" calls to create a separate source file with attached SourceMaps in the browser devtools.
 * If you are trying to read the output file, select a different devtool (https://webpack.js.org/configuration/devtool/)
 * or disable the default devtool with "devtool: false".
 * If you are looking for production-ready output files, see mode: "production" (https://webpack.js.org/configuration/mode/).
 */
self["webpackHotUpdate_N_E"]("app/page",{

/***/ "(app-pages-browser)/./.contentlayer/generated/Blog/slowfast__index.mdx.json":
/*!***************************************************************!*\
  !*** ./.contentlayer/generated/Blog/slowfast__index.mdx.json ***!
  \***************************************************************/
/***/ (function(module, __unused_webpack_exports, __webpack_require__) {

module.exports = JSON.parse('{"title":"[Paper Review] SlowFast Networks for Video Recognition","publishedAt":"2024-07-23T00:00:00.000Z","updatedAt":"2024-07-23T00:00:00.000Z","description":"The SlowFast network employs dual pathways, with the Slow Pathway capturing high-resolution spatial details and the Fast Pathway capturing rapid temporal changes, to achieve advanced video recognition.","image":{"filePath":"../public/blogs/slowfast/screenshot.png","relativeFilePath":"../../public/blogs/slowfast/screenshot.png","format":"png","height":1276,"width":2528,"aspectRatio":1.9811912225705328,"blurhashDataUrl":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAKlBMVEXr7Oz19fT////d4eDm5+fw8PD5+fnW1tXMzM6oqKfIz9q2wNK4ubq8vb9psxfqAAAACXBIWXMAABYlAAAWJQFJUiTwAAAANklEQVR4nAXBiQHAMAwCsQMc5+3+61aCwaJKrJn7nfsInFkJti21mOVWD7EXoQK7FIMZltz2DyemAPZRxtcpAAAAAElFTkSuQmCC"},"isPublished":true,"author":"junbrro","tags":["Deep Learning"],"body":{"raw":"\\nThis post is the review of slowfast paper.\\n\\n[**Cited - Joochan Github Blog**](https://joochann.github.io/posts/SlowFast-Networks-for-Video-Recognition/)\\n\\n# SlowFast Networks for Video Recognition\\n\\nVideo recognition, unlike image recognition, must account for the time domain since the essence of action involves changes over time. Video data needs to capture movement in a three-dimensional space, including the temporal axis, making simple extensions of 2D image processing techniques insufficient. Therefore, a structure is necessary that can introduce existing vision processing techniques on a frame-by-frame basis while retaining past information. Just as neural networks were inspired by neural cells, the SlowFast paper draws from the brain\'s structure for action recognition.\\n\\n![SlowFast Network Structure](https://github.com/user-attachments/assets/9a060284-ba46-4e58-93b5-8b59c6f4dd2d)\\n\\n## SlowFast Network Architecture\\n\\nThe SlowFast network is inspired by the human visual system, particularly the M cells and P cells. The Slow Pathway processes high-resolution spatial information, while the Fast Pathway focuses on temporal information such as motion. Mimicking this structure, the SlowFast network comprises two distinct pathways.\\n\\n### Slow Pathway\\n\\nThe Slow Pathway processes input data at a low frame rate, capturing high-resolution spatial information such as color and spatial details. This pathway is designed based on ResNet and aims to retain significant spatial information while improving computational efficiency. The pathway operates on several key principles:\\n\\n- **Low Frame Rate Sampling**: This reduces the number of frames, focusing on detailed spatial features. By sampling fewer frames, the pathway can dedicate more computational resources to analyzing each frame in greater detail.\\n- **High Spatial Resolution**: Preserves detailed spatial information, which is crucial for understanding the scene context and object characteristics. This helps in capturing fine details that are necessary for recognizing complex objects and scenes.\\n\\nIf X_s represents the input to the Slow Pathway and f_s the transformation function, then:\\nY_s = f_s(X_s)\\nwhere Y_s is the output feature map with preserved high spatial resolution.\\n\\n### Fast Pathway\\n\\nThe Fast Pathway processes input data at a high frame rate, focusing on capturing temporal information such as motion. This pathway also uses a ResNet-based design but samples more frames densely while reducing the number of channels to save computational resources. Key aspects include:\\n\\n- **High Frame Rate Sampling**: Captures temporal changes effectively by processing more frames. This enables the pathway to accurately detect and analyze motion, which is crucial for understanding dynamic scenes.\\n- **Channel Reduction**: Reduces the number of channels to optimize computational efficiency while retaining critical motion information. By lowering the channel count, the network can process frames quickly without compromising too much on the quality of motion detection.\\n\\nIf X_f represents the input to the Fast Pathway and f_f the transformation function, then:\\nY_f = f_f(X_f)\\nwhere Y_f is the output feature map emphasizing temporal information.\\n\\n![Pathway Details](https://github.com/user-attachments/assets/0d00197a-8c57-4adb-9d8b-1bb8d48175db)\\n\\n### Lateral Connections\\n\\nTo integrate the information from both pathways, the SlowFast network employs lateral connections to exchange information between the Slow and Fast Pathways. These connections enable complementary effects, allowing each pathway to supplement what the other has not learned. The Slow Pathway provides high-resolution spatial details, while the Fast Pathway captures temporal changes, enabling comprehensive video recognition.\\n\\nThe lateral connections can be represented as:\\nY_s\' = Y_s + g(Y_f)\\nY_f\' = Y_f + h(Y_s)\\nwhere g and h are transformation functions that map the feature maps between the pathways to enable effective information exchange.\\n\\n![Lateral Connection Diagram](https://github.com/user-attachments/assets/837c45cd-13d1-4fde-bf83-2454b5542149)\\n\\n## Experiments and Validation\\n\\nThe SlowFast network has been evaluated on various large-scale video datasets. Key datasets include Kinetics-400, Kinetics-600, Charades, and AVA.\\n\\n### Kinetics-400 / 600\\n\\nKinetics-400 is a video dataset collected from YouTube, comprising 400 classes of actions. It includes 240,000 training video clips and 20,000 validation clips. Kinetics-600 is an extended version with 600 classes and more video clips.\\n\\n![Kinetics Dataset](https://github.com/user-attachments/assets/91a84661-54c8-48fe-8fe1-967174f31620)\\n\\n### AVA\\n\\nThe AVA (Atomic Visual Actions) dataset is a high-resolution labeled dataset for human actions in videos. Each frame includes bounding boxes and action tags indicating the actions performed by individuals.\\n\\n![AVA Dataset](https://github.com/user-attachments/assets/69c74eef-704e-45ab-9c02-ad44e9cd9199)\\n\\n### Key Experimental Results\\n\\nOn the Kinetics-400 and Kinetics-600 datasets, the SlowFast network outperformed existing state-of-the-art models, particularly those based on 3D convolution. It demonstrated higher accuracy and efficiency. In the Charades dataset, the network showcased its ability to recognize multiple actions simultaneously. In the AVA dataset, it excelled in action detection, proving its effectiveness in capturing both spatial and temporal information.\\n\\nBy integrating the Slow and Fast Pathways, the SlowFast network achieves a balance of spatial and temporal feature extraction, leading to superior performance in video recognition tasks.\\n","code":"var Component=(()=>{var ln=Object.create;var H=Object.defineProperty;var dn=Object.getOwnPropertyDescriptor;var cn=Object.getOwnPropertyNames;var fn=Object.getPrototypeOf,mn=Object.prototype.hasOwnProperty;var K=(d,r)=>()=>(r||d((r={exports:{}}).exports,r),r.exports),bn=(d,r)=>{for(var p in r)H(d,p,{get:r[p],enumerable:!0})},xe=(d,r,p,v)=>{if(r&&typeof r==\\"object\\"||typeof r==\\"function\\")for(let y of cn(r))!mn.call(d,y)&&y!==p&&H(d,y,{get:()=>r[y],enumerable:!(v=dn(r,y))||v.enumerable});return d};var hn=(d,r,p)=>(p=d!=null?ln(fn(d)):{},xe(r||!d||!d.__esModule?H(p,\\"default\\",{value:d,enumerable:!0}):p,d)),pn=d=>xe(H({},\\"__esModule\\",{value:!0}),d);var je=K((Nn,Ne)=>{Ne.exports=React});var we=K(B=>{\\"use strict\\";(function(){\\"use strict\\";var d=je(),r=Symbol.for(\\"react.element\\"),p=Symbol.for(\\"react.portal\\"),v=Symbol.for(\\"react.fragment\\"),y=Symbol.for(\\"react.strict_mode\\"),z=Symbol.for(\\"react.profiler\\"),q=Symbol.for(\\"react.provider\\"),X=Symbol.for(\\"react.context\\"),E=Symbol.for(\\"react.forward_ref\\"),C=Symbol.for(\\"react.suspense\\"),O=Symbol.for(\\"react.suspense_list\\"),R=Symbol.for(\\"react.memo\\"),F=Symbol.for(\\"react.lazy\\"),Re=Symbol.for(\\"react.offscreen\\"),J=Symbol.iterator,Te=\\"@@iterator\\";function Ue(e){if(e===null||typeof e!=\\"object\\")return null;var n=J&&e[J]||e[Te];return typeof n==\\"function\\"?n:null}var N=d.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED;function m(e){{for(var n=arguments.length,a=new Array(n>1?n-1:0),i=1;i<n;i++)a[i-1]=arguments[i];Se(\\"error\\",e,a)}}function Se(e,n,a){{var i=N.ReactDebugCurrentFrame,s=i.getStackAddendum();s!==\\"\\"&&(n+=\\"%s\\",a=a.concat([s]));var l=a.map(function(u){return String(u)});l.unshift(\\"Warning: \\"+n),Function.prototype.apply.call(console[e],console,l)}}var Pe=!1,Ge=!1,He=!1,Ce=!1,Oe=!1,Z;Z=Symbol.for(\\"react.module.reference\\");function Fe(e){return!!(typeof e==\\"string\\"||typeof e==\\"function\\"||e===v||e===z||Oe||e===y||e===C||e===O||Ce||e===Re||Pe||Ge||He||typeof e==\\"object\\"&&e!==null&&(e.$$typeof===F||e.$$typeof===R||e.$$typeof===q||e.$$typeof===X||e.$$typeof===E||e.$$typeof===Z||e.getModuleId!==void 0))}function Ae(e,n,a){var i=e.displayName;if(i)return i;var s=n.displayName||n.name||\\"\\";return s!==\\"\\"?a+\\"(\\"+s+\\")\\":a}function Q(e){return e.displayName||\\"Context\\"}function _(e){if(e==null)return null;if(typeof e.tag==\\"number\\"&&m(\\"Received an unexpected object in getComponentNameFromType(). This is likely a bug in React. Please file an issue.\\"),typeof e==\\"function\\")return e.displayName||e.name||null;if(typeof e==\\"string\\")return e;switch(e){case v:return\\"Fragment\\";case p:return\\"Portal\\";case z:return\\"Profiler\\";case y:return\\"StrictMode\\";case C:return\\"Suspense\\";case O:return\\"SuspenseList\\"}if(typeof e==\\"object\\")switch(e.$$typeof){case X:var n=e;return Q(n)+\\".Consumer\\";case q:var a=e;return Q(a._context)+\\".Provider\\";case E:return Ae(e,e.render,\\"ForwardRef\\");case R:var i=e.displayName||null;return i!==null?i:_(e.type)||\\"Memo\\";case F:{var s=e,l=s._payload,u=s._init;try{return _(u(l))}catch{return null}}}return null}var x=Object.assign,k=0,ee,ne,re,te,ae,ie,oe;function ue(){}ue.__reactDisabledLog=!0;function Ie(){{if(k===0){ee=console.log,ne=console.info,re=console.warn,te=console.error,ae=console.group,ie=console.groupCollapsed,oe=console.groupEnd;var e={configurable:!0,enumerable:!0,value:ue,writable:!0};Object.defineProperties(console,{info:e,log:e,warn:e,error:e,group:e,groupCollapsed:e,groupEnd:e})}k++}}function Ye(){{if(k--,k===0){var e={configurable:!0,enumerable:!0,writable:!0};Object.defineProperties(console,{log:x({},e,{value:ee}),info:x({},e,{value:ne}),warn:x({},e,{value:re}),error:x({},e,{value:te}),group:x({},e,{value:ae}),groupCollapsed:x({},e,{value:ie}),groupEnd:x({},e,{value:oe})})}k<0&&m(\\"disabledDepth fell below zero. This is a bug in React. Please file an issue.\\")}}var A=N.ReactCurrentDispatcher,I;function T(e,n,a){{if(I===void 0)try{throw Error()}catch(s){var i=s.stack.trim().match(/\\\\n( *(at )?)/);I=i&&i[1]||\\"\\"}return`\\n`+I+e}}var Y=!1,U;{var Ve=typeof WeakMap==\\"function\\"?WeakMap:Map;U=new Ve}function se(e,n){if(!e||Y)return\\"\\";{var a=U.get(e);if(a!==void 0)return a}var i;Y=!0;var s=Error.prepareStackTrace;Error.prepareStackTrace=void 0;var l;l=A.current,A.current=null,Ie();try{if(n){var u=function(){throw Error()};if(Object.defineProperty(u.prototype,\\"props\\",{set:function(){throw Error()}}),typeof Reflect==\\"object\\"&&Reflect.construct){try{Reflect.construct(u,[])}catch(g){i=g}Reflect.construct(e,[],u)}else{try{u.call()}catch(g){i=g}e.call(u.prototype)}}else{try{throw Error()}catch(g){i=g}e()}}catch(g){if(g&&i&&typeof g.stack==\\"string\\"){for(var o=g.stack.split(`\\n`),b=i.stack.split(`\\n`),c=o.length-1,f=b.length-1;c>=1&&f>=0&&o[c]!==b[f];)f--;for(;c>=1&&f>=0;c--,f--)if(o[c]!==b[f]){if(c!==1||f!==1)do if(c--,f--,f<0||o[c]!==b[f]){var h=`\\n`+o[c].replace(\\" at new \\",\\" at \\");return e.displayName&&h.includes(\\"<anonymous>\\")&&(h=h.replace(\\"<anonymous>\\",e.displayName)),typeof e==\\"function\\"&&U.set(e,h),h}while(c>=1&&f>=0);break}}}finally{Y=!1,A.current=l,Ye(),Error.prepareStackTrace=s}var w=e?e.displayName||e.name:\\"\\",ve=w?T(w):\\"\\";return typeof e==\\"function\\"&&U.set(e,ve),ve}function $e(e,n,a){return se(e,!1)}function Me(e){var n=e.prototype;return!!(n&&n.isReactComponent)}function S(e,n,a){if(e==null)return\\"\\";if(typeof e==\\"function\\")return se(e,Me(e));if(typeof e==\\"string\\")return T(e);switch(e){case C:return T(\\"Suspense\\");case O:return T(\\"SuspenseList\\")}if(typeof e==\\"object\\")switch(e.$$typeof){case E:return $e(e.render);case R:return S(e.type,n,a);case F:{var i=e,s=i._payload,l=i._init;try{return S(l(s),n,a)}catch{}}}return\\"\\"}var P=Object.prototype.hasOwnProperty,le={},de=N.ReactDebugCurrentFrame;function G(e){if(e){var n=e._owner,a=S(e.type,e._source,n?n.type:null);de.setExtraStackFrame(a)}else de.setExtraStackFrame(null)}function We(e,n,a,i,s){{var l=Function.call.bind(P);for(var u in e)if(l(e,u)){var o=void 0;try{if(typeof e[u]!=\\"function\\"){var b=Error((i||\\"React class\\")+\\": \\"+a+\\" type `\\"+u+\\"` is invalid; it must be a function, usually from the `prop-types` package, but received `\\"+typeof e[u]+\\"`.This often happens because of typos such as `PropTypes.function` instead of `PropTypes.func`.\\");throw b.name=\\"Invariant Violation\\",b}o=e[u](n,u,i,a,null,\\"SECRET_DO_NOT_PASS_THIS_OR_YOU_WILL_BE_FIRED\\")}catch(c){o=c}o&&!(o instanceof Error)&&(G(s),m(\\"%s: type specification of %s `%s` is invalid; the type checker function must return `null` or an `Error` but returned a %s. You may have forgotten to pass an argument to the type checker creator (arrayOf, instanceOf, objectOf, oneOf, oneOfType, and shape all require an argument).\\",i||\\"React class\\",a,u,typeof o),G(null)),o instanceof Error&&!(o.message in le)&&(le[o.message]=!0,G(s),m(\\"Failed %s type: %s\\",a,o.message),G(null))}}}var Le=Array.isArray;function V(e){return Le(e)}function Ke(e){{var n=typeof Symbol==\\"function\\"&&Symbol.toStringTag,a=n&&e[Symbol.toStringTag]||e.constructor.name||\\"Object\\";return a}}function Be(e){try{return ce(e),!1}catch{return!0}}function ce(e){return\\"\\"+e}function fe(e){if(Be(e))return m(\\"The provided key is an unsupported type %s. This value must be coerced to a string before before using it here.\\",Ke(e)),ce(e)}var D=N.ReactCurrentOwner,ze={key:!0,ref:!0,__self:!0,__source:!0},me,be,$;$={};function qe(e){if(P.call(e,\\"ref\\")){var n=Object.getOwnPropertyDescriptor(e,\\"ref\\").get;if(n&&n.isReactWarning)return!1}return e.ref!==void 0}function Xe(e){if(P.call(e,\\"key\\")){var n=Object.getOwnPropertyDescriptor(e,\\"key\\").get;if(n&&n.isReactWarning)return!1}return e.key!==void 0}function Je(e,n){if(typeof e.ref==\\"string\\"&&D.current&&n&&D.current.stateNode!==n){var a=_(D.current.type);$[a]||(m(\'Component \\"%s\\" contains the string ref \\"%s\\". Support for string refs will be removed in a future major release. This case cannot be automatically converted to an arrow function. We ask you to manually fix this case by using useRef() or createRef() instead. Learn more about using refs safely here: https://reactjs.org/link/strict-mode-string-ref\',_(D.current.type),e.ref),$[a]=!0)}}function Ze(e,n){{var a=function(){me||(me=!0,m(\\"%s: `key` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\\",n))};a.isReactWarning=!0,Object.defineProperty(e,\\"key\\",{get:a,configurable:!0})}}function Qe(e,n){{var a=function(){be||(be=!0,m(\\"%s: `ref` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\\",n))};a.isReactWarning=!0,Object.defineProperty(e,\\"ref\\",{get:a,configurable:!0})}}var en=function(e,n,a,i,s,l,u){var o={$$typeof:r,type:e,key:n,ref:a,props:u,_owner:l};return o._store={},Object.defineProperty(o._store,\\"validated\\",{configurable:!1,enumerable:!1,writable:!0,value:!1}),Object.defineProperty(o,\\"_self\\",{configurable:!1,enumerable:!1,writable:!1,value:i}),Object.defineProperty(o,\\"_source\\",{configurable:!1,enumerable:!1,writable:!1,value:s}),Object.freeze&&(Object.freeze(o.props),Object.freeze(o)),o};function nn(e,n,a,i,s){{var l,u={},o=null,b=null;a!==void 0&&(fe(a),o=\\"\\"+a),Xe(n)&&(fe(n.key),o=\\"\\"+n.key),qe(n)&&(b=n.ref,Je(n,s));for(l in n)P.call(n,l)&&!ze.hasOwnProperty(l)&&(u[l]=n[l]);if(e&&e.defaultProps){var c=e.defaultProps;for(l in c)u[l]===void 0&&(u[l]=c[l])}if(o||b){var f=typeof e==\\"function\\"?e.displayName||e.name||\\"Unknown\\":e;o&&Ze(u,f),b&&Qe(u,f)}return en(e,o,b,s,i,D.current,u)}}var M=N.ReactCurrentOwner,he=N.ReactDebugCurrentFrame;function j(e){if(e){var n=e._owner,a=S(e.type,e._source,n?n.type:null);he.setExtraStackFrame(a)}else he.setExtraStackFrame(null)}var W;W=!1;function L(e){return typeof e==\\"object\\"&&e!==null&&e.$$typeof===r}function pe(){{if(M.current){var e=_(M.current.type);if(e)return`\\n\\nCheck the render method of \\\\``+e+\\"`.\\"}return\\"\\"}}function rn(e){{if(e!==void 0){var n=e.fileName.replace(/^.*[\\\\\\\\\\\\/]/,\\"\\"),a=e.lineNumber;return`\\n\\nCheck your code at `+n+\\":\\"+a+\\".\\"}return\\"\\"}}var _e={};function tn(e){{var n=pe();if(!n){var a=typeof e==\\"string\\"?e:e.displayName||e.name;a&&(n=`\\n\\nCheck the top-level render call using <`+a+\\">.\\")}return n}}function ge(e,n){{if(!e._store||e._store.validated||e.key!=null)return;e._store.validated=!0;var a=tn(n);if(_e[a])return;_e[a]=!0;var i=\\"\\";e&&e._owner&&e._owner!==M.current&&(i=\\" It was passed a child from \\"+_(e._owner.type)+\\".\\"),j(e),m(\'Each child in a list should have a unique \\"key\\" prop.%s%s See https://reactjs.org/link/warning-keys for more information.\',a,i),j(null)}}function ye(e,n){{if(typeof e!=\\"object\\")return;if(V(e))for(var a=0;a<e.length;a++){var i=e[a];L(i)&&ge(i,n)}else if(L(e))e._store&&(e._store.validated=!0);else if(e){var s=Ue(e);if(typeof s==\\"function\\"&&s!==e.entries)for(var l=s.call(e),u;!(u=l.next()).done;)L(u.value)&&ge(u.value,n)}}}function an(e){{var n=e.type;if(n==null||typeof n==\\"string\\")return;var a;if(typeof n==\\"function\\")a=n.propTypes;else if(typeof n==\\"object\\"&&(n.$$typeof===E||n.$$typeof===R))a=n.propTypes;else return;if(a){var i=_(n);We(a,e.props,\\"prop\\",i,e)}else if(n.PropTypes!==void 0&&!W){W=!0;var s=_(n);m(\\"Component %s declared `PropTypes` instead of `propTypes`. Did you misspell the property assignment?\\",s||\\"Unknown\\")}typeof n.getDefaultProps==\\"function\\"&&!n.getDefaultProps.isReactClassApproved&&m(\\"getDefaultProps is only used on classic React.createClass definitions. Use a static property named `defaultProps` instead.\\")}}function on(e){{for(var n=Object.keys(e.props),a=0;a<n.length;a++){var i=n[a];if(i!==\\"children\\"&&i!==\\"key\\"){j(e),m(\\"Invalid prop `%s` supplied to `React.Fragment`. React.Fragment can only have `key` and `children` props.\\",i),j(null);break}}e.ref!==null&&(j(e),m(\\"Invalid attribute `ref` supplied to `React.Fragment`.\\"),j(null))}}function un(e,n,a,i,s,l){{var u=Fe(e);if(!u){var o=\\"\\";(e===void 0||typeof e==\\"object\\"&&e!==null&&Object.keys(e).length===0)&&(o+=\\" You likely forgot to export your component from the file it\'s defined in, or you might have mixed up default and named imports.\\");var b=rn(s);b?o+=b:o+=pe();var c;e===null?c=\\"null\\":V(e)?c=\\"array\\":e!==void 0&&e.$$typeof===r?(c=\\"<\\"+(_(e.type)||\\"Unknown\\")+\\" />\\",o=\\" Did you accidentally export a JSX literal instead of a component?\\"):c=typeof e,m(\\"React.jsx: type is invalid -- expected a string (for built-in components) or a class/function (for composite components) but got: %s.%s\\",c,o)}var f=nn(e,n,a,s,l);if(f==null)return f;if(u){var h=n.children;if(h!==void 0)if(i)if(V(h)){for(var w=0;w<h.length;w++)ye(h[w],e);Object.freeze&&Object.freeze(h)}else m(\\"React.jsx: Static children should always be an array. You are likely explicitly calling React.jsxs or React.jsxDEV. Use the Babel transform instead.\\");else ye(h,e)}return e===v?on(f):an(f),f}}var sn=un;B.Fragment=v,B.jsxDEV=sn})()});var De=K((wn,ke)=>{\\"use strict\\";ke.exports=we()});var vn={};bn(vn,{default:()=>yn,frontmatter:()=>_n});var t=hn(De()),_n={title:\\"[Paper Review] SlowFast Networks for Video Recognition\\",description:\\"The SlowFast network employs dual pathways, with the Slow Pathway capturing high-resolution spatial details and the Fast Pathway capturing rapid temporal changes, to achieve advanced video recognition.\\",image:\\"../../public/blogs/slowfast/screenshot.png\\",publishedAt:\\"2024-07-23\\",updatedAt:\\"2024-07-23\\",author:\\"junbrro\\",isPublished:!0,tags:[\\"Deep Learning\\"]};function Ee(d){let r=Object.assign({p:\\"p\\",a:\\"a\\",strong:\\"strong\\",h1:\\"h1\\",span:\\"span\\",img:\\"img\\",h2:\\"h2\\",h3:\\"h3\\",ul:\\"ul\\",li:\\"li\\"},d.components);return(0,t.jsxDEV)(t.Fragment,{children:[(0,t.jsxDEV)(r.p,{children:\\"This post is the review of slowfast paper.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:13,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.p,{children:(0,t.jsxDEV)(r.a,{href:\\"https://joochann.github.io/posts/SlowFast-Networks-for-Video-Recognition/\\",children:(0,t.jsxDEV)(r.strong,{children:\\"Cited - Joochan Github Blog\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:15,columnNumber:2},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:15,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:15,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.h1,{id:\\"slowfast-networks-for-video-recognition\\",children:[(0,t.jsxDEV)(r.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#slowfast-networks-for-video-recognition\\",children:(0,t.jsxDEV)(r.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\"},this),\\"SlowFast Networks for Video Recognition\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:17,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.p,{children:\\"Video recognition, unlike image recognition, must account for the time domain since the essence of action involves changes over time. Video data needs to capture movement in a three-dimensional space, including the temporal axis, making simple extensions of 2D image processing techniques insufficient. Therefore, a structure is necessary that can introduce existing vision processing techniques on a frame-by-frame basis while retaining past information. Just as neural networks were inspired by neural cells, the SlowFast paper draws from the brain\'s structure for action recognition.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:19,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.p,{children:(0,t.jsxDEV)(r.img,{src:\\"https://github.com/user-attachments/assets/9a060284-ba46-4e58-93b5-8b59c6f4dd2d\\",alt:\\"SlowFast Network Structure\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:21,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:21,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.h2,{id:\\"slowfast-network-architecture\\",children:[(0,t.jsxDEV)(r.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#slowfast-network-architecture\\",children:(0,t.jsxDEV)(r.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\"},this),\\"SlowFast Network Architecture\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:23,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.p,{children:\\"The SlowFast network is inspired by the human visual system, particularly the M cells and P cells. The Slow Pathway processes high-resolution spatial information, while the Fast Pathway focuses on temporal information such as motion. Mimicking this structure, the SlowFast network comprises two distinct pathways.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:25,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.h3,{id:\\"slow-pathway\\",children:[(0,t.jsxDEV)(r.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#slow-pathway\\",children:(0,t.jsxDEV)(r.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\"},this),\\"Slow Pathway\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:27,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.p,{children:\\"The Slow Pathway processes input data at a low frame rate, capturing high-resolution spatial information such as color and spatial details. This pathway is designed based on ResNet and aims to retain significant spatial information while improving computational efficiency. The pathway operates on several key principles:\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:29,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.ul,{children:[`\\n`,(0,t.jsxDEV)(r.li,{children:[(0,t.jsxDEV)(r.strong,{children:\\"Low Frame Rate Sampling\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:31,columnNumber:3},this),\\": This reduces the number of frames, focusing on detailed spatial features. By sampling fewer frames, the pathway can dedicate more computational resources to analyzing each frame in greater detail.\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:31,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.li,{children:[(0,t.jsxDEV)(r.strong,{children:\\"High Spatial Resolution\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:32,columnNumber:3},this),\\": Preserves detailed spatial information, which is crucial for understanding the scene context and object characteristics. This helps in capturing fine details that are necessary for recognizing complex objects and scenes.\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:32,columnNumber:1},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:31,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.p,{children:`If X_s represents the input to the Slow Pathway and f_s the transformation function, then:\\nY_s = f_s(X_s)\\nwhere Y_s is the output feature map with preserved high spatial resolution.`},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:34,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.h3,{id:\\"fast-pathway\\",children:[(0,t.jsxDEV)(r.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#fast-pathway\\",children:(0,t.jsxDEV)(r.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\"},this),\\"Fast Pathway\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:38,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.p,{children:\\"The Fast Pathway processes input data at a high frame rate, focusing on capturing temporal information such as motion. This pathway also uses a ResNet-based design but samples more frames densely while reducing the number of channels to save computational resources. Key aspects include:\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:40,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.ul,{children:[`\\n`,(0,t.jsxDEV)(r.li,{children:[(0,t.jsxDEV)(r.strong,{children:\\"High Frame Rate Sampling\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:42,columnNumber:3},this),\\": Captures temporal changes effectively by processing more frames. This enables the pathway to accurately detect and analyze motion, which is crucial for understanding dynamic scenes.\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:42,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.li,{children:[(0,t.jsxDEV)(r.strong,{children:\\"Channel Reduction\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:43,columnNumber:3},this),\\": Reduces the number of channels to optimize computational efficiency while retaining critical motion information. By lowering the channel count, the network can process frames quickly without compromising too much on the quality of motion detection.\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:43,columnNumber:1},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:42,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.p,{children:`If X_f represents the input to the Fast Pathway and f_f the transformation function, then:\\nY_f = f_f(X_f)\\nwhere Y_f is the output feature map emphasizing temporal information.`},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:45,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.p,{children:(0,t.jsxDEV)(r.img,{src:\\"https://github.com/user-attachments/assets/0d00197a-8c57-4adb-9d8b-1bb8d48175db\\",alt:\\"Pathway Details\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:49,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:49,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.h3,{id:\\"lateral-connections\\",children:[(0,t.jsxDEV)(r.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#lateral-connections\\",children:(0,t.jsxDEV)(r.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\"},this),\\"Lateral Connections\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:51,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.p,{children:\\"To integrate the information from both pathways, the SlowFast network employs lateral connections to exchange information between the Slow and Fast Pathways. These connections enable complementary effects, allowing each pathway to supplement what the other has not learned. The Slow Pathway provides high-resolution spatial details, while the Fast Pathway captures temporal changes, enabling comprehensive video recognition.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:53,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.p,{children:`The lateral connections can be represented as:\\nY_s\' = Y_s + g(Y_f)\\nY_f\' = Y_f + h(Y_s)\\nwhere g and h are transformation functions that map the feature maps between the pathways to enable effective information exchange.`},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:55,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.p,{children:(0,t.jsxDEV)(r.img,{src:\\"https://github.com/user-attachments/assets/837c45cd-13d1-4fde-bf83-2454b5542149\\",alt:\\"Lateral Connection Diagram\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:60,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:60,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.h2,{id:\\"experiments-and-validation\\",children:[(0,t.jsxDEV)(r.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#experiments-and-validation\\",children:(0,t.jsxDEV)(r.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\"},this),\\"Experiments and Validation\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:62,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.p,{children:\\"The SlowFast network has been evaluated on various large-scale video datasets. Key datasets include Kinetics-400, Kinetics-600, Charades, and AVA.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:64,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.h3,{id:\\"kinetics-400--600\\",children:[(0,t.jsxDEV)(r.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#kinetics-400--600\\",children:(0,t.jsxDEV)(r.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\"},this),\\"Kinetics-400 / 600\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:66,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.p,{children:\\"Kinetics-400 is a video dataset collected from YouTube, comprising 400 classes of actions. It includes 240,000 training video clips and 20,000 validation clips. Kinetics-600 is an extended version with 600 classes and more video clips.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:68,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.p,{children:(0,t.jsxDEV)(r.img,{src:\\"https://github.com/user-attachments/assets/91a84661-54c8-48fe-8fe1-967174f31620\\",alt:\\"Kinetics Dataset\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:70,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:70,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.h3,{id:\\"ava\\",children:[(0,t.jsxDEV)(r.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#ava\\",children:(0,t.jsxDEV)(r.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\"},this),\\"AVA\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:72,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.p,{children:\\"The AVA (Atomic Visual Actions) dataset is a high-resolution labeled dataset for human actions in videos. Each frame includes bounding boxes and action tags indicating the actions performed by individuals.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:74,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.p,{children:(0,t.jsxDEV)(r.img,{src:\\"https://github.com/user-attachments/assets/69c74eef-704e-45ab-9c02-ad44e9cd9199\\",alt:\\"AVA Dataset\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:76,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:76,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.h3,{id:\\"key-experimental-results\\",children:[(0,t.jsxDEV)(r.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#key-experimental-results\\",children:(0,t.jsxDEV)(r.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\"},this),\\"Key Experimental Results\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:78,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.p,{children:\\"On the Kinetics-400 and Kinetics-600 datasets, the SlowFast network outperformed existing state-of-the-art models, particularly those based on 3D convolution. It demonstrated higher accuracy and efficiency. In the Charades dataset, the network showcased its ability to recognize multiple actions simultaneously. In the AVA dataset, it excelled in action detection, proving its effectiveness in capturing both spatial and temporal information.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:80,columnNumber:1},this),`\\n`,(0,t.jsxDEV)(r.p,{children:\\"By integrating the Slow and Fast Pathways, the SlowFast network achieves a balance of spatial and temporal feature extraction, leading to superior performance in video recognition tasks.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:82,columnNumber:1},this)]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\",lineNumber:1,columnNumber:1},this)}function gn(d={}){let{wrapper:r}=d.components||{};return r?(0,t.jsxDEV)(r,Object.assign({},d,{children:(0,t.jsxDEV)(Ee,d,void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\"},this)}),void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-ed3af11a-2816-47a2-ad32-6d0ee1cea043.mdx\\"},this):Ee(d)}var yn=gn;return pn(vn);})();\\n/*! Bundled license information:\\n\\nreact/cjs/react-jsx-dev-runtime.development.js:\\n  (**\\n   * @license React\\n   * react-jsx-dev-runtime.development.js\\n   *\\n   * Copyright (c) Facebook, Inc. and its affiliates.\\n   *\\n   * This source code is licensed under the MIT license found in the\\n   * LICENSE file in the root directory of this source tree.\\n   *)\\n*/\\n;return Component;"},"_id":"slowfast/index.mdx","_raw":{"sourceFilePath":"slowfast/index.mdx","sourceFileName":"index.mdx","sourceFileDir":"slowfast","contentType":"mdx","flattenedPath":"slowfast"},"type":"Blog","url":"/blogs/slowfast","readingTime":{"text":"4 min read","minutes":3.66,"time":219600,"words":732},"toc":[{"level":"one","text":"SlowFast Networks for Video Recognition","slug":"slowfast-networks-for-video-recognition"},{"level":"two","text":"SlowFast Network Architecture","slug":"slowfast-network-architecture"},{"level":"three","text":"Slow Pathway","slug":"slow-pathway"},{"level":"three","text":"Fast Pathway","slug":"fast-pathway"},{"level":"three","text":"Lateral Connections","slug":"lateral-connections"},{"level":"two","text":"Experiments and Validation","slug":"experiments-and-validation"},{"level":"three","text":"Kinetics-400 / 600","slug":"kinetics-400--600"},{"level":"three","text":"AVA","slug":"ava"},{"level":"three","text":"Key Experimental Results","slug":"key-experimental-results"}]}');

/***/ })

});