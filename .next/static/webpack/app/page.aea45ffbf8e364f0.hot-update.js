"use strict";
/*
 * ATTENTION: An "eval-source-map" devtool has been used.
 * This devtool is neither made for production nor for readable output files.
 * It uses "eval()" calls to create a separate source file with attached SourceMaps in the browser devtools.
 * If you are trying to read the output file, select a different devtool (https://webpack.js.org/configuration/devtool/)
 * or disable the default devtool with "devtool: false".
 * If you are looking for production-ready output files, see mode: "production" (https://webpack.js.org/configuration/mode/).
 */
self["webpackHotUpdate_N_E"]("app/page",{

/***/ "(app-pages-browser)/./.contentlayer/generated/Blog/mert__index.mdx.json":
/*!***********************************************************!*\
  !*** ./.contentlayer/generated/Blog/mert__index.mdx.json ***!
  \***********************************************************/
/***/ (function(module, __unused_webpack_exports, __webpack_require__) {

module.exports = JSON.parse('{"title":"[Paper Review] MERT - ACOUSTIC MUSIC UNDERSTANDING MODEL WITH LARGE-SCALE SELF-SUPERVISED TRAINING","publishedAt":"2024-07-12T00:00:00.000Z","updatedAt":"2024-07-12T00:00:00.000Z","description":"MERT is an advanced model for understanding acoustic music, trained on a large scale using self-supervised learning techniques to achieve high performance in music-related tasks.","image":{"filePath":"../public/blogs/mert/screenshot.png","relativeFilePath":"../../public/blogs/mert/screenshot.png","format":"png","height":972,"width":1922,"aspectRatio":1.977366255144033,"blurhashDataUrl":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAJ1BMVEX9/f3k5OT39/fv6unb2tzx7+7n6OjV2dve39/MztS6wMvu29nn5NjLihnMAAAACXBIWXMAABYlAAAWJQFJUiTwAAAANklEQVR4nBXBiRHAIAwEsT3jH/qvNxOJUrjLoF94KiCetWFBunfmFF1eConZ3eGI88OMq9QFPiobAR8yFjGWAAAAAElFTkSuQmCC"},"isPublished":true,"author":"junbrro","tags":["Deep Learning"],"body":{"raw":"\\n## Base: Masked Language Modeling\\n\\n### **How is MLM different from Word2Vec?**\\n\\nSimilar to masked language modeling and CLM, Word2Vec is an approach used in NLP where the vectors capture the semantics of the words and the relationships between them by using a neural network to learn the vector representations.\\n\\nHowever, Word2Vec differs from self-supervised training models such as masked language modeling in the following ways:\\n\\n- Word2Vec is an unsupervised learning algorithm that\'s used to generate word embeddings.\\n- It captures the syntactic and semantic links between words by representing them as dense vectors in a continuous vector space.\\n- Word2Vec acquires word embeddings by training on large corpora and predicting the context of words within a designated text window, encompassing either the target word itself or the surrounding words.\\n- It can be trained using two different algorithms -- [Continuous Bag of Words](https://medium.com/@codethulo/understanding-the-continuous-bag-of-words-cbow-model-architecture-working-mechanism-and-math-78c7284a8d5a) and Skip-Gram.\\n- Word2Vec embeddings are often used to measure word similarity or as input features for downstream natural language processing tasks.\\n\\n[What are Masked Language Models (MLMs)? | Definition from TechTarget](https://www.techtarget.com/searchenterpriseai/definition/masked-language-models-MLMs)\\n\\n# Introduction\\n\\n## Self-supervised Learning\\n\\n_Pre-trained language models (PLMs) can learn generalisable representations of data without human annotated labels in a self-supervised learning (SSL) style, leading to remarkable performance improvement in natural language processing and related fields (Brown et al., 2020; Fang et al., 2022; Chen et al., 2021a). →_ PLM (Pre-trained Language Model) 기반 유사성 판단은 음악 시퀀스 적용에 유망하다.\\n\\n_First, PLMs can potentially pave the way to unify the modelling of a wide range of music understanding, or the so-called Music Information Retrieval (MIR) tasks, including but not limited to music tagging, beat tracking, music transcription, and source separation, so that different tasks no longer need task-specific models or features._\\n\\n\\\\*Unfortunately, we are yet to see a general-purpose and cost-effective open-source PLM on acoustic music understanding. Most existing studies are designed to solely address music tagging problems (Pons and Serra, 2019; Spijkervet and Burgoyne, 2021; McCallum et al., 2022; Huang et al., 2022; Zhu et al., 2021; Zhao and Guo, 2021), and **many of them do not provide open-source code bases or checkpoints for further evaluation.\\\\***\\n\\n![](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/4ebb6c73-adac-48fd-9eaa-a34d824fd1c3)\\n\\n_MERT inherits a speech SSL paradigm, employing teacher models to generate pseudo targets for sequential audio clips. Specifically, to capture the distinctive pitched and tonal characteristics in music, MERT incorporates a multi-task paradigm to balance the acoustic and musical representation learning as demonstrated in Fig. 1._\\n\\n## RVQ-VAE (Residual Vector Quantization Variational AutoEncoder)\\n\\n[What is Residual Vector Quantization?](https://www.assemblyai.com/blog/what-is-residual-vector-quantization/)\\n\\n[[논문리뷰] Autoregressive Image Generation using Residual Quantization (RQ-VAE-Transformer)](https://kimjy99.github.io/논문리뷰/rq/)\\n\\n## CQT (Constant-Q Transformation)\\n\\n- _CQT is a type of frequency transform that is widely used in various MIR tasks, such as pitch detection, chord recognition, and music transcriptions_\\n\\n[Constant-Q transform](https://en.wikipedia.org/wiki/Constant-Q_transform)\\n\\n[[DL] 딥러닝 음성 이해 - Introduction to sound data analysis](https://heeya-stupidbutstudying.tistory.com/entry/DL-딥러닝-음성-이해-Introduction-to-sound-data-analysis)\\n\\n_To summarise, our contributions are:_\\n\\n- _proposing a multi-task style predictive acoustic self-supervised learning paradigm, which achieves SOTA performance on various MIR tasks, including important yet unexplored tasks for pre-training such as pitch detection, beat tracking and source separation applications;_\\n- _conducting a broad range of analysis based on ablation studies of the proposed MERT pretraining paradigm;_\\n- _exploring robust and stable strategies for acoustic music model training to overcome training instability and frequent crashes when scaling up the pre-training on model size;_\\n- _providing an open-source, generalisable and computationally affordable acoustic music pretrained model, which addresses the needs of both industry and research communities._\\n\\n---\\n\\n# Related Work\\n\\n## PLMs for Acoustic Music\\n\\n_Existing acoustic music pre-trained models primarily focus on tagging tasks and rely on supervised tagging labels for pre-training (Pons and Serra, 2019; Spijkervet and Burgoyne, 2021; McCallum et al., 2022; Huang et al., 2022)._\\n\\n_they face limitations in training data and model size, hampering the performance improvements (Choi et al., 2017; Li et al., 2022). Additionally, several models trained on inaccessible datasets or without publicly available codes and model weights make it difficult to reproduce or extend their approaches (McCallum et al., 2022; Castellon et al., 2021; Li et al., 2022; Zhu et al., 2021; Zhao and Guo, 2021)._\\n\\n## Self-Supervised Speech Processing\\n\\n_both acoustic music and speech processing models need to deal with the cocktail party problem (Brown and Bidelman, 2022; Petermann et al., 2022) since good source separation capabilities help both separating noises and background sounds with speech and processing polyphonic music audio._\\n\\n### Cocktail Party Problem\\n\\n[NCBI - WWW Error Blocked Diagnostic](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2692487/)\\n\\n코텔 파티 문제(Cocktail Party Problem)는 소리 신호 처리 및 인공지능 분야에서 중요한 문제로, 사람들이 여러 대화가 겹치는 소음 속에서 원하는 대화를 식별하는 상황을 다루고 있다. 이 문제는 실제 코텔 파티에서 많은 사람들이 동시에 대화하는 상황을 상상해 보면 이해하기 쉽다. 이 상황에서 우리는 한 명의 목소리에 집중하여 다른 모든 소음을 배제하고자 한다.\\n\\n컴퓨터 과학 및 신호 처리에서는 이러한 문제를 \\"블라인드 소스 분리(Blind Source Separation)\\"라고도 부르며, 한 가지 음성 신호를 여러 소스에서 분리하고 원하는 신호만을 추출하는 작업을 의미한다. 이 문제는 인간의 청각 능력에 비해 기계로 해결하는 데 있어 상당한 어려움이 있다.\\n\\n해결 방법으로는 독립 성분 분석(Independent Component Analysis, ICA), 파동 변환, 딥러닝 기반 모델 등 다양한 기술이 활용된다. ICA는 서로 독립적인 신호 성분을 분리하는 방법이고, 딥러닝 기반 모델은 대규모 데이터셋을 학습시켜 패턴 인식 및 음성 분리에 사용된다.\\n\\n## Audio Representation with Language Modelling\\n\\n\\\\*Mask strategy-based large-scale language models have been applied to a wide range of domains (Lample and Charton, 2019; Chen et al., 2021a;b; Fang et al., 2022), but **still remain under-explored in acoustic music understanding.\\\\***\\n\\n\\\\*Baevski and Mohamed (2020) introduce a pre-trained VQ-VAE (Baevski et al., 2019) to provide prediction targets to conduct speech representation learning with MLM. While introducing K-means to provide discrete token codebooks and pre-training the model to detect sound units, **Hsu et al. (2021) claim that a better teacher model in SSL could lead to better downstream task performance.\\\\***\\n\\n\\\\*the recently released **RVQ-VAEs (Zeghidour et al., 2021; D´ efossez et al., 2022), achieving good results in music reconstruction, could be adopted as teacher models for music understanding pre-training and provide acoustic information guidance.\\\\***\\n\\n---\\n\\n# Methodology\\n\\n## Pre-Training with MLM\\n\\n![Untitled 1](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/2d05f4c4-573b-43ed-8772-003218055edd)\\n\\n![Untitled 2](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/9d889361-88d5-4d84-9180-4e841fd66e0d)\\n\\n### HuBERT\\n\\n![Untitled 3](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/3d347f1a-85d9-49b7-aa4d-3989e879d849)\\n\\n[[논문리뷰 | Speech] HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units (2021) Summary](https://velog.io/@9e0na/논문리뷰-Speech-HuBERT-Self-Supervised-Speech-RepresentationLearning-by-Masked-Prediction-of-Hidden-Units-2021-Summary)\\n\\n## Modelling Acoustic Information\\n\\nMel-Frequency Cepstral Coefficients (MFCCs) are **only capable at** modeling acoustic and single-pitch signals\\n\\n[MFCC(Mel-Frequency Cepstral Coefficient) 이해하기](https://brightwon.tistory.com/11)\\n\\n### Method 1. Using K-means on the log-Mel spectrum & Chroma Features\\n\\n- In case of music, each frame contain more informations than that of speech. → Larger number of classes is needed!\\n- _The complexity of the k-means algorithm is linear with the number of centroids, leading to a time-consuming k-means for the music feature._ → 300-means for log-Mel spectrum with dimension 229, and 200-means for Chroma features with dimension 264 → **Computational complexity remains comparable to that of HuBERT**\\n- Disadvantage\\n  - difficult to scale up to a larger number of classes and larger datasets\\n  - results are sensitive to initialization\\n\\n### Method 2. EnCodec (8-layer residual VQ-VAE)\\n\\n- Each acoustic features are denoted as 2-dimensional auditory code matrix with L (length of the recording)\\n- _Converts 24kHz input waveforms to 8 different embeddings at 75Hz with a 320-fold reduction, and the quantizer has 1024 dimensions → Decoder of Encodec can reconstruct the waveform at 24kHz with authentic information in timbre._\\n\\n[High Fidelity Neural Audio Compression (EnCodec)](https://ostin.tistory.com/206)\\n\\n[[논문 리뷰] VQ-VAE: Neural Discrete Representation Learning](https://velog.io/@dien-eaststar/논문-리뷰-SSL-Neural-Discrete-Representation-Learning)\\n\\n## Modeling Musical Information\\n\\n- CQT Spectrogram is used solely for pitch-level information, regardless of acoustic information. (Similar to Fourier Transformation)\\n- Bin widths are proportional to frequency → **giving each octave the same number of bins**\\n\\n![Uploading Untitled 4.png…]()\\n\\n---\\n\\n- **다운스트림 작업:** 14개의 다운스트림 작업을 대상으로 평가하며, 음악 태깅, 키 탐지, 장르 분류, 감정 점수 회귀, 악기 분류, 피치 분류, 보컬 기술 탐지, 가수 식별과 같은 프레임 수준의 작업과 박자 추적, 소스 분리와 같은 순차 작업을 포함한다.\\n- **프로빙 프로토콜:** 백본 모델을 고정한 채 간단한 다운스트림 구조만 훈련하여 평가하며, 하이퍼파라미터 검색 공간도 제한한다.\\n- Baseline Model:\\n  - **음악:** MusiCNN, CLMR, MULE, Jukebox 및 JukeMIR\\n  - **스피치:** HuBERT, data2vec\\n\\n---\\n\\n# Appendix\\n\\n## DownStream Tasks\\n\\nWe evaluate the models on 14 downstream tasks to provide a comprehensive view of our method and the comparison between baselines. The full descriptions of the datasets and tasks are given as follows.\\n\\n- Music Tagging involves determining which of a set of fixed tags apply to a particular song. Tag categories may include genre, instrumentation, mood, tempo (e.g. fast) or other tags. We used two large datasets: MagnaTagATune (MTT) (Law et al., 2009) and MTG-Jamendo (Bogdanov et al., 2019). For both datasets, we limit the tag vocabulary according to official instructions. We use all clips in MTTandMTG-Jamendo. Since many of the audio recordings among 5.5k MTG-Jamendo excerpts are longer than the 30s, we averaged the multiple embeddings computed with a sliding window as the overall embedding. The window length is set to the same default length as in every system. For MERT series, the window length is typically set to 30s. The metrics are the macro-average of ROC-AUCs and the average precision (AP) / PR-AUC among all top-50 tags.\\n- Key detection predicts the tonal scale and dominant pitch level of a song. We use Giantsteps (Knees et al., 2015) as test set and a commonly-used subset of Giantsteps-MTG-keys dataset (Korzeniowski and Widmer, 2017) as the training and validation set. The splitting is the same as in (Castellon et al., 2021). The metric is a refined accuracy with error tolerance, giving partial credit to reasonable errors (Raffel et al., 2014).\\n- Genre classification estimates the most appropriate genre for each given song. We report the accuracy of the GTZAN (Tzanetakis and Cook, 2002) dataset along with ROC and AP on MTG-Genre, since the former task is a multi-class classification and the latter is multi-label. We used the standard ”fail-filtered” split (Kereliuk et al., 2015) for GTZAN.\\n- Emotion score regression. The Emomusic dataset (Soleymani et al., 2013) contains 744 music clips of 45 seconds in length, each reported on a two-dimensional valence-arousal plane after listening, where valence indicates positive and negative emotional responses, and arousal indicates emotional intensity. We use the same dataset split as (Castellon et al., 2021). The official evaluation metric is the determination coefficient (r2) between the model regression results and human annotations of arousal (EmoA) and valence (EmoV) (Soleymani et al., 2013). For inference, we split the 45-second clip into a 5-second sliding window and averaged the prediction.\\n- Instrument classification is the process of identifying which instruments are included in a given sound. We use the Nsynth (Engel et al., 2017) and MTG-instrument datasets. The former is a monophonic note-level multi-class task with 306k audio samples in 11 instrument classes with accuracy as an indicator. The latter is a subset of MTG-Jamendo, containing 25k polyphonic audio tracks and 41 instrument tags; each track can contain multiple instruments and is evaluated on ROC and AP.\\n- Pitch classification estimates which of the 128 pitch categories the given audio segment belongs to. Weuse the NSynth dataset for this task. Given these segments are short monophonic audio, this task is multi-class, and the accuracy is used as an evaluation metric.\\n- Vocal technique detection involves identifying what singing techniques are contained in a given audio clip. We use the VocalSet dataset (Wilkins et al., 2018), which is the only publicly available dataset for the study of singing techniques. The dataset contains the vocals of 20 different professional singers (9 female and 11 male) who perform 17 different singing techniques in various contexts for a total of 10.1 hours. As the audio clips are divided into 3 seconds, the task only requires a judgement on the type of technique and not on the start and end of the technique. We used the same 10 different singing techniques as in Yamamoto et al. (2022) as a subset and used the same 15 singers as the training and validation sets and 5 singers as the test set. Since there is no accepted division between training and validation sets, we selected 9 singers as the training set and 6 singers as the validation set. All the 3-second segments that originate from the same recording are allocated to the same part of the split (e.g. all are in the training set). The evaluation metric is accuracy. 16Published as a conference paper at ICLR 2024\\n- Singer identification identifies the vocal performer from a given recording. We use the VocalSet dataset for this task. We randomly divided the dataset into a training set, validation set and testing set based on a ratio of 12:8:5, all containing the same 20 singers.\\n- Beat tracking is the process of determining whether there is a beat in each frame of a given piece of music. We use an offline approach to the binary classification, i.e. the model can use information following each frame to help with inference. The model needs to output frame-by-frame predictions at a certain frequency and post-process them using a dynamic Bayesian network (DBN) (B¨ ock et al., 2016b) to obtain the final result. The DBN is implemented using madmom (B¨ ock et al., 2016a). The dataset we use is GTZAN Rhythm (Marchand and Peeters, 2015). We also label the two adjacent frames of each label as beat, which is a common way of label smoothing in beat tracking to improve the performance of the model and to compare the SSL model fairly with the spin model. The model is evaluated using the f measure implemented in mir eval (Raffel et al., 2014), and the prediction is considered correct if the difference between the predicted event and the ground truth does not exceed 20ms. In this task, some models were trained on other datasets, and the full GTZAN set was used as the test set.\\n- Source separation. Source separation aims to demix the music recording into its constituent parts, e.g., vocals, drums, bass, and others. We adopt MUSDB18 (Rafii et al., 2017), a widely used benchmark dataset in music source separation. MUSDB18 contains 150 full-length music tracks (˜ 10 hours), along with multiple isolated stems. We use 86 tracks for training, 14 tracks for validation, and 50 tracks for evaluation following the official setting in MUSDB18. During training, we randomly sample 6-second segments and apply random track mixing for augmentation. Due to the difficulty of this task, we adopt the baseline architecture in the Music Demixing Challenge (MDX) 2021 (Mitsufuji et al., 2022), which consists of three linear layers and three bi-directional LSTM layers. We directly compute the l2-loss between predicted and ground-truth spectrograms for optimisation. The metric for this task is the Source-to-Distortion Ratio (SDR) defined by MDX 2021 (Mitsufuji et al., 2022), which is the mean across the SDR scores of all songs.\\n","code":"var Component=(()=>{var sn=Object.create;var S=Object.defineProperty;var ln=Object.getOwnPropertyDescriptor;var cn=Object.getOwnPropertyNames;var mn=Object.getPrototypeOf,fn=Object.prototype.hasOwnProperty;var Y=(l,n)=>()=>(n||l((n={exports:{}}).exports,n),n.exports),bn=(l,n)=>{for(var p in n)S(l,p,{get:n[p],enumerable:!0})},xe=(l,n,p,N)=>{if(n&&typeof n==\\"object\\"||typeof n==\\"function\\")for(let y of cn(n))!fn.call(l,y)&&y!==p&&S(l,y,{get:()=>n[y],enumerable:!(N=ln(n,y))||N.enumerable});return l};var hn=(l,n,p)=>(p=l!=null?sn(mn(l)):{},xe(n||!l||!l.__esModule?S(p,\\"default\\",{value:l,enumerable:!0}):p,l)),pn=l=>xe(S({},\\"__esModule\\",{value:!0}),l);var ke=Y((jn,je)=>{je.exports=React});var ve=Y($=>{\\"use strict\\";(function(){\\"use strict\\";var l=ke(),n=Symbol.for(\\"react.element\\"),p=Symbol.for(\\"react.portal\\"),N=Symbol.for(\\"react.fragment\\"),y=Symbol.for(\\"react.strict_mode\\"),z=Symbol.for(\\"react.profiler\\"),Q=Symbol.for(\\"react.provider\\"),K=Symbol.for(\\"react.context\\"),G=Symbol.for(\\"react.forward_ref\\"),M=Symbol.for(\\"react.suspense\\"),P=Symbol.for(\\"react.suspense_list\\"),H=Symbol.for(\\"react.memo\\"),A=Symbol.for(\\"react.lazy\\"),He=Symbol.for(\\"react.offscreen\\"),Z=Symbol.iterator,we=\\"@@iterator\\";function Ee(e){if(e===null||typeof e!=\\"object\\")return null;var t=Z&&e[Z]||e[we];return typeof t==\\"function\\"?t:null}var j=l.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED;function f(e){{for(var t=arguments.length,i=new Array(t>1?t-1:0),a=1;a<t;a++)i[a-1]=arguments[a];Te(\\"error\\",e,i)}}function Te(e,t,i){{var a=j.ReactDebugCurrentFrame,u=a.getStackAddendum();u!==\\"\\"&&(t+=\\"%s\\",i=i.concat([u]));var s=i.map(function(d){return String(d)});s.unshift(\\"Warning: \\"+t),Function.prototype.apply.call(console[e],console,s)}}var Re=!1,Ce=!1,Se=!1,Me=!1,Pe=!1,J;J=Symbol.for(\\"react.module.reference\\");function Ae(e){return!!(typeof e==\\"string\\"||typeof e==\\"function\\"||e===N||e===z||Pe||e===y||e===M||e===P||Me||e===He||Re||Ce||Se||typeof e==\\"object\\"&&e!==null&&(e.$$typeof===A||e.$$typeof===H||e.$$typeof===Q||e.$$typeof===K||e.$$typeof===G||e.$$typeof===J||e.getModuleId!==void 0))}function Ie(e,t,i){var a=e.displayName;if(a)return a;var u=t.displayName||t.name||\\"\\";return u!==\\"\\"?i+\\"(\\"+u+\\")\\":i}function X(e){return e.displayName||\\"Context\\"}function g(e){if(e==null)return null;if(typeof e.tag==\\"number\\"&&f(\\"Received an unexpected object in getComponentNameFromType(). This is likely a bug in React. Please file an issue.\\"),typeof e==\\"function\\")return e.displayName||e.name||null;if(typeof e==\\"string\\")return e;switch(e){case N:return\\"Fragment\\";case p:return\\"Portal\\";case z:return\\"Profiler\\";case y:return\\"StrictMode\\";case M:return\\"Suspense\\";case P:return\\"SuspenseList\\"}if(typeof e==\\"object\\")switch(e.$$typeof){case K:var t=e;return X(t)+\\".Consumer\\";case Q:var i=e;return X(i._context)+\\".Provider\\";case G:return Ie(e,e.render,\\"ForwardRef\\");case H:var a=e.displayName||null;return a!==null?a:g(e.type)||\\"Memo\\";case A:{var u=e,s=u._payload,d=u._init;try{return g(d(s))}catch{return null}}}return null}var x=Object.assign,D=0,ee,ne,re,te,ie,ae,oe;function de(){}de.__reactDisabledLog=!0;function Be(){{if(D===0){ee=console.log,ne=console.info,re=console.warn,te=console.error,ie=console.group,ae=console.groupCollapsed,oe=console.groupEnd;var e={configurable:!0,enumerable:!0,value:de,writable:!0};Object.defineProperties(console,{info:e,log:e,warn:e,error:e,group:e,groupCollapsed:e,groupEnd:e})}D++}}function Oe(){{if(D--,D===0){var e={configurable:!0,enumerable:!0,writable:!0};Object.defineProperties(console,{log:x({},e,{value:ee}),info:x({},e,{value:ne}),warn:x({},e,{value:re}),error:x({},e,{value:te}),group:x({},e,{value:ie}),groupCollapsed:x({},e,{value:ae}),groupEnd:x({},e,{value:oe})})}D<0&&f(\\"disabledDepth fell below zero. This is a bug in React. Please file an issue.\\")}}var I=j.ReactCurrentDispatcher,B;function w(e,t,i){{if(B===void 0)try{throw Error()}catch(u){var a=u.stack.trim().match(/\\\\n( *(at )?)/);B=a&&a[1]||\\"\\"}return`\\n`+B+e}}var O=!1,E;{var Le=typeof WeakMap==\\"function\\"?WeakMap:Map;E=new Le}function ue(e,t){if(!e||O)return\\"\\";{var i=E.get(e);if(i!==void 0)return i}var a;O=!0;var u=Error.prepareStackTrace;Error.prepareStackTrace=void 0;var s;s=I.current,I.current=null,Be();try{if(t){var d=function(){throw Error()};if(Object.defineProperty(d.prototype,\\"props\\",{set:function(){throw Error()}}),typeof Reflect==\\"object\\"&&Reflect.construct){try{Reflect.construct(d,[])}catch(_){a=_}Reflect.construct(e,[],d)}else{try{d.call()}catch(_){a=_}e.call(d.prototype)}}else{try{throw Error()}catch(_){a=_}e()}}catch(_){if(_&&a&&typeof _.stack==\\"string\\"){for(var o=_.stack.split(`\\n`),b=a.stack.split(`\\n`),c=o.length-1,m=b.length-1;c>=1&&m>=0&&o[c]!==b[m];)m--;for(;c>=1&&m>=0;c--,m--)if(o[c]!==b[m]){if(c!==1||m!==1)do if(c--,m--,m<0||o[c]!==b[m]){var h=`\\n`+o[c].replace(\\" at new \\",\\" at \\");return e.displayName&&h.includes(\\"<anonymous>\\")&&(h=h.replace(\\"<anonymous>\\",e.displayName)),typeof e==\\"function\\"&&E.set(e,h),h}while(c>=1&&m>=0);break}}}finally{O=!1,I.current=s,Oe(),Error.prepareStackTrace=u}var v=e?e.displayName||e.name:\\"\\",Ne=v?w(v):\\"\\";return typeof e==\\"function\\"&&E.set(e,Ne),Ne}function We(e,t,i){return ue(e,!1)}function Fe(e){var t=e.prototype;return!!(t&&t.isReactComponent)}function T(e,t,i){if(e==null)return\\"\\";if(typeof e==\\"function\\")return ue(e,Fe(e));if(typeof e==\\"string\\")return w(e);switch(e){case M:return w(\\"Suspense\\");case P:return w(\\"SuspenseList\\")}if(typeof e==\\"object\\")switch(e.$$typeof){case G:return We(e.render);case H:return T(e.type,t,i);case A:{var a=e,u=a._payload,s=a._init;try{return T(s(u),t,i)}catch{}}}return\\"\\"}var R=Object.prototype.hasOwnProperty,se={},le=j.ReactDebugCurrentFrame;function C(e){if(e){var t=e._owner,i=T(e.type,e._source,t?t.type:null);le.setExtraStackFrame(i)}else le.setExtraStackFrame(null)}function Ve(e,t,i,a,u){{var s=Function.call.bind(R);for(var d in e)if(s(e,d)){var o=void 0;try{if(typeof e[d]!=\\"function\\"){var b=Error((a||\\"React class\\")+\\": \\"+i+\\" type `\\"+d+\\"` is invalid; it must be a function, usually from the `prop-types` package, but received `\\"+typeof e[d]+\\"`.This often happens because of typos such as `PropTypes.function` instead of `PropTypes.func`.\\");throw b.name=\\"Invariant Violation\\",b}o=e[d](t,d,a,i,null,\\"SECRET_DO_NOT_PASS_THIS_OR_YOU_WILL_BE_FIRED\\")}catch(c){o=c}o&&!(o instanceof Error)&&(C(u),f(\\"%s: type specification of %s `%s` is invalid; the type checker function must return `null` or an `Error` but returned a %s. You may have forgotten to pass an argument to the type checker creator (arrayOf, instanceOf, objectOf, oneOf, oneOfType, and shape all require an argument).\\",a||\\"React class\\",i,d,typeof o),C(null)),o instanceof Error&&!(o.message in se)&&(se[o.message]=!0,C(u),f(\\"Failed %s type: %s\\",i,o.message),C(null))}}}var qe=Array.isArray;function L(e){return qe(e)}function Ye(e){{var t=typeof Symbol==\\"function\\"&&Symbol.toStringTag,i=t&&e[Symbol.toStringTag]||e.constructor.name||\\"Object\\";return i}}function $e(e){try{return ce(e),!1}catch{return!0}}function ce(e){return\\"\\"+e}function me(e){if($e(e))return f(\\"The provided key is an unsupported type %s. This value must be coerced to a string before before using it here.\\",Ye(e)),ce(e)}var U=j.ReactCurrentOwner,ze={key:!0,ref:!0,__self:!0,__source:!0},fe,be,W;W={};function Qe(e){if(R.call(e,\\"ref\\")){var t=Object.getOwnPropertyDescriptor(e,\\"ref\\").get;if(t&&t.isReactWarning)return!1}return e.ref!==void 0}function Ke(e){if(R.call(e,\\"key\\")){var t=Object.getOwnPropertyDescriptor(e,\\"key\\").get;if(t&&t.isReactWarning)return!1}return e.key!==void 0}function Ze(e,t){if(typeof e.ref==\\"string\\"&&U.current&&t&&U.current.stateNode!==t){var i=g(U.current.type);W[i]||(f(\'Component \\"%s\\" contains the string ref \\"%s\\". Support for string refs will be removed in a future major release. This case cannot be automatically converted to an arrow function. We ask you to manually fix this case by using useRef() or createRef() instead. Learn more about using refs safely here: https://reactjs.org/link/strict-mode-string-ref\',g(U.current.type),e.ref),W[i]=!0)}}function Je(e,t){{var i=function(){fe||(fe=!0,f(\\"%s: `key` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\\",t))};i.isReactWarning=!0,Object.defineProperty(e,\\"key\\",{get:i,configurable:!0})}}function Xe(e,t){{var i=function(){be||(be=!0,f(\\"%s: `ref` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\\",t))};i.isReactWarning=!0,Object.defineProperty(e,\\"ref\\",{get:i,configurable:!0})}}var en=function(e,t,i,a,u,s,d){var o={$$typeof:n,type:e,key:t,ref:i,props:d,_owner:s};return o._store={},Object.defineProperty(o._store,\\"validated\\",{configurable:!1,enumerable:!1,writable:!0,value:!1}),Object.defineProperty(o,\\"_self\\",{configurable:!1,enumerable:!1,writable:!1,value:a}),Object.defineProperty(o,\\"_source\\",{configurable:!1,enumerable:!1,writable:!1,value:u}),Object.freeze&&(Object.freeze(o.props),Object.freeze(o)),o};function nn(e,t,i,a,u){{var s,d={},o=null,b=null;i!==void 0&&(me(i),o=\\"\\"+i),Ke(t)&&(me(t.key),o=\\"\\"+t.key),Qe(t)&&(b=t.ref,Ze(t,u));for(s in t)R.call(t,s)&&!ze.hasOwnProperty(s)&&(d[s]=t[s]);if(e&&e.defaultProps){var c=e.defaultProps;for(s in c)d[s]===void 0&&(d[s]=c[s])}if(o||b){var m=typeof e==\\"function\\"?e.displayName||e.name||\\"Unknown\\":e;o&&Je(d,m),b&&Xe(d,m)}return en(e,o,b,u,a,U.current,d)}}var F=j.ReactCurrentOwner,he=j.ReactDebugCurrentFrame;function k(e){if(e){var t=e._owner,i=T(e.type,e._source,t?t.type:null);he.setExtraStackFrame(i)}else he.setExtraStackFrame(null)}var V;V=!1;function q(e){return typeof e==\\"object\\"&&e!==null&&e.$$typeof===n}function pe(){{if(F.current){var e=g(F.current.type);if(e)return`\\n\\nCheck the render method of \\\\``+e+\\"`.\\"}return\\"\\"}}function rn(e){{if(e!==void 0){var t=e.fileName.replace(/^.*[\\\\\\\\\\\\/]/,\\"\\"),i=e.lineNumber;return`\\n\\nCheck your code at `+t+\\":\\"+i+\\".\\"}return\\"\\"}}var ge={};function tn(e){{var t=pe();if(!t){var i=typeof e==\\"string\\"?e:e.displayName||e.name;i&&(t=`\\n\\nCheck the top-level render call using <`+i+\\">.\\")}return t}}function _e(e,t){{if(!e._store||e._store.validated||e.key!=null)return;e._store.validated=!0;var i=tn(t);if(ge[i])return;ge[i]=!0;var a=\\"\\";e&&e._owner&&e._owner!==F.current&&(a=\\" It was passed a child from \\"+g(e._owner.type)+\\".\\"),k(e),f(\'Each child in a list should have a unique \\"key\\" prop.%s%s See https://reactjs.org/link/warning-keys for more information.\',i,a),k(null)}}function ye(e,t){{if(typeof e!=\\"object\\")return;if(L(e))for(var i=0;i<e.length;i++){var a=e[i];q(a)&&_e(a,t)}else if(q(e))e._store&&(e._store.validated=!0);else if(e){var u=Ee(e);if(typeof u==\\"function\\"&&u!==e.entries)for(var s=u.call(e),d;!(d=s.next()).done;)q(d.value)&&_e(d.value,t)}}}function an(e){{var t=e.type;if(t==null||typeof t==\\"string\\")return;var i;if(typeof t==\\"function\\")i=t.propTypes;else if(typeof t==\\"object\\"&&(t.$$typeof===G||t.$$typeof===H))i=t.propTypes;else return;if(i){var a=g(t);Ve(i,e.props,\\"prop\\",a,e)}else if(t.PropTypes!==void 0&&!V){V=!0;var u=g(t);f(\\"Component %s declared `PropTypes` instead of `propTypes`. Did you misspell the property assignment?\\",u||\\"Unknown\\")}typeof t.getDefaultProps==\\"function\\"&&!t.getDefaultProps.isReactClassApproved&&f(\\"getDefaultProps is only used on classic React.createClass definitions. Use a static property named `defaultProps` instead.\\")}}function on(e){{for(var t=Object.keys(e.props),i=0;i<t.length;i++){var a=t[i];if(a!==\\"children\\"&&a!==\\"key\\"){k(e),f(\\"Invalid prop `%s` supplied to `React.Fragment`. React.Fragment can only have `key` and `children` props.\\",a),k(null);break}}e.ref!==null&&(k(e),f(\\"Invalid attribute `ref` supplied to `React.Fragment`.\\"),k(null))}}function dn(e,t,i,a,u,s){{var d=Ae(e);if(!d){var o=\\"\\";(e===void 0||typeof e==\\"object\\"&&e!==null&&Object.keys(e).length===0)&&(o+=\\" You likely forgot to export your component from the file it\'s defined in, or you might have mixed up default and named imports.\\");var b=rn(u);b?o+=b:o+=pe();var c;e===null?c=\\"null\\":L(e)?c=\\"array\\":e!==void 0&&e.$$typeof===n?(c=\\"<\\"+(g(e.type)||\\"Unknown\\")+\\" />\\",o=\\" Did you accidentally export a JSX literal instead of a component?\\"):c=typeof e,f(\\"React.jsx: type is invalid -- expected a string (for built-in components) or a class/function (for composite components) but got: %s.%s\\",c,o)}var m=nn(e,t,i,u,s);if(m==null)return m;if(d){var h=t.children;if(h!==void 0)if(a)if(L(h)){for(var v=0;v<h.length;v++)ye(h[v],e);Object.freeze&&Object.freeze(h)}else f(\\"React.jsx: Static children should always be an array. You are likely explicitly calling React.jsxs or React.jsxDEV. Use the Babel transform instead.\\");else ye(h,e)}return e===N?on(m):an(m),m}}var un=dn;$.Fragment=N,$.jsxDEV=un})()});var Ue=Y((vn,De)=>{\\"use strict\\";De.exports=ve()});var Nn={};bn(Nn,{default:()=>yn,frontmatter:()=>gn});var r=hn(Ue()),gn={title:\\"[Paper Review] MERT - ACOUSTIC MUSIC UNDERSTANDING MODEL WITH LARGE-SCALE SELF-SUPERVISED TRAINING\\",description:\\"MERT is an advanced model for understanding acoustic music, trained on a large scale using self-supervised learning techniques to achieve high performance in music-related tasks.\\",image:\\"../../public/blogs/mert/screenshot.png\\",publishedAt:\\"2024-07-12\\",updatedAt:\\"2024-07-12\\",author:\\"junbrro\\",isPublished:!0,tags:[\\"Deep Learning\\"]};function Ge(l){let n=Object.assign({h2:\\"h2\\",a:\\"a\\",span:\\"span\\",h3:\\"h3\\",strong:\\"strong\\",p:\\"p\\",ul:\\"ul\\",li:\\"li\\",h1:\\"h1\\",em:\\"em\\",img:\\"img\\",hr:\\"hr\\"},l.components);return(0,r.jsxDEV)(r.Fragment,{children:[(0,r.jsxDEV)(n.h2,{id:\\"base-masked-language-modeling\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#base-masked-language-modeling\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this),\\"Base: Masked Language Modeling\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:13,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h3,{id:\\"how-is-mlm-different-from-word2vec\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#how-is-mlm-different-from-word2vec\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this),(0,r.jsxDEV)(n.strong,{children:\\"How is MLM different from Word2Vec?\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:15,columnNumber:5},this)]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:15,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:\\"Similar to masked language modeling and CLM, Word2Vec is an approach used in NLP where the vectors capture the semantics of the words and the relationships between them by using a neural network to learn the vector representations.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:17,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:\\"However, Word2Vec differs from self-supervised training models such as masked language modeling in the following ways:\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:19,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.ul,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:\\"Word2Vec is an unsupervised learning algorithm that\'s used to generate word embeddings.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:21,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:\\"It captures the syntactic and semantic links between words by representing them as dense vectors in a continuous vector space.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:22,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:\\"Word2Vec acquires word embeddings by training on large corpora and predicting the context of words within a designated text window, encompassing either the target word itself or the surrounding words.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:23,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:[\\"It can be trained using two different algorithms -- \\",(0,r.jsxDEV)(n.a,{href:\\"https://medium.com/@codethulo/understanding-the-continuous-bag-of-words-cbow-model-architecture-working-mechanism-and-math-78c7284a8d5a\\",children:\\"Continuous Bag of Words\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:24,columnNumber:55},this),\\" and Skip-Gram.\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:24,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:\\"Word2Vec embeddings are often used to measure word similarity or as input features for downstream natural language processing tasks.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:25,columnNumber:1},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:21,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.a,{href:\\"https://www.techtarget.com/searchenterpriseai/definition/masked-language-models-MLMs\\",children:\\"What are Masked Language Models (MLMs)? | Definition from TechTarget\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:27,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:27,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h1,{id:\\"introduction\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#introduction\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this),\\"Introduction\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:29,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h2,{id:\\"self-supervised-learning\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#self-supervised-learning\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this),\\"Self-supervised Learning\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:31,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:[(0,r.jsxDEV)(n.em,{children:\\"Pre-trained language models (PLMs) can learn generalisable representations of data without human annotated labels in a self-supervised learning (SSL) style, leading to remarkable performance improvement in natural language processing and related fields (Brown et al., 2020; Fang et al., 2022; Chen et al., 2021a). \\\\u2192\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:33,columnNumber:1},this),\\" PLM (Pre-trained Language Model) \\\\uAE30\\\\uBC18 \\\\uC720\\\\uC0AC\\\\uC131 \\\\uD310\\\\uB2E8\\\\uC740 \\\\uC74C\\\\uC545 \\\\uC2DC\\\\uD000\\\\uC2A4 \\\\uC801\\\\uC6A9\\\\uC5D0 \\\\uC720\\\\uB9DD\\\\uD558\\\\uB2E4.\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:33,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.em,{children:\\"First, PLMs can potentially pave the way to unify the modelling of a wide range of music understanding, or the so-called Music Information Retrieval (MIR) tasks, including but not limited to music tagging, beat tracking, music transcription, and source separation, so that different tasks no longer need task-specific models or features.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:35,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:35,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:[\\"*Unfortunately, we are yet to see a general-purpose and cost-effective open-source PLM on acoustic music understanding. Most existing studies are designed to solely address music tagging problems (Pons and Serra, 2019; Spijkervet and Burgoyne, 2021; McCallum et al., 2022; Huang et al., 2022; Zhu et al., 2021; Zhao and Guo, 2021), and \\",(0,r.jsxDEV)(n.strong,{children:\\"many of them do not provide open-source code bases or checkpoints for further evaluation.*\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:37,columnNumber:338},this)]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:37,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/4ebb6c73-adac-48fd-9eaa-a34d824fd1c3\\",alt:\\"\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:39,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:39,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.em,{children:\\"MERT inherits a speech SSL paradigm, employing teacher models to generate pseudo targets for sequential audio clips. Specifically, to capture the distinctive pitched and tonal characteristics in music, MERT incorporates a multi-task paradigm to balance the acoustic and musical representation learning as demonstrated in Fig. 1.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:41,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:41,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h2,{id:\\"rvq-vae-residual-vector-quantization-variational-autoencoder\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#rvq-vae-residual-vector-quantization-variational-autoencoder\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this),\\"RVQ-VAE (Residual Vector Quantization Variational AutoEncoder)\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:43,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.a,{href:\\"https://www.assemblyai.com/blog/what-is-residual-vector-quantization/\\",children:\\"What is Residual Vector Quantization?\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:45,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:45,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.a,{href:\\"https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/rq/\\",children:\\"[\\\\uB17C\\\\uBB38\\\\uB9AC\\\\uBDF0] Autoregressive Image Generation using Residual Quantization (RQ-VAE-Transformer)\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:47,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:47,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h2,{id:\\"cqt-constant-q-transformation\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#cqt-constant-q-transformation\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this),\\"CQT (Constant-Q Transformation)\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:49,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.ul,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:(0,r.jsxDEV)(n.em,{children:\\"CQT is a type of frequency transform that is widely used in various MIR tasks, such as pitch detection, chord recognition, and music transcriptions\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:51,columnNumber:3},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:51,columnNumber:1},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:51,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.a,{href:\\"https://en.wikipedia.org/wiki/Constant-Q_transform\\",children:\\"Constant-Q transform\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:53,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:53,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.a,{href:\\"https://heeya-stupidbutstudying.tistory.com/entry/DL-%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9D%8C%EC%84%B1-%EC%9D%B4%ED%95%B4-Introduction-to-sound-data-analysis\\",children:\\"[DL] \\\\uB525\\\\uB7EC\\\\uB2DD \\\\uC74C\\\\uC131 \\\\uC774\\\\uD574 - Introduction to sound data analysis\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:55,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:55,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.em,{children:\\"To summarise, our contributions are:\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:57,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:57,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.ul,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:(0,r.jsxDEV)(n.em,{children:\\"proposing a multi-task style predictive acoustic self-supervised learning paradigm, which achieves SOTA performance on various MIR tasks, including important yet unexplored tasks for pre-training such as pitch detection, beat tracking and source separation applications;\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:59,columnNumber:3},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:59,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:(0,r.jsxDEV)(n.em,{children:\\"conducting a broad range of analysis based on ablation studies of the proposed MERT pretraining paradigm;\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:60,columnNumber:3},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:60,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:(0,r.jsxDEV)(n.em,{children:\\"exploring robust and stable strategies for acoustic music model training to overcome training instability and frequent crashes when scaling up the pre-training on model size;\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:61,columnNumber:3},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:61,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:(0,r.jsxDEV)(n.em,{children:\\"providing an open-source, generalisable and computationally affordable acoustic music pretrained model, which addresses the needs of both industry and research communities.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:62,columnNumber:3},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:62,columnNumber:1},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:59,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.hr,{},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:64,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h1,{id:\\"related-work\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#related-work\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this),\\"Related Work\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:66,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h2,{id:\\"plms-for-acoustic-music\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#plms-for-acoustic-music\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this),\\"PLMs for Acoustic Music\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:68,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.em,{children:\\"Existing acoustic music pre-trained models primarily focus on tagging tasks and rely on supervised tagging labels for pre-training (Pons and Serra, 2019; Spijkervet and Burgoyne, 2021; McCallum et al., 2022; Huang et al., 2022).\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:70,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:70,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.em,{children:\\"they face limitations in training data and model size, hampering the performance improvements (Choi et al., 2017; Li et al., 2022). Additionally, several models trained on inaccessible datasets or without publicly available codes and model weights make it difficult to reproduce or extend their approaches (McCallum et al., 2022; Castellon et al., 2021; Li et al., 2022; Zhu et al., 2021; Zhao and Guo, 2021).\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:72,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:72,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h2,{id:\\"self-supervised-speech-processing\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#self-supervised-speech-processing\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this),\\"Self-Supervised Speech Processing\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:74,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.em,{children:\\"both acoustic music and speech processing models need to deal with the cocktail party problem (Brown and Bidelman, 2022; Petermann et al., 2022) since good source separation capabilities help both separating noises and background sounds with speech and processing polyphonic music audio.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:76,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:76,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h3,{id:\\"cocktail-party-problem\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#cocktail-party-problem\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this),\\"Cocktail Party Problem\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:78,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.a,{href:\\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2692487/\\",children:\\"NCBI - WWW Error Blocked Diagnostic\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:80,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:80,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:\\"\\\\uCF54\\\\uD154 \\\\uD30C\\\\uD2F0 \\\\uBB38\\\\uC81C(Cocktail Party Problem)\\\\uB294 \\\\uC18C\\\\uB9AC \\\\uC2E0\\\\uD638 \\\\uCC98\\\\uB9AC \\\\uBC0F \\\\uC778\\\\uACF5\\\\uC9C0\\\\uB2A5 \\\\uBD84\\\\uC57C\\\\uC5D0\\\\uC11C \\\\uC911\\\\uC694\\\\uD55C \\\\uBB38\\\\uC81C\\\\uB85C, \\\\uC0AC\\\\uB78C\\\\uB4E4\\\\uC774 \\\\uC5EC\\\\uB7EC \\\\uB300\\\\uD654\\\\uAC00 \\\\uACB9\\\\uCE58\\\\uB294 \\\\uC18C\\\\uC74C \\\\uC18D\\\\uC5D0\\\\uC11C \\\\uC6D0\\\\uD558\\\\uB294 \\\\uB300\\\\uD654\\\\uB97C \\\\uC2DD\\\\uBCC4\\\\uD558\\\\uB294 \\\\uC0C1\\\\uD669\\\\uC744 \\\\uB2E4\\\\uB8E8\\\\uACE0 \\\\uC788\\\\uB2E4. \\\\uC774 \\\\uBB38\\\\uC81C\\\\uB294 \\\\uC2E4\\\\uC81C \\\\uCF54\\\\uD154 \\\\uD30C\\\\uD2F0\\\\uC5D0\\\\uC11C \\\\uB9CE\\\\uC740 \\\\uC0AC\\\\uB78C\\\\uB4E4\\\\uC774 \\\\uB3D9\\\\uC2DC\\\\uC5D0 \\\\uB300\\\\uD654\\\\uD558\\\\uB294 \\\\uC0C1\\\\uD669\\\\uC744 \\\\uC0C1\\\\uC0C1\\\\uD574 \\\\uBCF4\\\\uBA74 \\\\uC774\\\\uD574\\\\uD558\\\\uAE30 \\\\uC27D\\\\uB2E4. \\\\uC774 \\\\uC0C1\\\\uD669\\\\uC5D0\\\\uC11C \\\\uC6B0\\\\uB9AC\\\\uB294 \\\\uD55C \\\\uBA85\\\\uC758 \\\\uBAA9\\\\uC18C\\\\uB9AC\\\\uC5D0 \\\\uC9D1\\\\uC911\\\\uD558\\\\uC5EC \\\\uB2E4\\\\uB978 \\\\uBAA8\\\\uB4E0 \\\\uC18C\\\\uC74C\\\\uC744 \\\\uBC30\\\\uC81C\\\\uD558\\\\uACE0\\\\uC790 \\\\uD55C\\\\uB2E4.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:82,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:\'\\\\uCEF4\\\\uD4E8\\\\uD130 \\\\uACFC\\\\uD559 \\\\uBC0F \\\\uC2E0\\\\uD638 \\\\uCC98\\\\uB9AC\\\\uC5D0\\\\uC11C\\\\uB294 \\\\uC774\\\\uB7EC\\\\uD55C \\\\uBB38\\\\uC81C\\\\uB97C \\"\\\\uBE14\\\\uB77C\\\\uC778\\\\uB4DC \\\\uC18C\\\\uC2A4 \\\\uBD84\\\\uB9AC(Blind Source Separation)\\"\\\\uB77C\\\\uACE0\\\\uB3C4 \\\\uBD80\\\\uB974\\\\uBA70, \\\\uD55C \\\\uAC00\\\\uC9C0 \\\\uC74C\\\\uC131 \\\\uC2E0\\\\uD638\\\\uB97C \\\\uC5EC\\\\uB7EC \\\\uC18C\\\\uC2A4\\\\uC5D0\\\\uC11C \\\\uBD84\\\\uB9AC\\\\uD558\\\\uACE0 \\\\uC6D0\\\\uD558\\\\uB294 \\\\uC2E0\\\\uD638\\\\uB9CC\\\\uC744 \\\\uCD94\\\\uCD9C\\\\uD558\\\\uB294 \\\\uC791\\\\uC5C5\\\\uC744 \\\\uC758\\\\uBBF8\\\\uD55C\\\\uB2E4. \\\\uC774 \\\\uBB38\\\\uC81C\\\\uB294 \\\\uC778\\\\uAC04\\\\uC758 \\\\uCCAD\\\\uAC01 \\\\uB2A5\\\\uB825\\\\uC5D0 \\\\uBE44\\\\uD574 \\\\uAE30\\\\uACC4\\\\uB85C \\\\uD574\\\\uACB0\\\\uD558\\\\uB294 \\\\uB370 \\\\uC788\\\\uC5B4 \\\\uC0C1\\\\uB2F9\\\\uD55C \\\\uC5B4\\\\uB824\\\\uC6C0\\\\uC774 \\\\uC788\\\\uB2E4.\'},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:84,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:\\"\\\\uD574\\\\uACB0 \\\\uBC29\\\\uBC95\\\\uC73C\\\\uB85C\\\\uB294 \\\\uB3C5\\\\uB9BD \\\\uC131\\\\uBD84 \\\\uBD84\\\\uC11D(Independent Component Analysis, ICA), \\\\uD30C\\\\uB3D9 \\\\uBCC0\\\\uD658, \\\\uB525\\\\uB7EC\\\\uB2DD \\\\uAE30\\\\uBC18 \\\\uBAA8\\\\uB378 \\\\uB4F1 \\\\uB2E4\\\\uC591\\\\uD55C \\\\uAE30\\\\uC220\\\\uC774 \\\\uD65C\\\\uC6A9\\\\uB41C\\\\uB2E4. ICA\\\\uB294 \\\\uC11C\\\\uB85C \\\\uB3C5\\\\uB9BD\\\\uC801\\\\uC778 \\\\uC2E0\\\\uD638 \\\\uC131\\\\uBD84\\\\uC744 \\\\uBD84\\\\uB9AC\\\\uD558\\\\uB294 \\\\uBC29\\\\uBC95\\\\uC774\\\\uACE0, \\\\uB525\\\\uB7EC\\\\uB2DD \\\\uAE30\\\\uBC18 \\\\uBAA8\\\\uB378\\\\uC740 \\\\uB300\\\\uADDC\\\\uBAA8 \\\\uB370\\\\uC774\\\\uD130\\\\uC14B\\\\uC744 \\\\uD559\\\\uC2B5\\\\uC2DC\\\\uCF1C \\\\uD328\\\\uD134 \\\\uC778\\\\uC2DD \\\\uBC0F \\\\uC74C\\\\uC131 \\\\uBD84\\\\uB9AC\\\\uC5D0 \\\\uC0AC\\\\uC6A9\\\\uB41C\\\\uB2E4.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:86,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h2,{id:\\"audio-representation-with-language-modelling\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#audio-representation-with-language-modelling\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this),\\"Audio Representation with Language Modelling\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:88,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:[\\"*Mask strategy-based large-scale language models have been applied to a wide range of domains (Lample and Charton, 2019; Chen et al., 2021a;b; Fang et al., 2022), but \\",(0,r.jsxDEV)(n.strong,{children:\\"still remain under-explored in acoustic music understanding.*\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:90,columnNumber:169},this)]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:90,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:[\\"*Baevski and Mohamed (2020) introduce a pre-trained VQ-VAE (Baevski et al., 2019) to provide prediction targets to conduct speech representation learning with MLM. While introducing K-means to provide discrete token codebooks and pre-training the model to detect sound units, \\",(0,r.jsxDEV)(n.strong,{children:\\"Hsu et al. (2021) claim that a better teacher model in SSL could lead to better downstream task performance.*\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:92,columnNumber:278},this)]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:92,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:[\\"*the recently released \\",(0,r.jsxDEV)(n.strong,{children:\\"RVQ-VAEs (Zeghidour et al., 2021; D\\\\xB4 efossez et al., 2022), achieving good results in music reconstruction, could be adopted as teacher models for music understanding pre-training and provide acoustic information guidance.*\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:94,columnNumber:25},this)]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:94,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.hr,{},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:96,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h1,{id:\\"methodology\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#methodology\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this),\\"Methodology\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:98,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h2,{id:\\"pre-training-with-mlm\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#pre-training-with-mlm\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this),\\"Pre-Training with MLM\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:100,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/2d05f4c4-573b-43ed-8772-003218055edd\\",alt:\\"Untitled 1\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:102,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:102,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/9d889361-88d5-4d84-9180-4e841fd66e0d\\",alt:\\"Untitled 2\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:104,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:104,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h3,{id:\\"hubert\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#hubert\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this),\\"HuBERT\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:106,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/3d347f1a-85d9-49b7-aa4d-3989e879d849\\",alt:\\"Untitled 3\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:108,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:108,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.a,{href:\\"https://velog.io/@9e0na/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Speech-HuBERT-Self-Supervised-Speech-RepresentationLearning-by-Masked-Prediction-of-Hidden-Units-2021-Summary\\",children:\\"[\\\\uB17C\\\\uBB38\\\\uB9AC\\\\uBDF0 | Speech] HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units (2021) Summary\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:110,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:110,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h2,{id:\\"modelling-acoustic-information\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#modelling-acoustic-information\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this),\\"Modelling Acoustic Information\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:112,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:[\\"Mel-Frequency Cepstral Coefficients (MFCCs) are \\",(0,r.jsxDEV)(n.strong,{children:\\"only capable at\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:114,columnNumber:49},this),\\" modeling acoustic and single-pitch signals\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:114,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.a,{href:\\"https://brightwon.tistory.com/11\\",children:\\"MFCC(Mel-Frequency Cepstral Coefficient) \\\\uC774\\\\uD574\\\\uD558\\\\uAE30\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:116,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:116,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h3,{id:\\"method-1-using-k-means-on-the-log-mel-spectrum--chroma-features\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#method-1-using-k-means-on-the-log-mel-spectrum--chroma-features\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this),\\"Method 1. Using K-means on the log-Mel spectrum & Chroma Features\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:118,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.ul,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:\\"In case of music, each frame contain more informations than that of speech. \\\\u2192 Larger number of classes is needed!\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:120,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.em,{children:\\"The complexity of the k-means algorithm is linear with the number of centroids, leading to a time-consuming k-means for the music feature.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:121,columnNumber:3},this),\\" \\\\u2192 300-means for log-Mel spectrum with dimension 229, and 200-means for Chroma features with dimension 264 \\\\u2192 \\",(0,r.jsxDEV)(n.strong,{children:\\"Computational complexity remains comparable to that of HuBERT\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:121,columnNumber:252},this)]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:121,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:[\\"Disadvantage\\",`\\n`,(0,r.jsxDEV)(n.ul,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:\\"difficult to scale up to a larger number of classes and larger datasets\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:123,columnNumber:3},this),`\\n`,(0,r.jsxDEV)(n.li,{children:\\"results are sensitive to initialization\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:124,columnNumber:3},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:123,columnNumber:3},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:122,columnNumber:1},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:120,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h3,{id:\\"method-2-encodec-8-layer-residual-vq-vae\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#method-2-encodec-8-layer-residual-vq-vae\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this),\\"Method 2. EnCodec (8-layer residual VQ-VAE)\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:126,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.ul,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:\\"Each acoustic features are denoted as 2-dimensional auditory code matrix with L (length of the recording)\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:128,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:(0,r.jsxDEV)(n.em,{children:\\"Converts 24kHz input waveforms to 8 different embeddings at 75Hz with a 320-fold reduction, and the quantizer has 1024 dimensions \\\\u2192 Decoder of Encodec can reconstruct the waveform at 24kHz with authentic information in timbre.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:129,columnNumber:3},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:129,columnNumber:1},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:128,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.a,{href:\\"https://ostin.tistory.com/206\\",children:\\"High Fidelity Neural Audio Compression (EnCodec)\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:131,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:131,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.a,{href:\\"https://velog.io/@dien-eaststar/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-SSL-Neural-Discrete-Representation-Learning\\",children:\\"[\\\\uB17C\\\\uBB38 \\\\uB9AC\\\\uBDF0] VQ-VAE: Neural Discrete Representation Learning\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:133,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:133,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h2,{id:\\"modeling-musical-information\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#modeling-musical-information\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this),\\"Modeling Musical Information\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:135,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.ul,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:\\"CQT Spectrogram is used solely for pitch-level information, regardless of acoustic information. (Similar to Fourier Transformation)\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:137,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:[\\"Bin widths are proportional to frequency \\\\u2192 \\",(0,r.jsxDEV)(n.strong,{children:\\"giving each octave the same number of bins\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:138,columnNumber:46},this)]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:138,columnNumber:1},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:137,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"\\",alt:\\"Uploading Untitled 4.png\\\\u2026\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:140,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:140,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.hr,{},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:142,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.ul,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\\"\\\\uB2E4\\\\uC6B4\\\\uC2A4\\\\uD2B8\\\\uB9BC \\\\uC791\\\\uC5C5:\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:144,columnNumber:3},this),\\" 14\\\\uAC1C\\\\uC758 \\\\uB2E4\\\\uC6B4\\\\uC2A4\\\\uD2B8\\\\uB9BC \\\\uC791\\\\uC5C5\\\\uC744 \\\\uB300\\\\uC0C1\\\\uC73C\\\\uB85C \\\\uD3C9\\\\uAC00\\\\uD558\\\\uBA70, \\\\uC74C\\\\uC545 \\\\uD0DC\\\\uAE45, \\\\uD0A4 \\\\uD0D0\\\\uC9C0, \\\\uC7A5\\\\uB974 \\\\uBD84\\\\uB958, \\\\uAC10\\\\uC815 \\\\uC810\\\\uC218 \\\\uD68C\\\\uADC0, \\\\uC545\\\\uAE30 \\\\uBD84\\\\uB958, \\\\uD53C\\\\uCE58 \\\\uBD84\\\\uB958, \\\\uBCF4\\\\uCEEC \\\\uAE30\\\\uC220 \\\\uD0D0\\\\uC9C0, \\\\uAC00\\\\uC218 \\\\uC2DD\\\\uBCC4\\\\uACFC \\\\uAC19\\\\uC740 \\\\uD504\\\\uB808\\\\uC784 \\\\uC218\\\\uC900\\\\uC758 \\\\uC791\\\\uC5C5\\\\uACFC \\\\uBC15\\\\uC790 \\\\uCD94\\\\uC801, \\\\uC18C\\\\uC2A4 \\\\uBD84\\\\uB9AC\\\\uC640 \\\\uAC19\\\\uC740 \\\\uC21C\\\\uCC28 \\\\uC791\\\\uC5C5\\\\uC744 \\\\uD3EC\\\\uD568\\\\uD55C\\\\uB2E4.\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:144,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\\"\\\\uD504\\\\uB85C\\\\uBE59 \\\\uD504\\\\uB85C\\\\uD1A0\\\\uCF5C:\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:145,columnNumber:3},this),\\" \\\\uBC31\\\\uBCF8 \\\\uBAA8\\\\uB378\\\\uC744 \\\\uACE0\\\\uC815\\\\uD55C \\\\uCC44 \\\\uAC04\\\\uB2E8\\\\uD55C \\\\uB2E4\\\\uC6B4\\\\uC2A4\\\\uD2B8\\\\uB9BC \\\\uAD6C\\\\uC870\\\\uB9CC \\\\uD6C8\\\\uB828\\\\uD558\\\\uC5EC \\\\uD3C9\\\\uAC00\\\\uD558\\\\uBA70, \\\\uD558\\\\uC774\\\\uD37C\\\\uD30C\\\\uB77C\\\\uBBF8\\\\uD130 \\\\uAC80\\\\uC0C9 \\\\uACF5\\\\uAC04\\\\uB3C4 \\\\uC81C\\\\uD55C\\\\uD55C\\\\uB2E4.\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:145,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:[\\"Baseline Model:\\",`\\n`,(0,r.jsxDEV)(n.ul,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\\"\\\\uC74C\\\\uC545:\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:147,columnNumber:5},this),\\" MusiCNN, CLMR, MULE, Jukebox \\\\uBC0F JukeMIR\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:147,columnNumber:3},this),`\\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\\"\\\\uC2A4\\\\uD53C\\\\uCE58:\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:148,columnNumber:5},this),\\" HuBERT, data2vec\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:148,columnNumber:3},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:147,columnNumber:3},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:146,columnNumber:1},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:144,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.hr,{},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:150,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h1,{id:\\"appendix\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#appendix\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this),\\"Appendix\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:152,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h2,{id:\\"downstream-tasks\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#downstream-tasks\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this),\\"DownStream Tasks\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:154,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:\\"We evaluate the models on 14 downstream tasks to provide a comprehensive view of our method and the comparison between baselines. The full descriptions of the datasets and tasks are given as follows.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:156,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.ul,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:\\"Music Tagging involves determining which of a set of fixed tags apply to a particular song. Tag categories may include genre, instrumentation, mood, tempo (e.g. fast) or other tags. We used two large datasets: MagnaTagATune (MTT) (Law et al., 2009) and MTG-Jamendo (Bogdanov et al., 2019). For both datasets, we limit the tag vocabulary according to official instructions. We use all clips in MTTandMTG-Jamendo. Since many of the audio recordings among 5.5k MTG-Jamendo excerpts are longer than the 30s, we averaged the multiple embeddings computed with a sliding window as the overall embedding. The window length is set to the same default length as in every system. For MERT series, the window length is typically set to 30s. The metrics are the macro-average of ROC-AUCs and the average precision (AP) / PR-AUC among all top-50 tags.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:158,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:\\"Key detection predicts the tonal scale and dominant pitch level of a song. We use Giantsteps (Knees et al., 2015) as test set and a commonly-used subset of Giantsteps-MTG-keys dataset (Korzeniowski and Widmer, 2017) as the training and validation set. The splitting is the same as in (Castellon et al., 2021). The metric is a refined accuracy with error tolerance, giving partial credit to reasonable errors (Raffel et al., 2014).\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:159,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:\\"Genre classification estimates the most appropriate genre for each given song. We report the accuracy of the GTZAN (Tzanetakis and Cook, 2002) dataset along with ROC and AP on MTG-Genre, since the former task is a multi-class classification and the latter is multi-label. We used the standard \\\\u201Dfail-filtered\\\\u201D split (Kereliuk et al., 2015) for GTZAN.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:160,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:\\"Emotion score regression. The Emomusic dataset (Soleymani et al., 2013) contains 744 music clips of 45 seconds in length, each reported on a two-dimensional valence-arousal plane after listening, where valence indicates positive and negative emotional responses, and arousal indicates emotional intensity. We use the same dataset split as (Castellon et al., 2021). The official evaluation metric is the determination coefficient (r2) between the model regression results and human annotations of arousal (EmoA) and valence (EmoV) (Soleymani et al., 2013). For inference, we split the 45-second clip into a 5-second sliding window and averaged the prediction.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:161,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:\\"Instrument classification is the process of identifying which instruments are included in a given sound. We use the Nsynth (Engel et al., 2017) and MTG-instrument datasets. The former is a monophonic note-level multi-class task with 306k audio samples in 11 instrument classes with accuracy as an indicator. The latter is a subset of MTG-Jamendo, containing 25k polyphonic audio tracks and 41 instrument tags; each track can contain multiple instruments and is evaluated on ROC and AP.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:162,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:\\"Pitch classification estimates which of the 128 pitch categories the given audio segment belongs to. Weuse the NSynth dataset for this task. Given these segments are short monophonic audio, this task is multi-class, and the accuracy is used as an evaluation metric.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:163,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:\\"Vocal technique detection involves identifying what singing techniques are contained in a given audio clip. We use the VocalSet dataset (Wilkins et al., 2018), which is the only publicly available dataset for the study of singing techniques. The dataset contains the vocals of 20 different professional singers (9 female and 11 male) who perform 17 different singing techniques in various contexts for a total of 10.1 hours. As the audio clips are divided into 3 seconds, the task only requires a judgement on the type of technique and not on the start and end of the technique. We used the same 10 different singing techniques as in Yamamoto et al. (2022) as a subset and used the same 15 singers as the training and validation sets and 5 singers as the test set. Since there is no accepted division between training and validation sets, we selected 9 singers as the training set and 6 singers as the validation set. All the 3-second segments that originate from the same recording are allocated to the same part of the split (e.g. all are in the training set). The evaluation metric is accuracy. 16Published as a conference paper at ICLR 2024\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:164,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:\\"Singer identification identifies the vocal performer from a given recording. We use the VocalSet dataset for this task. We randomly divided the dataset into a training set, validation set and testing set based on a ratio of 12:8:5, all containing the same 20 singers.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:165,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:\\"Beat tracking is the process of determining whether there is a beat in each frame of a given piece of music. We use an offline approach to the binary classification, i.e. the model can use information following each frame to help with inference. The model needs to output frame-by-frame predictions at a certain frequency and post-process them using a dynamic Bayesian network (DBN) (B\\\\xA8 ock et al., 2016b) to obtain the final result. The DBN is implemented using madmom (B\\\\xA8 ock et al., 2016a). The dataset we use is GTZAN Rhythm (Marchand and Peeters, 2015). We also label the two adjacent frames of each label as beat, which is a common way of label smoothing in beat tracking to improve the performance of the model and to compare the SSL model fairly with the spin model. The model is evaluated using the f measure implemented in mir eval (Raffel et al., 2014), and the prediction is considered correct if the difference between the predicted event and the ground truth does not exceed 20ms. In this task, some models were trained on other datasets, and the full GTZAN set was used as the test set.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:166,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:\\"Source separation. Source separation aims to demix the music recording into its constituent parts, e.g., vocals, drums, bass, and others. We adopt MUSDB18 (Rafii et al., 2017), a widely used benchmark dataset in music source separation. MUSDB18 contains 150 full-length music tracks (\\\\u02DC 10 hours), along with multiple isolated stems. We use 86 tracks for training, 14 tracks for validation, and 50 tracks for evaluation following the official setting in MUSDB18. During training, we randomly sample 6-second segments and apply random track mixing for augmentation. Due to the difficulty of this task, we adopt the baseline architecture in the Music Demixing Challenge (MDX) 2021 (Mitsufuji et al., 2022), which consists of three linear layers and three bi-directional LSTM layers. We directly compute the l2-loss between predicted and ground-truth spectrograms for optimisation. The metric for this task is the Source-to-Distortion Ratio (SDR) defined by MDX 2021 (Mitsufuji et al., 2022), which is the mean across the SDR scores of all songs.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:167,columnNumber:1},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:158,columnNumber:1},this)]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\",lineNumber:1,columnNumber:1},this)}function _n(l={}){let{wrapper:n}=l.components||{};return n?(0,r.jsxDEV)(n,Object.assign({},l,{children:(0,r.jsxDEV)(Ge,l,void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this)}),void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-d232f612-9199-452a-884c-231a5d95d290.mdx\\"},this):Ge(l)}var yn=_n;return pn(Nn);})();\\n/*! Bundled license information:\\n\\nreact/cjs/react-jsx-dev-runtime.development.js:\\n  (**\\n   * @license React\\n   * react-jsx-dev-runtime.development.js\\n   *\\n   * Copyright (c) Facebook, Inc. and its affiliates.\\n   *\\n   * This source code is licensed under the MIT license found in the\\n   * LICENSE file in the root directory of this source tree.\\n   *)\\n*/\\n;return Component;"},"_id":"mert/index.mdx","_raw":{"sourceFilePath":"mert/index.mdx","sourceFileName":"index.mdx","sourceFileDir":"mert","contentType":"mdx","flattenedPath":"mert"},"type":"Blog","url":"/blogs/mert","readingTime":{"text":"15 min read","minutes":14.03,"time":841800,"words":2806},"toc":[{"level":"two","text":"Base: Masked Language Modeling","slug":"base-masked-language-modeling"},{"level":"three","text":"**How is MLM different from Word2Vec?**","slug":"how-is-mlm-different-from-word2vec"},{"level":"one","text":"Introduction","slug":"introduction"},{"level":"two","text":"Self-supervised Learning","slug":"self-supervised-learning"},{"level":"two","text":"RVQ-VAE (Residual Vector Quantization Variational AutoEncoder)","slug":"rvq-vae-residual-vector-quantization-variational-autoencoder"},{"level":"two","text":"CQT (Constant-Q Transformation)","slug":"cqt-constant-q-transformation"},{"level":"one","text":"Related Work","slug":"related-work"},{"level":"two","text":"PLMs for Acoustic Music","slug":"plms-for-acoustic-music"},{"level":"two","text":"Self-Supervised Speech Processing","slug":"self-supervised-speech-processing"},{"level":"three","text":"Cocktail Party Problem","slug":"cocktail-party-problem"},{"level":"two","text":"Audio Representation with Language Modelling","slug":"audio-representation-with-language-modelling"},{"level":"one","text":"Methodology","slug":"methodology"},{"level":"two","text":"Pre-Training with MLM","slug":"pre-training-with-mlm"},{"level":"three","text":"HuBERT","slug":"hubert"},{"level":"two","text":"Modelling Acoustic Information","slug":"modelling-acoustic-information"},{"level":"three","text":"Method 1. Using K-means on the log-Mel spectrum & Chroma Features","slug":"method-1-using-k-means-on-the-log-mel-spectrum--chroma-features"},{"level":"three","text":"Method 2. EnCodec (8-layer residual VQ-VAE)","slug":"method-2-encodec-8-layer-residual-vq-vae"},{"level":"two","text":"Modeling Musical Information","slug":"modeling-musical-information"},{"level":"one","text":"Appendix","slug":"appendix"},{"level":"two","text":"DownStream Tasks","slug":"downstream-tasks"}]}');

/***/ })

});