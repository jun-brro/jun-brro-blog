"use strict";
/*
 * ATTENTION: An "eval-source-map" devtool has been used.
 * This devtool is neither made for production nor for readable output files.
 * It uses "eval()" calls to create a separate source file with attached SourceMaps in the browser devtools.
 * If you are trying to read the output file, select a different devtool (https://webpack.js.org/configuration/devtool/)
 * or disable the default devtool with "devtool: false".
 * If you are looking for production-ready output files, see mode: "production" (https://webpack.js.org/configuration/mode/).
 */
self["webpackHotUpdate_N_E"]("app/page",{

/***/ "(app-pages-browser)/./.contentlayer/generated/Blog/detr__index.mdx.json":
/*!***********************************************************!*\
  !*** ./.contentlayer/generated/Blog/detr__index.mdx.json ***!
  \***********************************************************/
/***/ (function(module, __unused_webpack_exports, __webpack_require__) {

module.exports = JSON.parse('{"title":"[Paper Review] End-to-End Object Detection with Transformers (DETR)","publishedAt":"2024-07-23T00:00:00.000Z","updatedAt":"2024-07-23T00:00:00.000Z","description":"","image":{"filePath":"../public/blogs/slowfast/screenshot.png","relativeFilePath":"../../public/blogs/slowfast/screenshot.png","format":"png","height":1276,"width":2528,"aspectRatio":1.9811912225705328,"blurhashDataUrl":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAKlBMVEXr7Oz19fT////d4eDm5+fw8PD5+fnW1tXMzM6oqKfIz9q2wNK4ubq8vb9psxfqAAAACXBIWXMAABYlAAAWJQFJUiTwAAAANklEQVR4nAXBiQHAMAwCsQMc5+3+61aCwaJKrJn7nfsInFkJti21mOVWD7EXoQK7FIMZltz2DyemAPZRxtcpAAAAAElFTkSuQmCC"},"isPublished":true,"author":"junbrro","tags":["Deep Learning"],"body":{"raw":"\\nThis post is the review of DETR Paper\\n\\n**Citations**\\n[DSBA Lab\'s Youtube video](https://youtu.be/q1wSykClIMk?si=fFxHKqCrNkuOcW9F)\\n[HerbWood Tistory blog](https://herbwood.tistory.com/26)\\n\\n# DETR: End-to-End Object Detection with Transformers\\n\\nObject detection is a task that involves detecting objects within an image and predicting their class and location. Traditional object detection models are divided into two categories: one-stage detectors and two-stage detectors. One-stage detectors, like YOLO (You Only Look Once), are known for their speed and suitability for real-time applications. In contrast, two-stage detectors, such as Faster R-CNN, generally offer higher accuracy but at the cost of speed.\\n\\nThe Transformer model, initially designed for NLP tasks, has been adapted for vision tasks in DETR. DETR leverages the Transformer architecture\'s encoder-decoder structure and its ability to learn global information through the attention mechanism. However, unlike Vision Transformers (ViT), which directly input image patches into the encoder, DETR utilizes image features extracted via a CNN.\\n\\n## DETR Network Architecture\\n\\n![DETR Architecture](https://github.com/user-attachments/assets/ce5f48c6-73d6-45d6-9c0d-ecd0ec88dd47)\\n\\n![Detailed Structure](https://github.com/user-attachments/assets/192780ef-86dd-46e8-8340-868ab95e7a5f)\\n\\n### Key Differences Between DETR and Traditional Transformers\\n\\n1. **Feature Map Input**\\n\\n   - DETR receives image feature maps from the encoder, whereas traditional Transformers take embeddings from sentences as input. DETR extracts feature maps using a CNN backbone, reduces their dimensions through a 1x1 convolution layer, and flattens the spatial dimensions before inputting them into the encoder. For example, a feature map with height \\\\( h \\\\), width \\\\( w \\\\), and channel count \\\\( C \\\\) is converted to \\\\( d \\\\times hw \\\\) for input, where \\\\( d \\\\) is smaller than \\\\( C \\\\).\\n\\n2. **Positional Encoding**\\n\\n   - Transformers use positional encoding to maintain order within the input sequence due to their permutation invariant nature. DETR generalizes this to 2D spatial positional encoding for feature maps. It applies 2D sine and cosine functions along the x and y axes to derive positional encoding, which is then concatenated channel-wise and added to the input features.\\n\\n3. **Object Queries**\\n\\n   - Instead of target embeddings used in traditional Transformers, DETR uses object queries, which are learnable embeddings of length \\\\( N \\\\).\\n\\n4. **Attention Mechanism**\\n\\n   - Traditional Transformers employ masked multi-head attention during the first attention operation in the decoder to prevent future token information from being used. DETR, however, performs multi-head self-attention without masking since it predicts all object locations in parallel.\\n\\n5. **Dual Head Output**\\n   - DETR has two heads in the decoder: one for predicting bounding boxes and the other for class probabilities, unlike traditional Transformers, which have a single linear layer for predicting class probabilities.\\n\\n![Transformer Encoder](https://github.com/user-attachments/assets/7031c6c2-b347-4211-9fb2-f2d39af866c8)\\n\\n### Encoder\\n\\nThe image is converted into a feature map via a CNN, which serves as the input for the Transformer encoder. Unlike traditional CNNs, the encoder can capture global information.\\n\\n![Positional Encoding](https://github.com/user-attachments/assets/1df7e4f1-0a3d-4012-9607-0e3651b387ff)\\n\\n### Decoder\\n\\nThe Transformer decoder receives object queries as input, learning to detect specific parts of the image. Positional embedding in the decoder is not meaningful since object queries are independent and parallel, rendering position information irrelevant. Instead, object queries output positional encoding via attention mechanisms.\\n\\n![Object Queries](https://github.com/user-attachments/assets/7ddd7b31-f831-4120-9613-6c2512d8e47d)\\n\\nDETR introduces the concept of Bipartite Matching to match predictions with ground truth objects, optimizing the match using the Hungarian algorithm based on class prediction and box prediction costs.\\n\\n![Hungarian Algorithm](https://github.com/user-attachments/assets/ff3e72db-4684-46ae-9d2b-8a0ffae1c482)\\n\\n### Loss Function\\n\\n1. **Classification Loss**\\n   - Measures the difference between the predicted class probabilities and the actual classes using cross-entropy loss.\\n2. **Bounding Box Loss**\\n   - Measures the difference between predicted and actual bounding box coordinates using L1 loss and Generalized IoU (GIoU) loss. While IoU measures the ratio of overlapping area between two bounding boxes, GIoU introduces an additional term to handle cases where boxes do not overlap, providing a more suitable metric.\\n\\n![Loss Function](https://github.com/user-attachments/assets/a13435a8-7719-43c5-b4c4-69e7358f0707)\\n\\n### Performance Evaluation and Experimental Results\\n\\nDETR\'s performance was evaluated using the COCO dataset, a standard benchmark for object detection models. The key metrics used were Average Precision (AP) and Average Recall (AR).\\n\\n- **Large Object Detection**\\n  - DETR excels at detecting large objects due to the Transformer\'s global attention mechanism, which captures the overall shape and features of large objects. DETR outperformed Faster R-CNN in AP for large objects.\\n- **Small Object Detection**\\n  - Performance on small objects was relatively lower. This is attributed to the CNN\'s inability to effectively capture features of small objects and the Transformer\'s limitations in learning multi-scale features.\\n- **Training Time**\\n  - DETR required longer training times compared to Faster R-CNN due to the slow convergence of the attention mechanism in the early stages. However, inference time was relatively shorter.\\n- **Reduced False Positives and Negatives**\\n  - The introduction of Bipartite Matching allowed for unique, one-to-one matching of object queries to actual objects, reducing the rates of false positives and negatives, thereby improving overall accuracy.\\n\\n![Performance Comparison](https://github.com/user-attachments/assets/16e83512-6ec4-492c-827a-5e3b01cdcb88)\\n\\n### Conclusion\\n\\nDETR represents a significant advancement in object detection by leveraging the strengths of Transformer architecture. While it shows superior performance in detecting large objects and offers a novel approach to matching predictions with ground truth, challenges remain in optimizing its performance for small objects and reducing training times. The incorporation of Bipartite Matching and the dual-head decoder architecture are notable innovations that contribute to its accuracy and efficiency.\\n\\n---\\n","code":"var Component=(()=>{var ln=Object.create;var O=Object.defineProperty;var dn=Object.getOwnPropertyDescriptor;var cn=Object.getOwnPropertyNames;var fn=Object.getPrototypeOf,mn=Object.prototype.hasOwnProperty;var q=(d,n)=>()=>(n||d((n={exports:{}}).exports,n),n.exports),bn=(d,n)=>{for(var _ in n)O(d,_,{get:n[_],enumerable:!0})},je=(d,n,_,N)=>{if(n&&typeof n==\\"object\\"||typeof n==\\"function\\")for(let y of cn(n))!mn.call(d,y)&&y!==_&&O(d,y,{get:()=>n[y],enumerable:!(N=dn(n,y))||N.enumerable});return d};var hn=(d,n,_)=>(_=d!=null?ln(fn(d)):{},je(n||!d||!d.__esModule?O(_,\\"default\\",{value:d,enumerable:!0}):_,d)),_n=d=>je(O({},\\"__esModule\\",{value:!0}),d);var ve=q((xn,xe)=>{xe.exports=React});var De=q(B=>{\\"use strict\\";(function(){\\"use strict\\";var d=ve(),n=Symbol.for(\\"react.element\\"),_=Symbol.for(\\"react.portal\\"),N=Symbol.for(\\"react.fragment\\"),y=Symbol.for(\\"react.strict_mode\\"),z=Symbol.for(\\"react.profiler\\"),K=Symbol.for(\\"react.provider\\"),X=Symbol.for(\\"react.context\\"),U=Symbol.for(\\"react.forward_ref\\"),P=Symbol.for(\\"react.suspense\\"),S=Symbol.for(\\"react.suspense_list\\"),T=Symbol.for(\\"react.memo\\"),F=Symbol.for(\\"react.lazy\\"),Te=Symbol.for(\\"react.offscreen\\"),Q=Symbol.iterator,He=\\"@@iterator\\";function Ge(e){if(e===null||typeof e!=\\"object\\")return null;var t=Q&&e[Q]||e[He];return typeof t==\\"function\\"?t:null}var x=d.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED;function m(e){{for(var t=arguments.length,a=new Array(t>1?t-1:0),i=1;i<t;i++)a[i-1]=arguments[i];Re(\\"error\\",e,a)}}function Re(e,t,a){{var i=x.ReactDebugCurrentFrame,s=i.getStackAddendum();s!==\\"\\"&&(t+=\\"%s\\",a=a.concat([s]));var l=a.map(function(u){return String(u)});l.unshift(\\"Warning: \\"+t),Function.prototype.apply.call(console[e],console,l)}}var we=!1,Ce=!1,Oe=!1,Pe=!1,Se=!1,J;J=Symbol.for(\\"react.module.reference\\");function Fe(e){return!!(typeof e==\\"string\\"||typeof e==\\"function\\"||e===N||e===z||Se||e===y||e===P||e===S||Pe||e===Te||we||Ce||Oe||typeof e==\\"object\\"&&e!==null&&(e.$$typeof===F||e.$$typeof===T||e.$$typeof===K||e.$$typeof===X||e.$$typeof===U||e.$$typeof===J||e.getModuleId!==void 0))}function Ae(e,t,a){var i=e.displayName;if(i)return i;var s=t.displayName||t.name||\\"\\";return s!==\\"\\"?a+\\"(\\"+s+\\")\\":a}function Z(e){return e.displayName||\\"Context\\"}function p(e){if(e==null)return null;if(typeof e.tag==\\"number\\"&&m(\\"Received an unexpected object in getComponentNameFromType(). This is likely a bug in React. Please file an issue.\\"),typeof e==\\"function\\")return e.displayName||e.name||null;if(typeof e==\\"string\\")return e;switch(e){case N:return\\"Fragment\\";case _:return\\"Portal\\";case z:return\\"Profiler\\";case y:return\\"StrictMode\\";case P:return\\"Suspense\\";case S:return\\"SuspenseList\\"}if(typeof e==\\"object\\")switch(e.$$typeof){case X:var t=e;return Z(t)+\\".Consumer\\";case K:var a=e;return Z(a._context)+\\".Provider\\";case U:return Ae(e,e.render,\\"ForwardRef\\");case T:var i=e.displayName||null;return i!==null?i:p(e.type)||\\"Memo\\";case F:{var s=e,l=s._payload,u=s._init;try{return p(u(l))}catch{return null}}}return null}var j=Object.assign,k=0,ee,ne,re,te,ae,ie,oe;function ue(){}ue.__reactDisabledLog=!0;function Ie(){{if(k===0){ee=console.log,ne=console.info,re=console.warn,te=console.error,ae=console.group,ie=console.groupCollapsed,oe=console.groupEnd;var e={configurable:!0,enumerable:!0,value:ue,writable:!0};Object.defineProperties(console,{info:e,log:e,warn:e,error:e,group:e,groupCollapsed:e,groupEnd:e})}k++}}function Le(){{if(k--,k===0){var e={configurable:!0,enumerable:!0,writable:!0};Object.defineProperties(console,{log:j({},e,{value:ee}),info:j({},e,{value:ne}),warn:j({},e,{value:re}),error:j({},e,{value:te}),group:j({},e,{value:ae}),groupCollapsed:j({},e,{value:ie}),groupEnd:j({},e,{value:oe})})}k<0&&m(\\"disabledDepth fell below zero. This is a bug in React. Please file an issue.\\")}}var A=x.ReactCurrentDispatcher,I;function H(e,t,a){{if(I===void 0)try{throw Error()}catch(s){var i=s.stack.trim().match(/\\\\n( *(at )?)/);I=i&&i[1]||\\"\\"}return`\\n`+I+e}}var L=!1,G;{var Me=typeof WeakMap==\\"function\\"?WeakMap:Map;G=new Me}function se(e,t){if(!e||L)return\\"\\";{var a=G.get(e);if(a!==void 0)return a}var i;L=!0;var s=Error.prepareStackTrace;Error.prepareStackTrace=void 0;var l;l=A.current,A.current=null,Ie();try{if(t){var u=function(){throw Error()};if(Object.defineProperty(u.prototype,\\"props\\",{set:function(){throw Error()}}),typeof Reflect==\\"object\\"&&Reflect.construct){try{Reflect.construct(u,[])}catch(g){i=g}Reflect.construct(e,[],u)}else{try{u.call()}catch(g){i=g}e.call(u.prototype)}}else{try{throw Error()}catch(g){i=g}e()}}catch(g){if(g&&i&&typeof g.stack==\\"string\\"){for(var o=g.stack.split(`\\n`),b=i.stack.split(`\\n`),c=o.length-1,f=b.length-1;c>=1&&f>=0&&o[c]!==b[f];)f--;for(;c>=1&&f>=0;c--,f--)if(o[c]!==b[f]){if(c!==1||f!==1)do if(c--,f--,f<0||o[c]!==b[f]){var h=`\\n`+o[c].replace(\\" at new \\",\\" at \\");return e.displayName&&h.includes(\\"<anonymous>\\")&&(h=h.replace(\\"<anonymous>\\",e.displayName)),typeof e==\\"function\\"&&G.set(e,h),h}while(c>=1&&f>=0);break}}}finally{L=!1,A.current=l,Le(),Error.prepareStackTrace=s}var D=e?e.displayName||e.name:\\"\\",Ne=D?H(D):\\"\\";return typeof e==\\"function\\"&&G.set(e,Ne),Ne}function We(e,t,a){return se(e,!1)}function Ye(e){var t=e.prototype;return!!(t&&t.isReactComponent)}function R(e,t,a){if(e==null)return\\"\\";if(typeof e==\\"function\\")return se(e,Ye(e));if(typeof e==\\"string\\")return H(e);switch(e){case P:return H(\\"Suspense\\");case S:return H(\\"SuspenseList\\")}if(typeof e==\\"object\\")switch(e.$$typeof){case U:return We(e.render);case T:return R(e.type,t,a);case F:{var i=e,s=i._payload,l=i._init;try{return R(l(s),t,a)}catch{}}}return\\"\\"}var w=Object.prototype.hasOwnProperty,le={},de=x.ReactDebugCurrentFrame;function C(e){if(e){var t=e._owner,a=R(e.type,e._source,t?t.type:null);de.setExtraStackFrame(a)}else de.setExtraStackFrame(null)}function $e(e,t,a,i,s){{var l=Function.call.bind(w);for(var u in e)if(l(e,u)){var o=void 0;try{if(typeof e[u]!=\\"function\\"){var b=Error((i||\\"React class\\")+\\": \\"+a+\\" type `\\"+u+\\"` is invalid; it must be a function, usually from the `prop-types` package, but received `\\"+typeof e[u]+\\"`.This often happens because of typos such as `PropTypes.function` instead of `PropTypes.func`.\\");throw b.name=\\"Invariant Violation\\",b}o=e[u](t,u,i,a,null,\\"SECRET_DO_NOT_PASS_THIS_OR_YOU_WILL_BE_FIRED\\")}catch(c){o=c}o&&!(o instanceof Error)&&(C(s),m(\\"%s: type specification of %s `%s` is invalid; the type checker function must return `null` or an `Error` but returned a %s. You may have forgotten to pass an argument to the type checker creator (arrayOf, instanceOf, objectOf, oneOf, oneOfType, and shape all require an argument).\\",i||\\"React class\\",a,u,typeof o),C(null)),o instanceof Error&&!(o.message in le)&&(le[o.message]=!0,C(s),m(\\"Failed %s type: %s\\",a,o.message),C(null))}}}var Ve=Array.isArray;function M(e){return Ve(e)}function qe(e){{var t=typeof Symbol==\\"function\\"&&Symbol.toStringTag,a=t&&e[Symbol.toStringTag]||e.constructor.name||\\"Object\\";return a}}function Be(e){try{return ce(e),!1}catch{return!0}}function ce(e){return\\"\\"+e}function fe(e){if(Be(e))return m(\\"The provided key is an unsupported type %s. This value must be coerced to a string before before using it here.\\",qe(e)),ce(e)}var E=x.ReactCurrentOwner,ze={key:!0,ref:!0,__self:!0,__source:!0},me,be,W;W={};function Ke(e){if(w.call(e,\\"ref\\")){var t=Object.getOwnPropertyDescriptor(e,\\"ref\\").get;if(t&&t.isReactWarning)return!1}return e.ref!==void 0}function Xe(e){if(w.call(e,\\"key\\")){var t=Object.getOwnPropertyDescriptor(e,\\"key\\").get;if(t&&t.isReactWarning)return!1}return e.key!==void 0}function Qe(e,t){if(typeof e.ref==\\"string\\"&&E.current&&t&&E.current.stateNode!==t){var a=p(E.current.type);W[a]||(m(\'Component \\"%s\\" contains the string ref \\"%s\\". Support for string refs will be removed in a future major release. This case cannot be automatically converted to an arrow function. We ask you to manually fix this case by using useRef() or createRef() instead. Learn more about using refs safely here: https://reactjs.org/link/strict-mode-string-ref\',p(E.current.type),e.ref),W[a]=!0)}}function Je(e,t){{var a=function(){me||(me=!0,m(\\"%s: `key` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\\",t))};a.isReactWarning=!0,Object.defineProperty(e,\\"key\\",{get:a,configurable:!0})}}function Ze(e,t){{var a=function(){be||(be=!0,m(\\"%s: `ref` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\\",t))};a.isReactWarning=!0,Object.defineProperty(e,\\"ref\\",{get:a,configurable:!0})}}var en=function(e,t,a,i,s,l,u){var o={$$typeof:n,type:e,key:t,ref:a,props:u,_owner:l};return o._store={},Object.defineProperty(o._store,\\"validated\\",{configurable:!1,enumerable:!1,writable:!0,value:!1}),Object.defineProperty(o,\\"_self\\",{configurable:!1,enumerable:!1,writable:!1,value:i}),Object.defineProperty(o,\\"_source\\",{configurable:!1,enumerable:!1,writable:!1,value:s}),Object.freeze&&(Object.freeze(o.props),Object.freeze(o)),o};function nn(e,t,a,i,s){{var l,u={},o=null,b=null;a!==void 0&&(fe(a),o=\\"\\"+a),Xe(t)&&(fe(t.key),o=\\"\\"+t.key),Ke(t)&&(b=t.ref,Qe(t,s));for(l in t)w.call(t,l)&&!ze.hasOwnProperty(l)&&(u[l]=t[l]);if(e&&e.defaultProps){var c=e.defaultProps;for(l in c)u[l]===void 0&&(u[l]=c[l])}if(o||b){var f=typeof e==\\"function\\"?e.displayName||e.name||\\"Unknown\\":e;o&&Je(u,f),b&&Ze(u,f)}return en(e,o,b,s,i,E.current,u)}}var Y=x.ReactCurrentOwner,he=x.ReactDebugCurrentFrame;function v(e){if(e){var t=e._owner,a=R(e.type,e._source,t?t.type:null);he.setExtraStackFrame(a)}else he.setExtraStackFrame(null)}var $;$=!1;function V(e){return typeof e==\\"object\\"&&e!==null&&e.$$typeof===n}function _e(){{if(Y.current){var e=p(Y.current.type);if(e)return`\\n\\nCheck the render method of \\\\``+e+\\"`.\\"}return\\"\\"}}function rn(e){{if(e!==void 0){var t=e.fileName.replace(/^.*[\\\\\\\\\\\\/]/,\\"\\"),a=e.lineNumber;return`\\n\\nCheck your code at `+t+\\":\\"+a+\\".\\"}return\\"\\"}}var pe={};function tn(e){{var t=_e();if(!t){var a=typeof e==\\"string\\"?e:e.displayName||e.name;a&&(t=`\\n\\nCheck the top-level render call using <`+a+\\">.\\")}return t}}function ge(e,t){{if(!e._store||e._store.validated||e.key!=null)return;e._store.validated=!0;var a=tn(t);if(pe[a])return;pe[a]=!0;var i=\\"\\";e&&e._owner&&e._owner!==Y.current&&(i=\\" It was passed a child from \\"+p(e._owner.type)+\\".\\"),v(e),m(\'Each child in a list should have a unique \\"key\\" prop.%s%s See https://reactjs.org/link/warning-keys for more information.\',a,i),v(null)}}function ye(e,t){{if(typeof e!=\\"object\\")return;if(M(e))for(var a=0;a<e.length;a++){var i=e[a];V(i)&&ge(i,t)}else if(V(e))e._store&&(e._store.validated=!0);else if(e){var s=Ge(e);if(typeof s==\\"function\\"&&s!==e.entries)for(var l=s.call(e),u;!(u=l.next()).done;)V(u.value)&&ge(u.value,t)}}}function an(e){{var t=e.type;if(t==null||typeof t==\\"string\\")return;var a;if(typeof t==\\"function\\")a=t.propTypes;else if(typeof t==\\"object\\"&&(t.$$typeof===U||t.$$typeof===T))a=t.propTypes;else return;if(a){var i=p(t);$e(a,e.props,\\"prop\\",i,e)}else if(t.PropTypes!==void 0&&!$){$=!0;var s=p(t);m(\\"Component %s declared `PropTypes` instead of `propTypes`. Did you misspell the property assignment?\\",s||\\"Unknown\\")}typeof t.getDefaultProps==\\"function\\"&&!t.getDefaultProps.isReactClassApproved&&m(\\"getDefaultProps is only used on classic React.createClass definitions. Use a static property named `defaultProps` instead.\\")}}function on(e){{for(var t=Object.keys(e.props),a=0;a<t.length;a++){var i=t[a];if(i!==\\"children\\"&&i!==\\"key\\"){v(e),m(\\"Invalid prop `%s` supplied to `React.Fragment`. React.Fragment can only have `key` and `children` props.\\",i),v(null);break}}e.ref!==null&&(v(e),m(\\"Invalid attribute `ref` supplied to `React.Fragment`.\\"),v(null))}}function un(e,t,a,i,s,l){{var u=Fe(e);if(!u){var o=\\"\\";(e===void 0||typeof e==\\"object\\"&&e!==null&&Object.keys(e).length===0)&&(o+=\\" You likely forgot to export your component from the file it\'s defined in, or you might have mixed up default and named imports.\\");var b=rn(s);b?o+=b:o+=_e();var c;e===null?c=\\"null\\":M(e)?c=\\"array\\":e!==void 0&&e.$$typeof===n?(c=\\"<\\"+(p(e.type)||\\"Unknown\\")+\\" />\\",o=\\" Did you accidentally export a JSX literal instead of a component?\\"):c=typeof e,m(\\"React.jsx: type is invalid -- expected a string (for built-in components) or a class/function (for composite components) but got: %s.%s\\",c,o)}var f=nn(e,t,a,s,l);if(f==null)return f;if(u){var h=t.children;if(h!==void 0)if(i)if(M(h)){for(var D=0;D<h.length;D++)ye(h[D],e);Object.freeze&&Object.freeze(h)}else m(\\"React.jsx: Static children should always be an array. You are likely explicitly calling React.jsxs or React.jsxDEV. Use the Babel transform instead.\\");else ye(h,e)}return e===N?on(f):an(f),f}}var sn=un;B.Fragment=N,B.jsxDEV=sn})()});var Ee=q((Dn,ke)=>{\\"use strict\\";ke.exports=De()});var Nn={};bn(Nn,{default:()=>yn,frontmatter:()=>pn});var r=hn(Ee()),pn={title:\\"[Paper Review] End-to-End Object Detection with Transformers (DETR)\\",description:\\"\\",image:\\"../../public/blogs/slowfast/screenshot.png\\",publishedAt:\\"2024-07-23\\",updatedAt:\\"2024-07-23\\",author:\\"junbrro\\",isPublished:!0,tags:[\\"Deep Learning\\"]};function Ue(d){let n=Object.assign({p:\\"p\\",strong:\\"strong\\",a:\\"a\\",h1:\\"h1\\",span:\\"span\\",h2:\\"h2\\",img:\\"img\\",h3:\\"h3\\",ol:\\"ol\\",li:\\"li\\",ul:\\"ul\\",hr:\\"hr\\"},d.components);return(0,r.jsxDEV)(r.Fragment,{children:[(0,r.jsxDEV)(n.p,{children:\\"This post is the review of DETR Paper\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:13,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:[(0,r.jsxDEV)(n.strong,{children:\\"Citations\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:15,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.a,{href:\\"https://youtu.be/q1wSykClIMk?si=fFxHKqCrNkuOcW9F\\",children:\\"DSBA Lab\'s Youtube video\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:16,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.a,{href:\\"https://herbwood.tistory.com/26\\",children:\\"HerbWood Tistory blog\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:17,columnNumber:1},this)]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:15,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h1,{id:\\"detr-end-to-end-object-detection-with-transformers\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#detr-end-to-end-object-detection-with-transformers\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\"},this),\\"DETR: End-to-End Object Detection with Transformers\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:19,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:\\"Object detection is a task that involves detecting objects within an image and predicting their class and location. Traditional object detection models are divided into two categories: one-stage detectors and two-stage detectors. One-stage detectors, like YOLO (You Only Look Once), are known for their speed and suitability for real-time applications. In contrast, two-stage detectors, such as Faster R-CNN, generally offer higher accuracy but at the cost of speed.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:21,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:\\"The Transformer model, initially designed for NLP tasks, has been adapted for vision tasks in DETR. DETR leverages the Transformer architecture\'s encoder-decoder structure and its ability to learn global information through the attention mechanism. However, unlike Vision Transformers (ViT), which directly input image patches into the encoder, DETR utilizes image features extracted via a CNN.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:23,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h2,{id:\\"detr-network-architecture\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#detr-network-architecture\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\"},this),\\"DETR Network Architecture\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:25,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/user-attachments/assets/ce5f48c6-73d6-45d6-9c0d-ecd0ec88dd47\\",alt:\\"DETR Architecture\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:27,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:27,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/user-attachments/assets/192780ef-86dd-46e8-8340-868ab95e7a5f\\",alt:\\"Detailed Structure\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:29,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:29,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h3,{id:\\"key-differences-between-detr-and-traditional-transformers\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#key-differences-between-detr-and-traditional-transformers\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\"},this),\\"Key Differences Between DETR and Traditional Transformers\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:31,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.ol,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:[`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.strong,{children:\\"Feature Map Input\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:33,columnNumber:4},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:33,columnNumber:4},this),`\\n`,(0,r.jsxDEV)(n.ul,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:\\"DETR receives image feature maps from the encoder, whereas traditional Transformers take embeddings from sentences as input. DETR extracts feature maps using a CNN backbone, reduces their dimensions through a 1x1 convolution layer, and flattens the spatial dimensions before inputting them into the encoder. For example, a feature map with height ( h ), width ( w ), and channel count ( C ) is converted to ( d \\\\\\\\times hw ) for input, where ( d ) is smaller than ( C ).\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:35,columnNumber:4},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:35,columnNumber:4},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:33,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:[`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.strong,{children:\\"Positional Encoding\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:37,columnNumber:4},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:37,columnNumber:4},this),`\\n`,(0,r.jsxDEV)(n.ul,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:\\"Transformers use positional encoding to maintain order within the input sequence due to their permutation invariant nature. DETR generalizes this to 2D spatial positional encoding for feature maps. It applies 2D sine and cosine functions along the x and y axes to derive positional encoding, which is then concatenated channel-wise and added to the input features.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:39,columnNumber:4},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:39,columnNumber:4},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:37,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:[`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.strong,{children:\\"Object Queries\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:41,columnNumber:4},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:41,columnNumber:4},this),`\\n`,(0,r.jsxDEV)(n.ul,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:\\"Instead of target embeddings used in traditional Transformers, DETR uses object queries, which are learnable embeddings of length ( N ).\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:43,columnNumber:4},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:43,columnNumber:4},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:41,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:[`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.strong,{children:\\"Attention Mechanism\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:45,columnNumber:4},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:45,columnNumber:4},this),`\\n`,(0,r.jsxDEV)(n.ul,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:\\"Traditional Transformers employ masked multi-head attention during the first attention operation in the decoder to prevent future token information from being used. DETR, however, performs multi-head self-attention without masking since it predicts all object locations in parallel.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:47,columnNumber:4},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:47,columnNumber:4},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:45,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:[`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.strong,{children:\\"Dual Head Output\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:49,columnNumber:4},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:49,columnNumber:4},this),`\\n`,(0,r.jsxDEV)(n.ul,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:\\"DETR has two heads in the decoder: one for predicting bounding boxes and the other for class probabilities, unlike traditional Transformers, which have a single linear layer for predicting class probabilities.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:50,columnNumber:4},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:50,columnNumber:4},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:49,columnNumber:1},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:33,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/user-attachments/assets/7031c6c2-b347-4211-9fb2-f2d39af866c8\\",alt:\\"Transformer Encoder\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:52,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:52,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h3,{id:\\"encoder\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#encoder\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\"},this),\\"Encoder\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:54,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:\\"The image is converted into a feature map via a CNN, which serves as the input for the Transformer encoder. Unlike traditional CNNs, the encoder can capture global information.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:56,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/user-attachments/assets/1df7e4f1-0a3d-4012-9607-0e3651b387ff\\",alt:\\"Positional Encoding\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:58,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:58,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h3,{id:\\"decoder\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#decoder\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\"},this),\\"Decoder\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:60,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:\\"The Transformer decoder receives object queries as input, learning to detect specific parts of the image. Positional embedding in the decoder is not meaningful since object queries are independent and parallel, rendering position information irrelevant. Instead, object queries output positional encoding via attention mechanisms.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:62,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/user-attachments/assets/7ddd7b31-f831-4120-9613-6c2512d8e47d\\",alt:\\"Object Queries\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:64,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:64,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:\\"DETR introduces the concept of Bipartite Matching to match predictions with ground truth objects, optimizing the match using the Hungarian algorithm based on class prediction and box prediction costs.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:66,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/user-attachments/assets/ff3e72db-4684-46ae-9d2b-8a0ffae1c482\\",alt:\\"Hungarian Algorithm\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:68,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:68,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h3,{id:\\"loss-function\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#loss-function\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\"},this),\\"Loss Function\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:70,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.ol,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\\"Classification Loss\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:72,columnNumber:4},this),`\\n`,(0,r.jsxDEV)(n.ul,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:\\"Measures the difference between the predicted class probabilities and the actual classes using cross-entropy loss.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:73,columnNumber:4},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:73,columnNumber:4},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:72,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\\"Bounding Box Loss\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:74,columnNumber:4},this),`\\n`,(0,r.jsxDEV)(n.ul,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:\\"Measures the difference between predicted and actual bounding box coordinates using L1 loss and Generalized IoU (GIoU) loss. While IoU measures the ratio of overlapping area between two bounding boxes, GIoU introduces an additional term to handle cases where boxes do not overlap, providing a more suitable metric.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:75,columnNumber:4},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:75,columnNumber:4},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:74,columnNumber:1},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:72,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/user-attachments/assets/a13435a8-7719-43c5-b4c4-69e7358f0707\\",alt:\\"Loss Function\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:77,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:77,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h3,{id:\\"performance-evaluation-and-experimental-results\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#performance-evaluation-and-experimental-results\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\"},this),\\"Performance Evaluation and Experimental Results\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:79,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:\\"DETR\'s performance was evaluated using the COCO dataset, a standard benchmark for object detection models. The key metrics used were Average Precision (AP) and Average Recall (AR).\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:81,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.ul,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\\"Large Object Detection\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:83,columnNumber:3},this),`\\n`,(0,r.jsxDEV)(n.ul,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:\\"DETR excels at detecting large objects due to the Transformer\'s global attention mechanism, which captures the overall shape and features of large objects. DETR outperformed Faster R-CNN in AP for large objects.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:84,columnNumber:3},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:84,columnNumber:3},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:83,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\\"Small Object Detection\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:85,columnNumber:3},this),`\\n`,(0,r.jsxDEV)(n.ul,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:\\"Performance on small objects was relatively lower. This is attributed to the CNN\'s inability to effectively capture features of small objects and the Transformer\'s limitations in learning multi-scale features.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:86,columnNumber:3},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:86,columnNumber:3},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:85,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\\"Training Time\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:87,columnNumber:3},this),`\\n`,(0,r.jsxDEV)(n.ul,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:\\"DETR required longer training times compared to Faster R-CNN due to the slow convergence of the attention mechanism in the early stages. However, inference time was relatively shorter.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:88,columnNumber:3},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:88,columnNumber:3},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:87,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.li,{children:[(0,r.jsxDEV)(n.strong,{children:\\"Reduced False Positives and Negatives\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:89,columnNumber:3},this),`\\n`,(0,r.jsxDEV)(n.ul,{children:[`\\n`,(0,r.jsxDEV)(n.li,{children:\\"The introduction of Bipartite Matching allowed for unique, one-to-one matching of object queries to actual objects, reducing the rates of false positives and negatives, thereby improving overall accuracy.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:90,columnNumber:3},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:90,columnNumber:3},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:89,columnNumber:1},this),`\\n`]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:83,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:(0,r.jsxDEV)(n.img,{src:\\"https://github.com/user-attachments/assets/16e83512-6ec4-492c-827a-5e3b01cdcb88\\",alt:\\"Performance Comparison\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:92,columnNumber:1},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:92,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.h3,{id:\\"conclusion\\",children:[(0,r.jsxDEV)(n.a,{\\"aria-hidden\\":\\"true\\",tabIndex:\\"-1\\",href:\\"#conclusion\\",children:(0,r.jsxDEV)(n.span,{className:\\"icon icon-link\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\"},this)},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\"},this),\\"Conclusion\\"]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:94,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.p,{children:\\"DETR represents a significant advancement in object detection by leveraging the strengths of Transformer architecture. While it shows superior performance in detecting large objects and offers a novel approach to matching predictions with ground truth, challenges remain in optimizing its performance for small objects and reducing training times. The incorporation of Bipartite Matching and the dual-head decoder architecture are notable innovations that contribute to its accuracy and efficiency.\\"},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:96,columnNumber:1},this),`\\n`,(0,r.jsxDEV)(n.hr,{},void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:98,columnNumber:1},this)]},void 0,!0,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\",lineNumber:1,columnNumber:1},this)}function gn(d={}){let{wrapper:n}=d.components||{};return n?(0,r.jsxDEV)(n,Object.assign({},d,{children:(0,r.jsxDEV)(Ue,d,void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\"},this)}),void 0,!1,{fileName:\\"/Users/junhyeongpark/Documents/GitHub/jun-brro-blog/content/_mdx_bundler_entry_point-aff61825-a447-496e-a864-02e1ba34898a.mdx\\"},this):Ue(d)}var yn=gn;return _n(Nn);})();\\n/*! Bundled license information:\\n\\nreact/cjs/react-jsx-dev-runtime.development.js:\\n  (**\\n   * @license React\\n   * react-jsx-dev-runtime.development.js\\n   *\\n   * Copyright (c) Facebook, Inc. and its affiliates.\\n   *\\n   * This source code is licensed under the MIT license found in the\\n   * LICENSE file in the root directory of this source tree.\\n   *)\\n*/\\n;return Component;"},"_id":"detr/index.mdx","_raw":{"sourceFilePath":"detr/index.mdx","sourceFileName":"index.mdx","sourceFileDir":"detr","contentType":"mdx","flattenedPath":"detr"},"type":"Blog","url":"/blogs/detr","readingTime":{"text":"5 min read","minutes":4.24,"time":254400,"words":848},"toc":[{"level":"one","text":"DETR: End-to-End Object Detection with Transformers","slug":"detr-end-to-end-object-detection-with-transformers"},{"level":"two","text":"DETR Network Architecture","slug":"detr-network-architecture"},{"level":"three","text":"Key Differences Between DETR and Traditional Transformers","slug":"key-differences-between-detr-and-traditional-transformers"},{"level":"three","text":"Encoder","slug":"encoder"},{"level":"three","text":"Decoder","slug":"decoder"},{"level":"three","text":"Loss Function","slug":"loss-function"},{"level":"three","text":"Performance Evaluation and Experimental Results","slug":"performance-evaluation-and-experimental-results"},{"level":"three","text":"Conclusion","slug":"conclusion"}]}');

/***/ })

});